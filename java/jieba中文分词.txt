

        <dependency>
            <groupId>com.huaban</groupId>
            <artifactId>jieba-analysis</artifactId>
            <version>1.0.2</version>
        </dependency>

注意：版本，否则可能需要自己上传词典等；

import com.huaban.analysis.jieba.JiebaSegmenter;

private static JiebaSegmenter segmenter = new JiebaSegmenter();
        String content = "我是中国人,中文汉字分词";
        List<String> result = segmenter.sentenceProcess(content);
        System.out.println("没有过滤停用词======" + result);
//        result = result.stream().map(o -> o.trim()).filter(o -> !stop_words.contains(o)).collect(Collectors.toList());
//        System.out.println("过滤停用词=========" + result);

