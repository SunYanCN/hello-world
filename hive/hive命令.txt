
hive删除表：
drop table db_name.table_name;
hive删除表中数据：
truncate table db_name.table_name;

删除空数据库：
drop database db_name;

采用 if exists判断数据库是否存在，若存在则删除：
drop database if exists db_name;

如果数据库不为空，可以采用cascade命令，强制删除数据库：
drop database db_name cascade;

删除空行数据（根据查询结果不为空，覆盖旧数据）：
insert overwrite table sor select * from sor where id is not null;   //覆盖之前表里的数据

查看表的建表语句(这里是查看db_name库中的table_name表的建表语句)：
SHOW CREATE TABLE db_name.table_name;

查看表的详细属性信息（desc formatted）：
desc formatted db_name.table_name;

查看表的详细信息（describe extended）：
describe extended db_name.table_name;

查看表的总记录数：
select count(*) from db_name.table_name;

查看有哪些分区：
show partitions db_name.table_name;

# 进入Hive命令行，执行show databases;命令，可以列出hive中的所有数据库，默认有一个default数据库，进入Hive-Cli之后，即到default数据库下。
# 列出所有的数据库：
show databases;

# 对列出的数据库，模糊搜索，即查找含有某关键词的数据库,如查找数据库名还有‘weibo’的数据库名：
show databases like '*weibo*';

# 使用use databasename;可以切换到某个数据库下,切换到具体数据库；
use db_name;

# 查看某个数据库下面有哪些数据表（需先用use db_name切换数据库）：
# show tables; 即可查看该数据库下所有的表
use db_name;
show tables;

# 对当前数库下的表名模糊搜索，即搜索包含某关键词的数据表，如查找表名有‘uid’的数据表：
use db_name;
show tables '*uid*';

在现有表上增加一列(这里增加一列名为‘storage_time’)：
ALTER TABLE db_name.table_name ADD COLUMNS (storage_time timestamp comment "存储时间");

执行数据导出命令：
hive -e "set mapreduce.job.queuename=queue_6202_01;select * from db_name.table_name">/data/table_name_data.txt

sql的查询结果将直接保存到/tmp/out.txt中
$ hive -e "select user, login_timestamp from user_login" > /tmp/out.txt

当然我们也可以查询保存到某个文件file.sql中，按下面的方式执行查询，并保存结果
 $ hive -f test.sql > /tmp/out.txt

 cat test.sql
 select * from user_login

查看附件数据命令：
hadoop dfs -ls /apps-data/hduser2020/caijing_data
获取数据文件：
hadoop dfs -get /apps-data/hduser2020/caijing_data/259786.tar.gz
# 查看数据文件大小：
# hadoop dfs -du -h /apps-data/hduser2020/caijing_data/259786.tar.gz

# 查看某个数据表数据最后的更新时间：
第一步，获取hdfs路径：
desc.formatted db_name.table_name;
获取结果示例：Location: hdfs://hdfs01-sh/user/hive/warehouse/db_name.db/table_name
第二步：通过 dfs -ls <hdfs path> 命令查看数据文件最新更新时间：
hadoop dfs -ls hdfs://hdfs01-sh/user/hive/warehouse/db_name.db/table_name

kill掉某个正在运行的任务：
hadoop job -kill job_1603343185745

查询多个字段非空值数量：
select count(field2) as field2_num, count(field1) as field1_num from db_name.table_name;
注：count(*)对行的数目进行计算，包含NULL
count(column)对特定的列的值具有的行数进行计算，不包含NULL值。
count()还有一种使用方式，count(1)这个用法和count(*)的结果是一样的。
count(1)跟count(主键)一样，只扫描主键。count(*)跟count(非主键)一样，扫描整个表。明显前者更快一些。

# 查询某个字段的非空值：
select distinct field from db_name.table_name where field is not null limit 20；
或者：
select distinct field from db_name.table_name where length(field) > 0 limit 20;
或者：
select * from db_name.table_name where field1 is not null and field2 is not null limit 100;

# 输出数组最大值、最小值：
select sort_array(array(1.5, 2.3, 0))[size(array(1.5, 2.3, 0))-1] as max_value;
select sort_array(array(1.5, 2.3, 0))[0] as min_value;


# 查询字段重命名为中文，需要用尖引号：
select distinct field1 as  `字段1` from db_name.table_name where length(field1) > 0 limit 1000;

如何把一个表的查询结果插入到另一张表中(两个表的结构不同), 插入的字段个数和查询的字段个数必须一致
insert into 表名(字段1,字段2,字段3...) select 字段1,字段2,字段3.. from 查询表名 where ..

insert into A(id,names)select id,firstName from B;

另外，在使用 insert into select 语句时，可能会产生锁表，导致在用此句备份数据时，其他用户不能查询数据，特别是数据量大的时候更应该注意：
如：在生产使用insert into t_test_2 select * from t_test_1 where name like 'trest'语句时，若name 字段缺失索引，就会造成备份数据时，锁全表；
查看表有哪些字段有索引的命令：
use db_name;
show index on table_name;

查询两个表某个字段相同的结果：
select a.*, b.* from (
    select aa.* from db_name.tabel_name1 aa
    where aa.city='杭州市' and aa.flag='credit'
    )
    a
inner join (
    select bb.* from db_name.tabel_name2 bb
    )
    b
on a.id1=b.id2


hive中的查询结果插入语句有两种：
insert into table cite select * from cite;
这个的意思就是将cite表中的数据复制一份，然后插入到原表中去，而
insert overwrite table cite select * from tt;
这个的意思就是会用tt表查出来的数据覆盖掉cite表格中已经存在的数据

# 字符串的模糊匹配：
select * from ods.ods_sjc_events_rt where event like concat('%','OCR','%');

# 通过regexp 方式查询多个值，使用|实现or效果
select * from user where name regexp 'mac1|mac2|mac3'

# 去重查找：
select count(distinct field1,field2) from db_name.table_name
注意：使用count distinct 两列联合去重时，若有任何一列为null，那么这一行都不会计入到结果中

# 对某个字段进行统计计数(这里id,为唯一标识字段名)：
select field, count(distinct id) from db_name.table_name group by field;

# 统计计数的高级用法：
SELECT
    type
  , count(*)
  , count(DISTINCT u)
  , count(CASE WHEN plat=1 THEN u ELSE NULL END)
  , count(DISTINCT CASE WHEN plat=1 THEN u ELSE NULL END)
  , count(CASE WHEN (type=2 OR type=6) THEN u ELSE NULL END)
  , count(DISTINCT CASE WHEN (type=2 OR type=6) THEN u ELSE NULL END)
FROM t
WHERE dt in ("2018-05-20", "2018-05-21")
GROUP BY type
ORDER BY type

比如，统计每个用户的文本数据，及其包含‘信用卡’文本数量：
select uid, count(CASE WHEN text like concat('%', '信用卡', '%') THEN 1 ELSE NULL END), count(*)
from db_name.table_name group by uid;

# 字符串的截断
使用 Hive 中 substr() 函数来实现。
1、截取前两位：
substr(ID,0,2)
substr() 第一个参数表示待截取的字段名称，第二个参数表示截取的起始位置，第三个参数表示截取的长度。
2、截取后两位：
substr(ID,-2,2)
表示从倒数第二个位置开始截取两位，即截取后两位。
substr函数适合定长的字符串截取，如果起始位置超过了字符串长度返回的是空串。
select substr('123456', -8, 8); 返回结果为空；
select substr('123456', 2, 8); 返回结果为：23456
需注意的是：select substr('123456', 0, 8); 和 select substr('123456', 1, 8); 的返回结果均是：123456

# 不查询，仅仅返回一个固定字符串；
select "你好中国";

# 去除字符串首尾的空格：
select trim(' 你好中国 ')

# 小写字母变成大写字母：
select upper("abde EDW xyz")

# 按字段降序排列,这里按字段field1为例：
降序：select * from db_name.table_name order by field1 desc limit 500;
升序：select * from db_name.table_name order by field1 asc limit 500;
因为默认是升序，所以多字段降序排列时需特别注意：
如：select * from db_name.table_name order by field1, field2 desc limit 500; 实际上是按field1升序，field2 降序排列；等同于：
select * from db_name.table_name order by field1 asc, field2 desc limit 500;
当然，多字段降序，也可以这样：select * from db_name.table_name order by array(field1, field2) desc limit 500;
总之：
ORDER BY _column1, _column2; /* _column1升序，_column2升序 */ 
ORDER BY _column1, _column2 DESC; /* _column1升序，_column2降序 */ 
ORDER BY _column1 DESC, _column2 ; /* _column1降序，_column2升序 */ 
ORDER BY _column1 DESC, _column2 DESC; /* _column1降序，_column2降序 */ 

# 查询结果过滤，select 嵌套使用(统计用户说‘信用卡’或‘借记卡’的多少句话，用户总共说了多少句话，并且过滤掉没有说‘信用卡’或‘借记卡’的用户)：
select t1.uid, t1.num, t1.value from 
(select uid,
count(case when text regexp '信用卡|借记卡' then 1 else null end) as num,
count(*) as value
)as t1
where t1.num > 0;

# 重命名表：
下面是查询重命名表，把 employee 修改为 emp。
hive> ALTER TABLE employee RENAME TO emp;
这种操作会修改元数据，但不会修改数据本身;
所以在hdfs上warehouse目录下的该表的目录还是employee，因为只会更改元数据信息，而不会修改数据本身；

# Hive查询指定分隔符，指定导出数据分隔符；
使用hive -e导出后默认的分隔符是\t，这时候若原始文本中存在 ‘\t’，用pandas.read_csv读取数据时候，就会报错，所以需要导出数据时指定分隔符； 
可以使用hive的insert语法导出文件
insert overwrite local directory '/home/hadoop/20180303'
row format delimited
fields terminated by ','
select * from table_name limit 100;
需要注意的事，上式会删除数据导出目录“/home/hadoop/20180303”中的原有文件，并将导出100条数据到‘/home/hadoop/20180303’目录下名为‘000000_0’的文件中；

hive抽样：
一般情况下是使用排序函数和rand() 函数来完成随机抽样，limit关键字限制抽样返回的数据；
order by rand()
order by 是全局的，只会启用一个reduce所以比较耗时
select * from ods_user_bucket_log order by rand() limit 10;

sort by rand()
sort by 提供了单个 reducer 内的排序功能，但不保证整体有序，这个时候其实不能做到真正的随机的，因为此时的随机是针对分区去的，所以如果我们可以通过控制进入每个分区的数据也是随机的话，那我们就可以做到随机了
select * from ods_user_bucket_log sort by rand() limit 10;

distribute by rand() sort by rand()
rand函数前的distribute和sort关键字可以保证数据在mapper和reducer阶段是随机分布的，这个时候我们也能做到真正的随机，前面我们也介绍过cluster by 其实基本上是和distribute by sort by 等价的
select * from ods_user_bucket_log distribute by rand() sort by rand() limit 10;

cluster by rand()
cluster by 的功能是 distribute by 和 sort by 的功能相结合，distribute by rand() sort by rand() 进行了两次随机，cluster by rand() 仅一次随机，所以速度上会比上一种方法快
select * from ods_user_bucket_log cluster by rand() limit 10;

tablesample()抽样函数
使用 tablesample 抽取指定的 行数/比例/大小
取1000行：
SELECT * FROM ods_user_data TABLESAMPLE(1000 ROWS);
取20%：
SELECT * FROM ods_user_data TABLESAMPLE (20 PERCENT); 
取1M大小数据（注意的是这里必须是整数M）：
SELECT * FROM ods_user_data TABLESAMPLE(1M); 

按比例随机抽取，如果抽10%的话
select * from table where rand()<=0.1;
特别注意的是：抽样迁先确认表的总记录数

精确取N条
- 样本总量M, 你可以先 rand()<=(N/M+μ), 取多一些随机样本
- 然后再在这些随机样本里随机取N条
这样一定是随机取出N条. 且 能够 去掉最耗时间耗资源的全局排序
例1：select * from table where rand()<=0.15  cluster by rand()  limit  N;    ( 数据较大时 用 cluster by) 
例2：select * from table where rand()<=0.15  order by rand()   limit  N;   ( 数据较小时 用  order by) 

hive 查询某一重复字段记录第一条数据：
insert overwrite table hive_jdbc_test partition(day='2018-1-1') 
select key,value 
from (SELECT *, Row_Number() OVER (partition by key,value ORDER BY value desc) rank 
FROM hive_jdbc_test where day='2018-1-1') t 
where t.rank=1;

这里主要的代码就是row_number() OVER (PARTITION BY COL1 ORDER BY COL2) 
这行代码的意思是先对COL1列进行分组，然后按照COL2进行排序，row_number()函数是对分组后的每个组内记录按照COL2排序标号，我们最后取的时候就拿标号为1的一条记录，即达到我的需求（对于COL1中重复的数据仅留一条记录）。

hive修改表和字段注释：
修改表:
ALTER TABLE db_name.table_name SET TBLPROPERTIES('comment' = '这是表新注释!');
修改字段:
ALTER TABLE table_name CHANGE COLUMN muid muid_new STRING COMMENT '这里是列新注释!'; 

创建表，并插入数据：
CREATE TABLE `test_db.flag_all_client_party_credit_info`(
	`party_id` string COMMENT '客户号',
	`blacklist_type` string COMMENT '是否高风险客户',
	`dsi_cluster` string COMMENT '消费风险评估',
	`person_all_score_std` string COMMENT '总体信用评估',
	`have_loan_flag` string COMMENT '是否有过贷款',
	`have_car_loan_flag` string COMMENT '是否有过车贷',
	`have_house_loan_flag` string COMMENT '是否有过房贷',
	`is_more_credit_card` string COMMENT '是否有多张信用卡',
	`loan_overdue_flag` string COMMENT '是否有过贷款逾期',
	`credit_overdue_flag` string COMMENT '是否有过信用卡逾期',
	`fake_risk` string COMMENT '欺诈风险评估')
COMMENT '客户信用标签'
ROW FORMAT SERDE
'org.apache.hadoop.hive.ql.io.orc.OrcSerde'
STORED AS INPUTFORMAT
'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'
OUTPUTFORMAT
'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat';

INSERT INTO test_db.flag_all_client_party_credit_info
VALUES
('067095720414', '是', '', '中', '否', '', '', '是', '否', '是', ''),
('013029565207', '', '低', '低', '否', '', '否', '是', '', '是', '否'),
('065357069340', '', '', '中', '是', '否', '', '', '', '是', '');


hads查看文件路径：
hdfs dfs -ls /tmp/hd36

hdfs创建目录：
hdfs dfs -mkdir /tmp/hd36/collision

hdfs将文件内容，写入hdfs:
hdfs dfs -put /home/hd36/test_20210312/temp_sub_result.txt /tmp/hd36/collision/abcd12341234/detail.csv

hive数据导出到hdfs(这里是导出表users_info的数据):
hive -e "export table users_info to '/home/hd35/';"

计算文本的sha256,或者md5:
select sha2('123456',256) ;  -- 8d969eef6ecad3c29a3a629280e686cf0c3f5d5a86aff3ca12020c923adc6c92
select md5('123456');  -- e10adc3949ba59abbe56e057f20f883e

# 将一个无分区的表table1的数据导入到另一个分区表table2；
insert into db_name.table2
partition (y='2021',m='03',d='15')
select party_id, mobile, data_source, data_upd
from db_name.table1;

# 重命名表，把 employee 修改为 emp。
hive> ALTER TABLE db_name.employee RENAME TO db_name.emp;

# 拷贝表结构，而不拷贝数据（用like）
create table if not exists mydb.mytable like mydb.mytable2;

# 拷贝表结构，且拷贝数据（用as）
create table if not exists mydb.mytable as
select * from mydb.mytable2;

