1、原代码使用正常，改成 Server来使用的时候，遇到了这个错误：
TypeError: Cannot interpret feed_dict key as Tensor: Tensor Tensor("Placeholder_4:0", shape=(50, 3), dtype=float32) is not an element of this graph.
错误原因：
改完成 server之后，load model是在实例化我的调用mask rcnn的类的时候进行的，然而inference是在接收到request的时候才进行，显然不在一个进程里。而那个写成subscriber的版本，他们是在同一个进程里的，subscribe的图片不断的写入一个类成员变量里，这里利用了python多线程中互斥锁确保不会同时读写这个变量，然后就可以让model对当前的图片进行inference了:
# Right after loading or constructing your model, save the TensorFlow graph:
graph = tf.get_default_graph()
# In the other thread (or perhaps in an asynchronous event handler), do:
global graph
with graph.as_default():
    (... do inference here ...)

...
        self._model.load_weights(model_path, by_name=True)
        self.graph = tf.get_default_graph()
...
        # Run detection
        with self.graph.as_default():
            results = self._model.detect([np_image], verbose=0)
...

# 有时候docker镜像在某个机器上可以运行，但迁移到其他机器上却不能运行，如报错：
illegal instruction (core dumped)
这个时候，可能是依赖的tensorflow的版本太高，而对应的机器不支持；可以通过降低对应的tensorflow的版本来解决。
pip3 uninstall tensorflow
pip3 install tensorflow==1.5.0

# 训练时报错： TypeError: Using a `tf.Tensor` as a Python `bool` isnot allowed. Use `if t is not None:` instead of `if t:` to test if a tensor isdefined, and use TensorFlow ops such as tf.cond to execute subgraphsconditioned on the value of a tensor.
这里的原因是tensorflow的tensor不再是可以直接作为bool值来使用了，需要进行判断。

如：if grad: 改为  if grad is not None:

# 在GPU上运行报错：
ImportError: libcuda.so.1: cannot open shared object file: No such file or directory

解决方案：
vi ./root/.bashrc
添加：
export LD_LIBRARY_PATH="$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-10.0/compat"
export CUDA_HOME=/usr/local/cuda
执行：
source ./root/.bashrc

# 在GPU机器上docker容器中未能使用GPU
2019-07-15 18:49:56.276452: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2019-07-15 18:49:56.276526: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:155] no NVIDIA GPU device is present: /dev/nvidia0 does not exist
将docker run 改成nvidia-docker run，这样才可以成功使用tensorflow
或者：nvidia-docker-compose -f docker-compose.yml up -d

# docker run报错：
nvidia-docker run -it -d --rm tensorflow/tensorflow:1.13.1-gpu-py3
5604b3af8eb62eda5936596d0cb4b1c7f29d69ae023c73ae57e686b57a7360b7
docker: Error response from daemon: OCI runtime create failed: container_linux.go:348: starting container process caused "process_linux.go:402: container init caused \"process_linux.go:385: running prestart hook 1 caused \\\"error running hook: exit status 1, stdout: , stderr: exec command: [/usr/bin/nvidia-container-cli --load-kmods configure --ldconfig=@/sbin/ldconfig.real --device=all --compute --utility --require=cuda>=10.0 brand=tesla,driver>=384,driver<385 brand=tesla,driver>=410,driver<411 --pid=25526 /data/docker_storage/docker/aufs/mnt/e411b26804cffbb97e5a5af91e7bc939866c423c4d47ada986f99219e6c62874]\\\\nnvidia-container-cli: requirement error: invalid expression\\\\n\\\"\"": unknown.

可能是tensorflow版本与系统不兼容：
nvidia-docker run -it -d --rm tensorflow/tensorflow:1.10.0-gpu-py3
TensorFlow 1.13及更高版本的GPU版本（包括最新标签）需要支持CUDA 10的NVidia驱动程序。请参阅NVidia的支持映射表：https://docs.nvidia.com/deploy/cuda-compatibility/index.html#binary-compatibility__table-toolkit-driver

# ''tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[?]'' 错误分析
这是tensorflow 一个经常性错误，错误的原因在于：显卡内存不够。
解决方法就是降低显卡的使用内存，途径有以下几种措施:
1 减少Batch 的大小
2 分析错误的位置，在哪一层出现显卡不够，比如在全连接层出现的，则降低全连接层的维度，把2048改成1042啥的
3 增加pool 层，降低整个网络的维度。
4 修改输入图片的大小

用bert是会报错：
tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[11,1] = 5803 is not in [0, 5803)
主要是因为embedding的个数不对；
Embedding(input_dim=self.vocab_size, output_dim=self.embedding_size, name='Embedding-Token')
改为： Embedding(input_dim=self.vocab_size+1, output_dim=self.embedding_size, name='Embedding-Token')

用dropout有时候报错：
TypeError: dropout() got an unexpected keyword argument 'rate'
解决方法及原因：
rate在TensorFlow 1.13中是该函数的有效参数，但在早期版本中无效

问题：
AttributeError: module 'tensorflow' has no attribute 'batch_gather'
解决方法，更改 tensorflow 版本，比如更新为 version:1.12.0

为什么服务器上面有gpu, 写的代码也支持GPU,但实际运行起来却没有使用GPU:
出现这种情况除了要检查支持GPU使用的代码是否正常外，还得检查tensorflow的版本与CUDA的版本是否配对。
用如下代码可检测tensorflow的能使用设备情况：
from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())　
若版本不配套的话，输出结果可能是仅包括：name: "/device:CPU:0"
若GPU可用，其输出结果可能是包括，name: "/device:GPU:0"，name: "/device:GPU:1"
`nvidia-smi` 命令 或者 `nvcc --version` 命令可以查看对应的CUDA版本。
查看cudnn版本:
cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2
安装指定版本tensorflow:
pip3 install tensorflow-gpu==2.0.0

版本	Python 版本	编译器	构建工具	cuDNN	CUDA
tensorflow-2.1.0	2.7、3.5-3.7	GCC 7.3.1	Bazel 0.27.1	7.6	10.1
tensorflow-2.0.0	2.7、3.3-3.7	GCC 7.3.1	Bazel 0.26.1	7.4	10.0
tensorflow_gpu-1.14.0	2.7、3.3-3.7	GCC 4.8	Bazel 0.24.1	7.4	10.0
tensorflow_gpu-1.13.1	2.7、3.3-3.7	GCC 4.8	Bazel 0.19.2	7.4	10.0
tensorflow_gpu-1.12.0	2.7、3.3-3.6	GCC 4.8	Bazel 0.15.0	7	9

有时候，通过pip3 list|grep tensorflow查出来的 tensorflow 和tensorflow-gpu 与 cuDNN, CUDA 版本都是配对的，但是代码运行的时候，就是不使用GPU;
问题可能原因：可能是因为tensorflow比tensorflow-gpu后安装，虽说两个的版本号一样，但程序默认使用了后面安装的cpu版本的。
解决方案：将 tensorflow卸载掉；或者将tensorflow-gpu卸载掉并重新安装相同的版本tensorflow-gpu；

在tf 1.12上运行正常，在1.14版本运行报错：
    early_stopping_hook = tf.contrib.estimator.stop_if_no_decrease_hook(
    AttributeError: module 'tensorflow.contrib.estimator' has no attribute 'stop_if_no_decrease_hook'

解决方案：
在1.14版本中 应该是
tf.estimator.experimental.stop_if_no_decrease_hook

# 问题：
bertmodel = BertModel.from_pretrained(rf'D:\Users\{USERNAME}\data\bert_base_pytorch\bert-base-chinese',from_tf=True)# load the TF model for Pytorch
加载预训练模型时，报错：
AttributeError: module 'tensorflow._api.v1.initializers' has no attribute 'TruncatedNormal'
解决方法卸载旧版本，更新到新版本，如：
Successfully uninstalled tensorflow-1.14.0
Successfully installed tensorflow-2.5.0

