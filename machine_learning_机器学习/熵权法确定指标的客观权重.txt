
熵值法，也叫熵权法，是由数据离散程度分配权重，和信息熵并无本质的联系，本质上是对数据离散程度的表达。
其计算过程：
(1) 选取n个样本，m个指标，则Xij为第i个样本的第j个指标的数值。（i=1,2…,n; j=1,2,…,m）
(2) 指标的标准化处理:异质指标同质化
由于各项指标的计量单位并不统一,因此在用它们计算综合指标前,我们先要对它们进行标准化处理,即把指标的绝对值转化为相对值,从而解决各项不同质指标值的同质化问题。
而且,由于正向指标和负向指标数值代表的含义不同(正向指标数值越高越好,负向指标数值越低越好) ,因此,对于高低指标我们用不同的算法进行数据标准化处理。
其具体方法如下:
正向指标: 该取值的大小除以该指标所有取值的总和
负向指标: 先取倒数，再按正向指标同样处理
（3）按照（2）即可计算第 j 项指标下第 i 个样本占该指标的比重 Pij。
（4）计算第 j 项指标的熵值 -∑Pij*ln(Pij)。① 此处的熵值，跟信息熵、热力学熵等没有关系，仅仅是字相同而已。本质上是对数据离散程度的表达。而且，离散程度越大，熵值越小，差异系数越大，权值越大。
（5）计算第j项指标的差异系数(差异系数=1-熵值)。对第j项指标，指标值的差异越大，对方案评价的左右就越大。
（6）求权值，即当前指标的差异系数除以所有指标差异系数的总和。
（7）计算各样本的综合得分，即针对某个样本，将（3）中各个指标值与权值相乘再求和。

熵权法的基本思路是根据指标变异性的大小来确定客观权重。
在熵值法中，某列(指标)的行数据（样本）间差异性越小，则熵值越大，若全部数值相等则熵值取得最大值1，此时指标对评价对象不起作用；
反之，某列(指标)的行数据间差异性越大、熵值越小，则该指标对评价对象的比较作用越大。

在信息学里信息量大代表着数据离散范围小，不确定性小。信息熵代表着信息传递的不确定性的大小，所以在信息学上，使用状态概率对香农公式算出来的，在信息学上叫做信息熵。
在熵权法中使用取值相对大小对香农公式计算出来的，叫做熵值，虽说里头有个熵字但跟信息熵、热力学熵没有关系，它的本来含义是指一个确定无疑的信息源发送出来的信息，受到干扰以后，衡量偏离了原始精确信息的程度。
离散度越大，计算得这个值越小，则收到的信息越不可靠，得到的信息越小。这个值越大，则收到的信息越可靠，得到的信息越多。
对于统计领域来说，如果一组指标项，对其他事物有影响，并且影响度与离散度成强相关的正比或反比关系，那么就认为所含信息量大，是个重要指标项。
因此在统计学中，具体来说在熵权法中，离散范围越大的数列（指标），计算出来的熵值越小，信息量越大，而权重应该越大，熵值与权重大小正负向不统一。
所以信息熵权重法计算出来熵值以后要用1去减熵值，以使熵值与权重正负向统一起来。

信息学：离散度越大，不确定性大，信息熵越大，所含信息量越小；
熵权法：离散度越大，熵值越小，差异系数越大，权重越大；

信息熵计算公式中，p是指标 j 中值为 i 的样本数占总样本数量的比例，混乱程度越大，方差越大，熵越大，包含的信息越多，权重应当越大。
而熵权法计算熵的公式中，p不是各取值的比例，而是各个取值的相对大小（先归一化，对负向指标正向化处理，该取值的大小除以该指标所有取值的总和），方差越大，熵越小，包含的信息越多，权重应当越大。
在熵权法计算熵的过程中，为何要取值的相对大小替代概率，是因为对于数值型数据，比如学生考试分数，统计概率的话，看不出对于的分数差距，所以才用每个学生的分数除以总分数。
实际上这个时候计算出来的熵值，不应该叫信息熵。
信息率：衡量信号携带信息效率的度量，是整个信息的信息量除以信号数量，当均分时，信息量最大，所以我们就定义信息率最高为1。
冗余率：和信息率互补的的概念是冗余率，信息率越高，冗余率越低。信息率为r，冗余率为1-r。当信息源的分布是均分时，信息率最高。
在熵权法中，信息源的分布是均匀时的权重应该为0，所以我们不以信息率表示权重，用冗余率表示权重，因为均分时，虽然信息率最高，但正好这时我们想让其权重为0，而正好，冗余率为0。

自信息（self-information），用来衡量单一事件发生时所包含的信息量多寡。它的单位是bit,或是nats。自信息的含义包括两个方面：
1.自信息表示事件发生前，事件发生的不确定性。
2.自信息表示事件发生后，事件所包含的信息量，是提供给信宿的信息量，也是解除这种不确定性所需要的信息量。
自信息表示某一事件发生时所带来的信息量的多少，当事件发生的概率越大，则自信息越小；
某一事件发生的概率非常小，但是实际上却发生了(观察结果)，则此时的自信息非常大；某一事件发生的概率非常大，并且实际上也发生了，则此时的自信息较小。
互信息：一个随机事件包含另一个随机事件的度量。

信息量：信息量是度量弄清楚一个未知事物需要查询的信息的多少，单位是比特。
随机变量取某个值时，其概率倒数的对数log2(1/p)=-log2(p) (其中底数可以是2，单位是比特，也可以自然对数e为底或者是以10为底，得到的单位也不同，分别是nat和Hart)就是信息量。
通俗的说就是，事物所含信息量与其发生的概率负相关。
一件事物出现的概率决定了它的不确定性大小，也就决定了所含信息量的大小。出现的概率越大，不确定性越小，所含信息量也就越小。

信息是用来消除随机不确定性的东西。一条信息的信息量大小和其不确定性有直接的关系。越小概率发生的事件产生的信息量越大。

当信息被拥有它的实体传递给接收它的实体时，仅当接收实体不知道信息的先验知识时信息才得到传递。信息量度量的是一个具体事件所带来了多少信息。
如果接收实体事先知道了消息的内容，这条消息所传递的信息量就是0。
一个具体事件所产生的信息量随着发生概率增加而递减。信息量为非负值。

自信息量是与概率空间中的单一随机事件出现后所带来的信息量。一个随机产生的事件所包含的自信息数量，只与事件发生的几率相关。
事件发生的几率越低，在事件真的发生时，接收到的信息中，包含的自信息越大。自信息量常用事件发生概率的负对数来表示。

事件发生概率越小——不确定性越大——所含信息量越大
事件发生概率越大——不确定性越小——所含信息量越小
————————————————
信息熵——接收者对某件事物已经存在的不确定性的程度；
信息——接收者未来消除对某件事物已经存在的不确定性的东西；
信息量——接收者未来消除对某件事物已经存在的不确定性所需要的东西的总量。
信息熵越小，提供的信息量越大。这里面的关键词是“提供的”，其含义就是过去时——已经提供的。
而且，这个“信息”显然也不是信息论“还能说什么”的“信息”，而是我们日常中所指的“已经说了什么”的“信息”。
信息熵越小，不确定性越低，说明已经存在的确定性越高，也就是已经获得的消除不确定性的信息量越大。
依此逻辑，“信息熵越大，信息量越小”这个说法也就顺道讲通了，其实指的就是，已经获得的消除不确定性的信息量越小。

信息熵是在随机事件的结果出来之前，对随机事件可能产生的信息量的期望。
即考虑随机事件所有出现情况的发生概率，换句话说，考虑该随机变量的所有取值。熵只取决于随机变量的分布，与随机变量的取值无关。

信息熵是信息量的期望，它不是针对每条信息，而是针对整个不确定性结果集而言的，信息熵越大，事件不确定性就越大，单条信息只是或多或少的影响着结果集概率的分布。

信息熵：信息熵也就是信息量的期望。可以把信息熵理解成不确定性的大小，不确定性越大，信息熵也就越大。

信息熵其表示的是某一事件的不确定性，而信息的作用就是用于降低这种不确定性，其中输入的信息量就等于该事件不确定性减少的大小，也就是熵减少的大小，所以熵本身不是对信息量的一个度量，而是对事件不确定性的一个度量，而熵减才是对信息量的度量。
信息熵本身不是衡量信息量的，而是对某一事件的不确定性的衡量，信息量的输入导致熵减少，才是对信息量的衡量。

在信息论中，信息是系统有序程度的一个度量，熵是系统无序程度的一个度量。二者绝对值相等，方向相反。

熵越大混乱程度越大。如果相空间所有态中只有一个态的概率是1，其它态概率都是0，那这个系统的熵是0，完全没有混乱度，系统只能取这个态。
反之，当所有态概率相等的时候熵最大，这个时候系统对取什么态没有偏向性，所以混乱度最大。
信息熵越大信息量越大。因为熵越大的系统承载信息的能力越大。一个熵为0的系统只能取一个态，当然承载不了任何信息。

如果一个宏观态对应的微观态越多，这个事件的不确定性就越大，信息熵就越大，尤其是在所有可能性等概率的情况下，信息熵最大。
信息熵越大，信息容量越大，与信息量无关。
（信息量为消除不确定性后的熵差；信息熵越大，可容许的熵差（即信息容量）越大）
熵用来评估随机事件的不确定程度，熵值越大，则对应的事件的不确定性越大，以明天是否下雨这一随机事件为例，如果下不下雨的概率都为50%，那事件的不确定性是最大，因此根据熵值计算公式得到熵值也是最大的，为0.6931。
信息熵计算公式中，p是指标 j 中值为 i 的样本数占总样本数量的比例，熵越大，信息量越大。而熵权法计算熵的公式中，p不是各取值的比例，而是各个取值的相对大小，熵越小，信息量越小。

任何信息都存在冗余，冗余大小与信息中每个符号（数字、字母或单词）的出现概率或者说不确定性有关。
信息冗余度一“信息剩余度”。是指一定数量的信号单元可能有的最大信息量与其包含的实际信息量之差。

如果指标的信息熵越小，该指标提供的信息量越大， 在综合评价中所起作用理当越大，权重就应该越高。因此，可利用信息熵这个工具，计算出各个指标的权重，为多指标综合评价提供依据。
信息量和信息熵只是在量上面相等。信息是为了消除不确定性，而需要消除不确定性的信息量和信息熵的大小相等。
熵是对不确定信息的度量，如果一个指标的信息熵越小，则熵减越大，该指标提供的信息量越大，在综合评价中所起的作用理应越大，权重就应该越高。

概率为1的确定事件的信息熵为0（注意：不是信息量）。

信息量是对信源发出的某一个信号所含信息的度量，信息熵是对一个信源所含信息的度量，也就是信息量的期望。

熵是对不确定信息的度量，如果一个指标的信息熵越小，该指标提供的信息量越大，在综合评价中所起的作用理应越大，权重就应该越高。

熵是对不确定性的一种度量。不确定性越大，熵就越大，包含的信息量越大；不确定性越小，熵就越小，包含的信息量就越小。

根据熵的特性，可以通过计算熵值来判断一个事件的随机性及无序程度，也可以用熵值来判断某个指标的离散程度，指标的离散程度越大，该指标对综合评价的影响（权重）越大。比如样本数据在某指标下取值都相等，则该指标对总体评价的影响为0，权值为0.

1 熵权法的优点
熵值法是根据各项指标指标值的变异程度来确定指标权数的，这是一种客观赋权法，避免了人为因素带来的偏差。
相对那些主观赋值法，精度较高客观性更强，能够更好的解释所得到的结果。

2 熵权法的缺点
·忽略了指标本身重要程度，有时确定的指标权数会与预期的结果相差甚远，同时熵值法不能减少评价指标的维数，也就是熵权法符合数学规律具有严格的数学意义，但往往会忽视决策者主观的意图；
·如果指标值的变动很小或者很突然地变大变小，熵权法用起来有局限



import numpy as np
import pandas as pd
#文件路径
fp="d:/shangquan.xlsx"
#参数自行调参
data=pd.read_excel(fp,index_col=None,header=None,encoding='utf8')
#标准化数据，这里一行代码处理简直玄学
data = (data - data.min())/(data.max() - data.min())
m,n=data.shape
#第一步读取文件，如果未标准化，则标准化
data=data.as_matrix(columns=None)
#将dataframe格式转化为matrix格式
k=1/np.log(m)
yij=data.sum(axis=0)
pij=data/yij
#第二步，计算pij
test=pij*np.log(pij)
test=np.nan_to_num(test)
ej=-k*(test.sum(axis=0))
#计算每种指标的信息熵
wi=(1-ej)/np.sum(1-ej)
#计算每种指标的权重
————————————————
原文链接：https://blog.csdn.net/weixin_43868437/article/details/108020046

