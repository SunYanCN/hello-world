
# 预处理
```python
df_origin = pd.read_csv("原始数据.txt", encodings='utf-8')
df_origin['贷款期数'].value_counts()
36      2044354
20期      175738
36.0     166771
无期限       98107
24        54565
12        28660
24.0       3958
12.0       2322
60         1682
1           766
6           523
60.0        450
30期         147
10期          30
42           29
18           22
30            9
5             4
1.0           3
8             3
3             2
10            2
40            1
15            1
Name: 贷款期数, dtype: int64

# 无限期设置为10年，其他设置为整数
df_origin['贷款期数'] = [360 if t=='无期限' else (int(float(t.replace('期', ''))) if isinstance(t, str) else t) for t in df_origin['贷款期数'].values]
df_origin['贷款期数'].value_counts()
36.0     2211125
20.0      175738
360.0      98107
24.0       58523
12.0       30982
60.0        2132
1.0          769
6.0          523
30.0         156
10.0          32
42.0          29
18.0          22
5.0            4
8.0            3
3.0            2
40.0           1
15.0           1
Name: 贷款期数, dtype: int64

```

# 展示数值特征分布
```python
import matplotlib.pyplot as plt
from pylab import mpl
mpl.rcParams['font.sans-serif'] = ['SimHei'] #指定默认字体   
mpl.rcParams['axes.unicode_minus'] = False #解决保存图像是负号'-'显示为方块的问题
from datetime import datetime
import seaborn as sns
from scipy import stats
columns = ['贷款期数', '年化利率', '账龄', '逾期天数']
fig = plt.figure(figsize=[30, 100])

# KS检验
# Kolmogorov-Smirnov检验是基于累计分布函数的，用于检验一个分布是否符合某种理论分布或比较两个经验分布是否有显著差异。
def sub_ax(ax, idx, col):
    train_feat = [t for t in df_2022[col].values]
    test_feat = [t for t in df_2021[col].values]
    sns.kdeplot(train_feat, shade = True, color='r', label = '2022')
    sns.kdeplot(test_feat, shade = True, color='b', label = '2021')
    KS_statistic, p_value = stats.ks_2samp(train_feat, test_feat)
    ax.legend(fontsize=None)
    # 如果两个分布相同，则ks检验统计量将为0，p值为1。
    # p值越小越有理由相信小概率事件发生了，即认为两个分布不相似
    ax.set_title("{}(KS统计量:{:.2f},p值:{:.4f})".format(col, KS_statistic, p_value))

for idx, col in enumerate(columns, 0):
    ax = fig.add_subplot(2, 2, idx+1)
    sub_ax(ax, idx, col)
    
save_image_name = "./result/images/{}.jpg".format(time.strftime('%Y%m%d%H%M%S',time.localtime(time.time())))
fig.savefig(save_image_name, dpi=500,bbox_inches = 'tight')
print(save_image_name)
fig.show()
```

# 常用的特征变换操作
1. 数值型特征的常用变换  
(1) 特征缩放  
* Min-Max标准化 ∈[0,1]   
`XNorm = (x-min(xi))/(max(xi)-min(xi))`

* 缩放到区间[-1, 1]  
`Xnorm = (x-mean(xi))/(max(xi)-min(xi))`

* Z-score标准化   
`Xnorm = (x-mean(xi))/std(x)`  
这种方法基于原始数据的均值(mean)和标准差(standard deviation)进行数据的标准化。
z-score标准化方法适用于最大值和最小值未知的情况，或有超出取值范围的离群数据的情况。

* Log-based  
`Xlog = log(1+x)`  
`xlog-norm = (Xlog - mean(Xlog))/std(Xlog)`  

* L2-normalize  
`Xnorm = xi/(math.sqrt(sum(xi*xi)))`  

* Gauss-Rank  
首先我们对数据的统计值做一个排序，从小到大或从大到小都可以，得到数据的Rank值，
然后将Rank值缩放到(-1,1)区间，最后调用erfinv逆误差函数，就可以将变换后的Rank值分布调整为高斯分布（正态分布）。
但是代价是只保留了数据的排序信息。
```python
# erfinv函数曲线
t = np.random.randint(-99, 99, 5000)/100
plt.scatter(t, [erfinv(i) for i in t], marker='.')
```

* Robust scaling  
`Xxcaled = (x-median(x))/IQR`  
其中median(x)是x的中位数;  
四分位距（interquartile range, IQR），又称四分差，等于样本中75%分位点的值减去25%分位点的值。
经过Robust scaling变换，数据中较小的值依然有一定的区分性。
然而，对于这种存在异常值的数据，最好的处理方法还是提前将异常值识别出来，然后对其做删除或替换操作。

```python
import numpy as np
from scipy.special import erfinv 

def scale_minmax(x):
    '''归一化，缩放到区间[0, 1]'''
    return (x - x.min()) / (x.max() - x.min())

def scale_norm(x):
    '''缩放到区间[-1, 1]'''
    return (x - x.mean()) / (x.max() - x.min())

def Z_score(x):
    '''标准化, 经过变换处理后数据的均值为0，标准差为1'''
    return (x - x.mean()) / x.std()

def log_based(x):  
    x_log = np.array([math.log(1+i) for i in x])  
    return (x_log - x_log.mean())/x_log.std()

def L2_normalize(x):
    """除以平方和的开平方"""
    return x/np.sqrt(np.square(x).sum())

def scale_rankgauss(x, epsilon=1e-6): 
    '''rankgauss
    处理过后，数据均值为0，符合正态分布,即大多数值为0，＞0或＜0的数均递减'''
    x = x.argsort().argsort() # 排序,转换为每个数值的排序号
    x = (x/x.max()-0.5)*2 # 缩放到区间[-1, 1]
    x = np.clip(x, -1+epsilon, 1-epsilon)  # 调整极端值, 即若＞1-epsilon，则置为1-epsilon， 若＜-1+epsilon，则置为-1+epsilon
    x = erfinv(x) # erfinv(1) 等于正无穷，erfinv(-1)等于负无穷。
    return x

def Robust_scaling(x):
    """减去中位数除以IQR"""
    return (x-np.median(x))/(np.percentile(x,75)-np.percentile(x,25))

x = np.random.randint(0, 100, 1000)
# x = np.array([random.lognormvariate(0, 1) for _ in range(1000)])

x_minmax = scale_minmax(x)
x_norm = scale_norm(x)
x_z = Z_score(x)
x_log = log_based(x)
x_l2 = L2_normalize(x)
x_rankgauss = scale_rankgauss(x)
x_robust_scaling = Robust_scaling(x)

fig = plt.figure(figsize=[30, 100])
result_list = [
    ('原始', x),
    ('min-max', x_minmax),
    ('Scale to [-1,1]', x_norm),
    ('Z-score', x_z),
    ('log based', x_log),
    ('l2', x_l2),
    ("gauss rank", x_rankgauss),
    ("Robust scaling", x_robust_scaling)
]
def sub_ax(ax, name, data):
    ax.hist(data, bins=50)
    ax.set_title(name)

for idx, (name, data) in enumerate(result_list, 1):
    ax = fig.add_subplot(3, 3, idx)
    sub_ax(ax, name, data)
fig.show()
```

* 注意：当存在异常值时，除gauss rank、Robust scaling特征变换方法外，
其他的特征缩放方法都可能把转换后的特征值压缩到一个非常狭窄的区间内，从而使得这些特征失去区分度。

(2) 对异常值的处理  
数据中存在异常值时，除gauss rank、Robust scaling特征变换方法外，
用Z-score、Min-max这类特征缩放算法都可能会把转化后的特征值压缩到一个非常窄的区间内，从而使这些特征失去区分度。
经过Robust scaling变换， 数据中较小的值依然有一定的区分性。
然而，对于这种存在异常值的数据，最好的处理方法还是提前将异常值识别出来，然后对其做删除或替换操作。

(3) 分箱处理(Binning)  
分箱就是将连续的特征离散化，以某种方式将特征值映射到几个箱(bin)中。  
做特征分箱作用：   
  * 引入非线性变换，增强模型性能。因为原始值和目标值之间可能并不存在线性关系，所以直接使用模型预测起不到很好的效果。
  * 增强模型可解释性。通过分箱可以得到一个分段函数，模型可解释性更强。
  * 对异常值不敏感，防止过拟合。异常值最终也会被分到一个箱里面，不会影响其他箱内正常特征值，分箱的在一定程度上也可以防止过拟合。
  * 可以对不同的箱做进一步的统计和特征组合。

注：若特征存在多个子类别或用户分组，要先按照分组后再做分箱，不建议全局做分箱。  
并且我们分箱后要保存的应该是箱号，而不是具体范围值。  

2. 类别型特征的常用变换  
   (1) 交叉组合   
单特征区分性不强时，可以尝试组合不同特征  
   (2) 分箱处理  
类别型的特征有时候也是需要做分箱的，尤其是存在高基数特征时，不做分箱处理会导致高基数特征相对于低基数特征处于支配地位，并且容易引入噪音，导致模型过拟合。  
类别型特征的分箱方法通常有以下3种：
   * 基于业务理解。例如对userID分箱时可以根据职业划分，也可以根据年龄段来划分。
   * 基于特征的频次合并低频长尾部分(Back off)。
   * 基于决策树模型。  
将某一列数据作为训练集，将label作为结果，直接训练一个决策树，然后根据决策树的分裂节点的阈值作为分箱的依据。  
```python
import pandas as pd
import numpy as np
from sklearn.datasets import make_classification
from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree

def decision_tree_binning(x_value: np.ndarray, y_value: np.ndarray, max_bin=10) -> list:
    '''利用决策树获得最优分箱的边界值列表
    max_bin: 目标分箱数
    '''
    
    clf = DecisionTreeClassifier(
        criterion='entropy',  # “信息熵”最小化准则划分
        max_leaf_nodes=max_bin,  # 最大叶子节点数，即分箱数
        min_samples_leaf=0.05)  # 叶子节点样本数量最小占比
    clf.fit(x_value.reshape(-1, 1), y_value)  # 训练决策树

    # 绘图
    plt.figure(figsize=(14, 12))  # 指定图片大小
    plot_tree(clf)
    plt.show()

    # 根据决策树进行分箱
    n_nodes = clf.tree_.node_count  # 决策树节点
    children_left = clf.tree_.children_left
    children_right = clf.tree_.children_right
    threshold = clf.tree_.threshold

    # 开始分箱
    boundary = []
    for i in range(n_nodes):
        if children_left[i] != children_right[i]:  # 获得决策树节点上的划分边界值
            boundary.append(threshold[i])

    boundary.sort()

    min_x = x_value.min()
    max_x = x_value.max()
    # max_x = x_value.max() + 0.1  # +0.1是为了考虑后续groupby操作时，能包含特征最大值的样本
    boundary = [min_x] + boundary + [max_x]
    return boundary


data_x, data_y = make_classification(n_samples=10000, n_classes=4, n_features=10, n_informative=8, random_state=0)  
bin_result = decision_tree_binning(data_x[:, 0], data_y, max_bin=6)  
bin_value = pd.cut(data_x[:, 0], bin_result).codes  # 分箱的结果  
```
  
(3) 统计编码   
* Count Encoding  
统计某类别型特征发生的频次。一般需要做特征变换后才能输入给模型，建议的特征变换操作包括Gauss Rank、Binning。

* Target Encoding  
统计某类别特征的目标转化率。如目标是点击就统计点击率，目标是成交就统计购买率。    
同时目标转化率需要考虑置信度问题，比如10次浏览有5次点击和1000次浏览500次点击置信度是不一样的，  
所以对于小的点击次数我们需要用全局的点击率做一个平滑处理（最简单的方法就是：在分子与分母上加上一个常数）。  
拿全局或者分组的平均转化率当当前特征的转化率做一个平滑，公式如下:  
当前类别平滑转化率=(当前类别点击数*当前类别转化率+平滑常数*总转化率)/(当前类别点击数+平滑常数)  
如：两个展示次数分别为10次、1次，转化次数分别为8次、1次的两个商品A、B，假设平均转化率为0.32，这里取平滑常数为：1。  
样本A、B平滑之前的CTR（Click-Through-Rate，即点击通过率）为0.8、1，而我们显然对样本A的信心更足一些，  
因为样本B受随机因素干扰的可能性更大，而样本A的展示次数较多的情况仍然有很高的转化率。  
平滑之后CTR为0.75、0.66;计算详情：(10*0.8+1*0.32)/(10+1)、(1*1+1*0.32)/(1+1)  

* Odds Ratio
优势比(odds ratio；OR), 可以用来度量用户对某一类目商品的偏好程度相对于其他类目是什么样的水平。  
优势比是当前特征取值的优势（odds）与其他特征取值的优势（odds）的比值，公式为：    
θ = (取值A的点击率/取值A的未点击率)/(非取值A的点击率/非取值A的未点击率) = (p1/(1-p1))/(p2/(1-p2))  
优势比的值有可能会非常小或非常大（例如，可能会有几乎不会点击的用户），  
因此我们可以对其进行简化，并加一个log转换使得除法变成减法：ln(取值A的点击率/取值A的未点击率) - ln(非取值A的点击率/非取值A的未点击率)。  

* Weight of Evidence  
度量某类别特征不同分箱下取值与目标的相关程度。值为正表示正相关，值为负表示负相关。  
WOE = ln(Event%/NonEvent%)  
假设某个特征共有5个取值，各取值点击次数分别为：150, 120, 110, 100, 50; 未点击次数分别为：1050, 780, 980, 1360, 1360  
[math.log( k/v) for k, v in zip([i/530 for i in [150, 120, 110, 100, 50]], [i/5530 for i in [1050, 780, 980, 1360, 1360]])]  
计算得特征各个取值对应的WOE值分别为：0.3992, 0.4733, 0.158, -0.265, -0.9582  

3. 时序特征  
时序特征包括统计过去1天、3天、7天、30天的总行为数或行为转化率，还可以对当前值和历史值做差异比较。  
比如在电商场景下，商品的价格相较于之前是升高了还是降低了可作为用户购买意愿的重要特征。  
* 历史事件分时段统计  
   统计过去1天、3天、7天、30天的总（平均）行为数  
   统计过去1天、3天、7天、30天的行为转化率  
* 差异  
   环比、同比  
* 行为序列  
   需要模型配合  







