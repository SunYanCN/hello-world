
虽说模型最终的效果是由数据+模型，两个方面决定的。
但若通过提高模型能力来弥补糟糕的数据，往往会事倍功半。
遇到数据问题，最好的方法，还是针对性解决数据问题（而不是想着通过模型的弥补），往往会事半功倍。

问题1：数据量不够。
如果你的数据集过小，你的模型将没有足够多的样本，概括找到其中的特征，在此基础上拟合的数据，会导致虽然训练结果没太出错但是测试错误会很高。
解决方案1：收集更多数据。
您可以尝试找到更多的相同源做为您的原始数据集，或者从另一个相似度很高的源，再或者如果你绝对要来概括。
注意事项：这通常不是一件容易的事，需要投入时间和金钱。
此外，你可能想要做一个分析，以确定你需要有多少额外的数据。将结果与不同的数据集大小进行比较，并尝试进行推断。
模型数据集大小预估过程：
1、选择不同大小（如1k、5k、10k...500k等）数据集；
2、用不同大小数据集训练模型，并计算错误率；
3、以数据集大小为X轴，对应错误率为Y轴，作图；
4、根据图示，预估要达到逾期目标错误(Target Error)，需要的数据集大小。

解决方案2：对原始数据进行数据增强
这种技术可以创造奇迹，并以极低的成本生成大量额外的样本。
对应图像问题，您可以尝试裁剪，旋转，平移或缩放图像。您可以添加 噪点，模糊，改变颜色或阻挡部分噪音。
对于NLP问题，可以通过添加、插入、删除停用字词，同义句生成模型等方法，增加样本数据；
在所有情况下，您需要确保数据仍然代表同一个类。
数据增强，往往可以提供指数级的样本，但通常不如收集更多的原始数据。
当然，数据增强，不能改变对分类有重要意义的特征；因这时的数据增强将使模型更难找到区别特征。

问题2：低质量的分类
解决方案1：如果可能的话，花些时间浏览一下您的数据集，并验证每个样本的标签。
这可能需要一段时间，但在数据集中使用反例会对 学习过程产生不利影响。
解决方案2：为您的类选择正确的粒度级别。
根据问题，您可能需要更多或更少的类。例如，您可以使用全局分类器对小猫的图像进行分类，以确定它是动物，然后通过动物分类器运行它以确定它是小猫。
一个巨大的模型可以做到这两点，但它会更难。
使用具有专门分类器进行多级预测，往往把复杂问题简单化；当然，这样做基于的前提是复杂问题可拆卸为多个子问题。

问题3：低质量的数据
低质量数据往往只会导致低质量的结果。
数据集中的数据样本可能与您要使用理想的数据集相差太远。比如，数据混乱，标注错误等。
解决方案：删除最糟糕的样本。
这是一个漫长的过程，但会改善您的结果。
另一个常见问题是当您的数据集由与真实世界应用程序不匹配的数据组成时。例如，训练数据、生产测试数据来自完全不同的来源，二者分布差异巨大等。

解决方案：如果可能，尝试使用相同的工具查找/构建数据集。
使用不能代表生产环境的数据通常是一个坏主意。您的模型可能会提取在生产环境中无法使用的特征。

问题4：不平衡的分类
如果数每类样本的不是大致的相同的所有类，模型可能有利于多数类别的倾向，因为它会导致一个较低的 错误。
我们说该模型存在偏差，因为类分布是偏态的。这是一个严重的问题，也是您需要查看精度，召回或混淆矩阵的原因。
解决方案1：收集代表性不足的分类的更多样本。
然而，这在时间和金钱上通常 是昂贵的，或者根本不可行。
解决方案2：对数据进行过度/不足的采样。
对多样本类别数据进行欠采样，对少样本类别数据进行过采样；
这意味着您从过度表示的类中删除一些样本，或从代表不足的类中复制样本。比重复更好，使用数据增加。

问题5：数据格式不统一
如果您的数据没有特定 格式，或者值不在特定 范围内，则您的模型可能无法处理它。你将有形象，有更好的结果横宽比和像素值。
解决方案1：对数据进行操作，使其具有与其他样本相同的方面或格式；如：裁剪或拉伸数据，使其具有与其他样本相同的方面或格式。
解决方案2：规范化数据，使每个样本的数据都在相同的值域内。
将值范围标准化为在整个数据集中保持一致。

问题6：没有验证集和测试集
清理，扩充和正确标记数据集后，需要将其拆分。许多人通过以下方式将其拆分：80％用于训练，20％用于测试，这 使您可以轻松发现过度拟合。
但是，如果您在同一测试集上尝试多个模型，则会发生其他情况。通过选择具有最佳测试精度的模型，您实际上过度拟合了测试集。
发生这种情况是因为您手动选择的模型不是其内在模型 值，但其性能上的特定数据集。
解决方案：将数据集拆分为三个：训练集、验证集、测试集。
该屏蔽你的测试被设置过度拟合由模型的选择。选择过程变为：
1、在训练集上训练你的模型。
2、在验证集上测试它们以确保没有过拟合。
3、选择最有希望的模型。在测试集上测试它，这将为您提供模型的真实准确性。
注意：一旦您选择了生产模型，请不要忘记在整个 数据集上进行训练！数据越多越好！

#####################################################训练集、测试集分布不一致及其解决方法#####################################################################

训练集高分，测试集预测提交后发现分数很低，为什么？
有可能是训练集和测试集分布不一致，导致模型过拟合训练集。

训练集和测试集分布不一致也被称作数据集偏移(Dataset Shift)。

一、训练集、测试集分布一致与否的判断方法：
方法1. KDE (核密度估计)分布图当我们一想到要对比训练集和测试集的分布，便是画概率密度函数直方图，但直方图看分布有两点缺陷: 受bin宽度影响大和不平滑，
因此多数人会偏向于使用核密度估计图(Kernel Density Estimation, KDE)，KDE是非参数检验，用于估计分布未知的密度函数，
相比于直方图，它受bin影响更小，绘图呈现更平滑，易于对比数据分布。

对比训练集和测试集特征分布时，我们可以用seaborn.kdeplot()进行绘图可视化，样例图和代码如下:
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# 创建样例特征
# 这里，训练集、测试集各50个样本；
train_feat = np.array([ 0.04713164, -0.59420459,  0.72722759, -1.86262763,  0.42963151,
        0.3917658 , -1.22730558,  1.75013324,  0.59274965,  1.54499907,
        0.9147363 , -1.50721661, -1.29796081, -0.52339539,  2.35084833,
       -1.14783059, -1.42782487,  2.13030133,  0.22928721, -0.95961044,
        0.46814737,  0.34063791, -1.37728727, -0.22063062,  0.68970534,
       -1.69199731, -0.75036548,  1.05619284, -1.37444014, -0.54359649,
       -2.11986533, -0.67740148, -0.06680477, -0.81678172,  0.58055085,
       -0.35116691, -0.02172894,  0.23925486, -1.45888491,  0.50292242,
       -0.17959934,  1.08117023, -1.5685712 , -0.62939644, -0.51721599,
       -1.73181535, -0.14381824, -1.07963632,  0.39406808, -0.13741855])
test_feat = np.array([-0.4164219 ,  1.55810753,  0.33469117, -0.29775228,  1.40273725,
        1.48306334,  0.6713928 ,  1.41677773,  0.5923574 ,  0.70810561,
        0.24551261,  1.2662537 , -1.5662219 , -0.56213149,  2.20130365,
       -0.27538218, -0.8462891 , -0.09568602, -0.13302627, -0.83936246,
        1.45820911, -0.7151835 , -0.67849601, -1.51662033,  0.22995397,
       -0.07458576, -0.12958228,  1.56612344, -0.1838463 , -0.30392262,
        0.9056705 ,  0.11250935,  0.6521239 , -1.13566026,  0.05185993,
       -0.86696259,  1.57733915,  0.453453  , -1.06064948, -1.10412014,
        0.01647268,  2.23964178,  2.0706573 , -0.46758033,  1.89185956,
       -0.19476342,  1.6456598 , -0.79562172,  1.03897016,  0.59991084])

# 绘KDE对比分布
sns.kdeplot(train_feat, shade = True, color='r', label = 'train')
sns.kdeplot(test_feat, shade = True, color='b', label = 'test')
plt.xlabel('Feature')
plt.legend()
plt.show()

方法2.KS检验
KDE是PDF(概率分布函数Probability Distribution Function)来对比，而KS检验是基于CDF(累计分布函数Cumulative Distribution Function)来检验两个数据分布是否一致，它也是非参数检验方法(即不知道数据分布情况)。
调用scipy.stats.ks_2samp()可轻松得到KS的统计值(最大垂直差)和假设检验下的p值：
from scipy import stats
KS_statistic, p_value = stats.ks_2samp(train_feat, test_feat)
输出：KstestResult(statistic=0.22, pvalue=0.17858668181221732)
若KS统计值小且p值大，则我们可以接受KS检验的原假设H0，即两个数据分布一致。
上面样例数据的统计值较低，p值大于10%但不是很高，因此反映分布略微不一致。
注意: p值<0.01，强烈建议拒绝原假设H0，p值越大，越倾向于原假设H0成立。

方法3. 对抗验证
对抗验证的思路是：我们构建一个分类器去分类训练集和测试集，如果模型能清楚分类，说明训练集和测试集存在明显区别(即分布不一致)，否则反之。
具体步骤如下:
训练集和测试集合并，同时新增标签‘Is_Test’去标记训练集样本为0，测试集样本为1。
构建分类器(例如LGB, XGB等)去训练混合后的数据集(可采用交叉验证的方式)，拟合目标标签‘Is_Test’。
输出交叉验证中最优的AUC分数。AUC越大(越接近1)，越说明训练集和测试集分布不一致。

二、训练集、测试集分布一致的解决方法：
方法1. 构造合适的验证集
当出现训练集和测试集分布不一致的，我们可以试图去构建跟测试集分布近似相同的验证集，保证线下验证跟线上测试分数不会抖动，这样我们就能得到稳定的指标。
三种构造合适的验证集的办法：
(1) 人工划分验证集
以时间序列举例，因为一般测试集也会是未来数据，所以我们也要保证训练集是历史数据，而划分出的验证集是未来数据，不然会发生“时间穿越”的数据泄露问题，
导致模型过拟合(例如用未来预测历史数据)，这个时候就有两种验证划分方式可参考使用：
TimeSeriesSplit：Sklearn提供的TimeSeriesSplit。
固定窗口滑动划分法：固定时间窗口，不断在数据集上滑动，获得训练集和验证集。

除了时间序列数据，其它数据集的验证集划分都要遵循一个原则，即尽可能符合测试集的数据模式。

(2) 选择和测试集最相似的样本作为验证集
前面在讲对抗验证时，我们有训练出一个分类器去分类训练集和测试集，那么自然我们也能预测出训练集属于测试集的概率(即训练集在‘Is_Test’标签下预测概率)，
我们对训练集的预测概率进行降序排列，选择概率最大的前20%样本划分作为验证集，这样我们就能从原始数据集中，得到分布跟测试集接近的一个验证集了。
之后，我们还可以评估划分好的验证集跟测试集的分布状况，评估方法：将验证集和测试集做对抗验证，若AUC越小，说明划分出的验证集和测试集分布越接近(即分类器越分不清验证集和测试集)。

(3) 有权重的交叉验证
如果我们对训练集里分布更偏向于测试集分布的样本更大的样本权重，给与测试集分布不太一致的训练集样本更小权重，也能一定程度上，帮助我们线下得到不易抖动的评估分数。
在lightgbm库的Dataset初始化参数中，便提供了样本加权的参数weight。也可将对抗验证的分类器预测训练集的Is_Test概率作为权重即可。

方法2. 删除分布不一致特征
如果我们遇到分布不一致且不太重要的特征，我们可以选择直接删去这种特征。
虽然建议的是删除分布不一致但不太重要的特征，但有时避免不了碰到分布不一致但又很重要的特征，这时候其实就需要自行取舍特征分布和特征重要性的关系了

方法3. 修正分布不一致的特征输入
当我们对比观察训练集和测试集的KDE时，若发现对数据做数学运算(例如加减乘除)或对增删样本就能修正分布，使得分布接近一致，那么我们可以试试。
比如，特征在训练集中包含0、1和-1，而测试集只有1和0样本，因此可对训练集删去了特征值为-1的样本，减少该特征在训练集和测试集的差异。

方法4. 修正分布不一致的预测输出
除了对输入特征进行分布检查，我们也可以检查目标特征的分布，看是否存在可修正的空间。

方法5. 伪标签
伪标签是半监督方法，利用未标注数据加入训练。
伪标签最常见的方法是：使用有标注的训练集训练模型M;然后用模型M预测未标注的测试集;选取测试集中预测置信度高的样本加入训练集中;
使用标注样本和高置信度的预测样本训练模型M';预测测试集，输出预测结果。
可以看到，模型的训练引入了部分测试集的样本，这样相当于引入了部分测试集的分布。
但需要注意：
(1) 相比于前面的方法，伪标签通常没有表现的很好，因为它引入的是置信度高的测试集样本，这些样本很可能跟训练集分布接近一致，所以才会预测概率高。
因此引入的测试集分布也没有很不同，所以使用时常发生过拟合的情况。
(2) 注意引入的是高置信度样本，如果引入低置信度样本，会带来很大的噪声。
另外，高置信度样本也不建议选取过多加入训练集，这也是为了避免模型过拟合。

