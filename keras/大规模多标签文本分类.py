#!/usr/bin/env python
# coding=utf-8

import os
from tensorflow.keras import layers
from tensorflow.keras.layers import Embedding, Flatten
from tensorflow import keras
import tensorflow as tf

from sklearn.model_selection import train_test_split
from ast import literal_eval

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

USERNAME = os.getenv("USERNAME")
arxiv_data = pd.read_csv(
    # "https://github.com/soumik12345/multi-label-text-classification/releases/download/v0.2/arxiv_data.csv"
    rf"D:\Users\{USERNAME}\Downloads\arxiv_data.csv"
)

arxiv_data.head()

# arxiv_data.values[:3]
# Out[8]:
# array([['Survey on Semantic Stereo Matching / Semantic Depth Estimation',
#         'Stereo matching is one of the widely used techniques for inferring depth from\nstereo images owing to its robustness and speed. It has become one of the major\ntopics of research since it finds its applications in autonomous driving,\nrobotic navigation, 3D reconstruction, and many other fields. Finding pixel\ncorrespondences in non-textured, occluded and reflective areas is the major\nchallenge in stereo matching. Recent developments have shown that semantic cues\nfrom image segmentation can be used to improve the results of stereo matching.\nMany deep neural network architectures have been proposed to leverage the\nadvantages of semantic segmentation in stereo matching. This paper aims to give\na comparison among the state of art networks both in terms of accuracy and in\nterms of speed which are of higher importance in real-time applications.',
#         "['cs.CV', 'cs.LG']"],
#        ['FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Future Medical Imaging',
#         "The recent advancements in artificial intelligence (AI) combined with the\nextensive amount of data generated by today's clinical systems, has led to the\ndevelopment of imaging AI solutions across the whole value chain of medical\nimaging, including image reconstruction, medical image segmentation,\nimage-based diagnosis and treatment planning. Notwithstanding the successes and\nfuture potential of AI in medical imaging, many stakeholders are concerned of\nthe potential risks and ethical implications of imaging AI solutions, which are\nperceived as complex, opaque, and difficult to comprehend, utilise, and trust\nin critical clinical applications. Despite these concerns and risks, there are\ncurrently no concrete guidelines and best practices for guiding future AI\ndevelopments in medical imaging towards increased trust, safety and adoption.\nTo bridge this gap, this paper introduces a careful selection of guiding\nprinciples drawn from the accumulated experiences, consensus, and best\npractices from five large European projects on AI in Health Imaging. These\nguiding principles are named FUTURE-AI and its building blocks consist of (i)\nFairness, (ii) Universality, (iii) Traceability, (iv) Usability, (v) Robustness\nand (vi) Explainability. In a step-by-step approach, these guidelines are\nfurther translated into a framework of concrete recommendations for specifying,\ndeveloping, evaluating, and deploying technically, clinically and ethically\ntrustworthy AI solutions into clinical practice.",
#         "['cs.CV', 'cs.AI', 'cs.LG']"],
#        ['Enforcing Mutual Consistency of Hard Regions for Semi-supervised Medical Image Segmentation',
#         "In this paper, we proposed a novel mutual consistency network (MC-Net+) to\neffectively exploit the unlabeled hard regions for semi-supervised medical\nimage segmentation. The MC-Net+ model is motivated by the observation that deep\nmodels trained with limited annotations are prone to output highly uncertain\nand easily mis-classified predictions in the ambiguous regions (e.g. adhesive\nedges or thin branches) for the image segmentation task. Leveraging these\nregion-level challenging samples can make the semi-supervised segmentation\nmodel training more effective. Therefore, our proposed MC-Net+ model consists\nof two new designs. First, the model contains one shared encoder and multiple\nsightly different decoders (i.e. using different up-sampling strategies). The\nstatistical discrepancy of multiple decoders' outputs is computed to denote the\nmodel's uncertainty, which indicates the unlabeled hard regions. Second, a new\nmutual consistency constraint is enforced between one decoder's probability\noutput and other decoders' soft pseudo labels. In this way, we minimize the\nmodel's uncertainty during training and force the model to generate invariant\nand low-entropy results in such challenging areas of unlabeled data, in order\nto learn a generalized feature representation. We compared the segmentation\nresults of the MC-Net+ with five state-of-the-art semi-supervised approaches on\nthree public medical datasets. Extension experiments with two common\nsemi-supervised settings demonstrate the superior performance of our model over\nother existing methods, which sets a new state of the art for semi-supervised\nmedical image segmentation.",
#         "['cs.CV', 'cs.AI']"]], dtype=object)

print(f"数据集行数：{len(arxiv_data)};标签类别数：{len(set(sum([eval(t) for t in arxiv_data['terms'].values], [])))}")
# 数据集行数：51774;标签类别数：1099
len(arxiv_data["terms"].unique())
# Out[15]: 3157
# 有1789个标签，都只有一条记录；
sum(arxiv_data["terms"].value_counts() == 1)
# Out[17]: 1789

# 为了准备具有分层（分层采样）的训练集、验证集和测试集，需要删除掉仅有一条记录的数据；
arxiv_data_filtered = arxiv_data.groupby("terms").filter(lambda x: len(x) > 1)
arxiv_data_filtered.shape
# Out[19]: (49985, 3)

# 将标签字符串转换为列表
arxiv_data_filtered["terms"] = arxiv_data_filtered["terms"].apply(
    lambda x: literal_eval(x)
)
arxiv_data_filtered["terms"].values[:5]
# Out[21]:
# array([list(['cs.CV', 'cs.LG']), list(['cs.CV', 'cs.AI', 'cs.LG']),
#        list(['cs.CV', 'cs.AI']), list(['cs.CV']),
#        list(['cs.CV', 'cs.LG'])], dtype=object)

# 分层采样：
test_split = 0.1

# Initial train and test split.
train_df, test_df = train_test_split(
    arxiv_data_filtered,
    test_size=test_split,
    stratify=arxiv_data_filtered["terms"].values,
)

# Splitting the test set further into validation
# and new test sets.
val_df = test_df.sample(frac=0.5)
test_df.drop(val_df.index, inplace=True)

print(f"训练集的数量: {len(train_df)}")
print(f"验证集的数量: {len(val_df)}")
print(f"测试集的数量: {len(test_df)}")

# 多标签二值化
# 现在，我们使用 StringLookup 层对标签进行预处理。
terms = tf.ragged.constant(train_df["terms"].values)
lookup = tf.keras.layers.StringLookup(output_mode="multi_hot")
lookup.adapt(terms)
vocab = lookup.get_vocabulary()


def invert_multi_hot(encoded_labels):
    """Reverse a single multi-hot encoded label to a tuple of vocab terms."""
    hot_indices = np.argwhere(encoded_labels == 1.0)[..., 0]
    return np.take(vocab, hot_indices)


print("Vocabulary:\n")
print(vocab)

# 我们将从标签池中分离出可用的各个唯一类，然后使用此信息用 0 和 1 表示给定的标签集。下面是一个示例。
sample_label = train_df["terms"].iloc[0]
print(f"原始标签(Original label): {sample_label}")

label_binarized = lookup([sample_label])
print(f"标签二值化表示(Label-binarized representation): {label_binarized}")

# 原始标签(Original label): ['cs.CV']
# 标签二值化表示(Label-binarized representation): [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
#   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
#   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
#   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
#   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
#   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
#   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
#   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
#   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
#   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
#   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
#   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
#   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
#   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
#   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
#   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
#   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
#   0.]]

# 获取序列长度的分位数估值
train_df["summaries"].apply(lambda x: len(x.split(" "))).describe()
# Out[27]:
# count    44986.000000
# mean       157.857044
# std         41.336669
# min          5.000000
# 25%        129.000000
# 50%        156.000000
# 75%        184.000000
# max        462.000000
# Name: summaries, dtype: float64

max_seqlen = 150
batch_size = 128
padding_token = "<pad>"
auto = tf.data.AUTOTUNE


def make_dataset(dataframe, is_train=True):
    labels = tf.ragged.constant(dataframe["terms"].values)
    label_binarized = lookup(labels).numpy()
    dataset = tf.data.Dataset.from_tensor_slices(
        (dataframe["summaries"].values, label_binarized)
    )
    dataset = dataset.shuffle(batch_size * 10) if is_train else dataset
    return dataset.batch(batch_size)

train_dataset = make_dataset(train_df, is_train=True)
validation_dataset = make_dataset(val_df, is_train=False)
test_dataset = make_dataset(test_df, is_train=False)

# 展示数据集
text_batch, label_batch = next(iter(train_dataset))

for i, text in enumerate(text_batch[:3]):
    label = label_batch[i].numpy()[None, ...]
    print(f"Abstract: {text}")
    print(f"Label(s): {invert_multi_hot(label[0])}")
    print(" ")

# Abstract: b'In this paper we propose a novel environmental sound classification approach\nincorporating unsupervised feature learning from codebook via spherical\n$K$-Means++ algorithm and a new architecture for high-level data augmentation.\nThe audio signal is transformed into a 2D representation using a discrete\nwavelet transform (DWT). The DWT spectrograms are then augmented by a novel\narchitecture for cycle-consistent generative adversarial network. This\nhigh-level augmentation bootstraps generated spectrograms in both intra and\ninter class manners by translating structural features from sample to sample. A\ncodebook is built by coding the DWT spectrograms with the speeded-up robust\nfeature detector (SURF) and the K-Means++ algorithm. The Random Forest is our\nfinal learning algorithm which learns the environmental sound classification\ntask from the clustered codewords in the codebook. Experimental results in four\nbenchmarking environmental sound datasets (ESC-10, ESC-50, UrbanSound8k, and\nDCASE-2017) have shown that the proposed classification approach outperforms\nthe state-of-the-art classifiers in the scope, including advanced and dense\nconvolutional neural networks such as AlexNet and GoogLeNet, improving the\nclassification rate between 3.51% and 14.34%, depending on the dataset.'
# Label(s): ['cs.LG' 'stat.ML' 'eess.AS' 'cs.SD']
#
# Abstract: b"We introduce a new regularization method for Artificial Neural Networks\n(ANNs) based on Kernel Flows (KFs). KFs were introduced as a method for kernel\nselection in regression/kriging based on the minimization of the loss of\naccuracy incurred by halving the number of interpolation points in random\nbatches of the dataset. Writing $f_\\theta(x) = \\big(f^{(n)}_{\\theta_n}\\circ\nf^{(n-1)}_{\\theta_{n-1}} \\circ \\dots \\circ f^{(1)}_{\\theta_1}\\big)(x)$ for the\nfunctional representation of compositional structure of the ANN, the inner\nlayers outputs $h^{(i)}(x) = \\big(f^{(i)}_{\\theta_i}\\circ\nf^{(i-1)}_{\\theta_{i-1}} \\circ \\dots \\circ f^{(1)}_{\\theta_1}\\big)(x)$ define a\nhierarchy of feature maps and kernels $k^{(i)}(x,x')=\\exp(- \\gamma_i\n\\|h^{(i)}(x)-h^{(i)}(x')\\|_2^2)$. When combined with a batch of the dataset\nthese kernels produce KF losses $e_2^{(i)}$ (the $L^2$ regression error\nincurred by using a random half of the batch to predict the other half)\ndepending on parameters of inner layers $\\theta_1,\\ldots,\\theta_i$ (and\n$\\gamma_i$). The proposed method simply consists in aggregating a subset of\nthese KF losses with a classical output loss. We test the proposed method on\nCNNs and WRNs without alteration of structure nor output classifier and report\nreduced test errors, decreased generalization gaps, and increased robustness to\ndistribution shift without significant increase in computational complexity. We\nsuspect that these results might be explained by the fact that while\nconventional training only employs a linear functional (a generalized moment)\nof the empirical distribution defined by the dataset and can be prone to\ntrapping in the Neural Tangent Kernel regime (under over-parameterizations),\nthe proposed loss function (defined as a nonlinear functional of the empirical\ndistribution) effectively trains the underlying kernel defined by the CNN\nbeyond regressing the data with that kernel."
# Label(s): ['cs.LG' 'stat.ML']
#
# Abstract: b'The repairing work of terracotta warriors in Emperor Qinshihuang Mausoleum\nSite Museum is handcrafted by experts, and the increasing amounts of unearthed\npieces of terracotta warriors make the archaeologists too challenging to\nconduct the restoration of terracotta warriors efficiently. We hope to segment\nthe 3D point cloud data of the terracotta warriors automatically and store the\nfragment data in the database to assist the archaeologists in matching the\nactual fragments with the ones in the database, which could result in higher\nrepairing efficiency of terracotta warriors. Moreover, the existing 3D neural\nnetwork research is mainly focusing on supervised classification, clustering,\nunsupervised representation, and reconstruction. There are few pieces of\nresearches concentrating on unsupervised point cloud part segmentation. In this\npaper, we present SRG-Net for 3D point clouds of terracotta warriors to address\nthese problems. Firstly, we adopt a customized seed-region-growing algorithm to\nsegment the point cloud coarsely. Then we present a supervised segmentation and\nunsupervised reconstruction networks to learn the characteristics of 3D point\nclouds. Finally, we combine the SRG algorithm with our improved CNN using a\nrefinement method. This pipeline is called SRG-Net, which aims at conducting\nsegmentation tasks on the terracotta warriors. Our proposed SRG-Net is\nevaluated on the terracotta warriors data and ShapeNet dataset by measuring the\naccuracy and the latency. The experimental results show that our SRG-Net\noutperforms the state-of-the-art methods. Our code is shown in Code File\n1~\\cite{Srgnet_2021}.'
# Label(s): ['cs.CV']

# Source: https://stackoverflow.com/a/18937309/7636462
vocabulary = set()
train_df["summaries"].str.lower().str.split().apply(vocabulary.update)
vocabulary_size = len(vocabulary)
print(vocabulary_size)
# 157927

# 创建个向量层
text_vectorizer = layers.TextVectorization(
    max_tokens=vocabulary_size, ngrams=2, output_mode="tf_idf"
)

# `TextVectorization` layer needs to be adapted as per the vocabulary from our
# training set.
with tf.device("/CPU:0"):
    text_vectorizer.adapt(train_dataset.map(lambda text, label: text))

train_dataset = train_dataset.map(
    lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto
).prefetch(auto)
validation_dataset = validation_dataset.map(
    lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto
).prefetch(auto)
test_dataset = test_dataset.map(
    lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto
).prefetch(auto)

# 创建分类模型
def make_model():
    shallow_mlp_model = keras.Sequential(
        [
            layers.Dense(512, activation="relu"),
            layers.Dense(256, activation="relu"),
            layers.Dense(lookup.vocabulary_size(), activation="sigmoid"),
        ]  # More on why "sigmoid" has been used here in a moment.
    )
    return shallow_mlp_model

# 训练模型
epochs = 20

shallow_mlp_model = make_model()
shallow_mlp_model.compile(
    loss="binary_crossentropy", optimizer="adam", metrics=["binary_accuracy"]
)

history = shallow_mlp_model.fit(
    train_dataset, validation_data=validation_dataset, epochs=epochs
)


def plot_result(item):
    plt.plot(history.history[item], label=item)
    plt.plot(history.history["val_" + item], label="val_" + item)
    plt.xlabel("Epochs")
    plt.ylabel(item)
    plt.title("Train and Validation {} Over Epochs".format(item), fontsize=14)
    plt.legend()
    plt.grid()
    plt.show()


plot_result("loss")
plot_result("binary_accuracy")

# 评估模型
_, binary_acc = shallow_mlp_model.evaluate(test_dataset)
print(f"Categorical accuracy on the test set: {round(binary_acc * 100, 2)}%.")

# 模型预测
# Create a model for inference.
model_for_inference = keras.Sequential([text_vectorizer, shallow_mlp_model])

# Create a small dataset just for demoing inference.
inference_dataset = make_dataset(test_df.sample(100), is_train=False)
text_batch, label_batch = next(iter(inference_dataset))
predicted_probabilities = model_for_inference.predict(text_batch)

# Perform inference.
for i, text in enumerate(text_batch[:5]):
    label = label_batch[i].numpy()[None, ...]
    print(f"Abstract: {text}")
    print(f"Label(s): {invert_multi_hot(label[0])}")
    predicted_proba = [proba for proba in predicted_probabilities[i]]
    top_3_labels = [
        x
        for _, x in sorted(
            zip(predicted_probabilities[i], lookup.get_vocabulary()),
            key=lambda pair: pair[0],
            reverse=True,
        )
    ][:3]
    print(f"Predicted Label(s): ({', '.join([label for label in top_3_labels])})")
    print(" ")

def main():
    pass


if __name__ == "__main__":
    main()
