#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import numpy as np
from keras.preprocessing.text import Tokenizer
from transformers import BertTokenizer

special_tokens = ["[UNK]", "[PAD]", "[CLS]", "[SEP]", "[MASK]"]


# 从GloVe文件中解析出每个词和它所对应的词向量，并用字典的方式存储

embeddings_index = {}
f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
EMBEDDING_DIM = len(coefs)
f.close()

print('Found %s word vectors.' % len(embeddings_index))


MAX_NB_WORDS=len(embeddings_index)
tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)
tokenizer.fit_on_texts(list(embeddings_index.keys()))
word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

# 可以根据得到的字典生成上文所定义的词向量矩阵
embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector

# 现在我们将这个词向量矩阵加载到Embedding层中，注意，我们设置trainable=False使得这个编码层不可再训练。

from keras.layers import Embedding

embedding_layer = Embedding(len(word_index) + 1,
                            EMBEDDING_DIM,
                            weights=[embedding_matrix],
                            input_length=MAX_SEQUENCE_LENGTH,
                            trainable=False)

# 使用预训练的词向量及一个小型的1D卷积进行新闻分类问题。

sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')
embedded_sequences = embedding_layer(sequence_input)
x = Conv1D(128, 5, activation='relu')(embedded_sequences)
x = MaxPooling1D(5)(x)
x = Conv1D(128, 5, activation='relu')(x)
x = MaxPooling1D(5)(x)
x = Conv1D(128, 5, activation='relu')(x)
x = MaxPooling1D(35)(x)  # global max pooling
x = Flatten()(x)
x = Dense(128, activation='relu')(x)
preds = Dense(len(labels_index), activation='softmax')(x)

model = Model(sequence_input, preds)
model.compile(loss='categorical_crossentropy',
              optimizer='rmsprop',
              metrics=['acc'])

# happy learning!
model.fit(x_train, y_train, validation_data=(x_val, y_val),
          nb_epoch=2, batch_size=128)

################################################################################################################################
# 当然也可以导出训练好的模型（如通过自动编码器生成词向量），导出Embedding，迁移到其他模型使用，即作为其他模型预训练词向量使用
# __________________________________________________________________________________________________
# 输出模型各层名称
for layer in model.layers:
    print(layer.name)
# input_1
# input_a1
# input_q1
# feature_embeddings
# time_distributed_1
# reshape_1
# dropout_2
# lstm_1
# dropout_3
# dense_3
# dot_3
# dot_2
# lambda_5

# 获取 Embedding层
feature_embeddings = model.get_layer('feature_embeddings')

# 输出Embedding层embeddings的维度
feature_embeddings.embeddings.shape
# TensorShape([1928, 32])
# 即有1928个词，每个词向量输出维度是32

# 展示向量值
feature_embeddings.embeddings
# <tf.Variable 'feature_embeddings/embeddings:0' shape=(1928, 32) dtype=float32, numpy=
# array([[ 0.09056077,  0.1091212 , -0.02109013, ..., -0.01866139,
#         -0.03773832,  0.05758953],
#        [ 0.030546  ,  0.00573606,  0.02638761, ..., -0.02815089,
#         -0.02997752, -0.02078813],
#        [ 0.01495321,  0.04614106, -0.01285633, ...,  0.0351727 ,
#         -0.0391034 , -0.04369973],
#        ...,
#        [-0.03566889, -0.03022184,  0.03539237, ..., -0.0274649 ,
#         -0.00947213,  0.02068614],
#        [-0.06849964, -0.32110888,  0.24041066, ..., -0.46497703,
#         -0.4874884 , -0.51523954],
#        [-0.02060996, -0.04868434,  0.01760063, ...,  0.0086992 ,
#          0.02943896,  0.02443523]], dtype=float32)>

input_1 = np.random.randint(0, 1928, size=(32, 6, 79))
input_q1 = np.random.randint(0, 1928, size=(32, 6, 79))
input_a1 = np.random.randint(0, 1928, size=(32, 6, 79))
input_1.shape, input_q1.shape, input_a1.shape
# ((32, 6, 79), (32, 6, 79), (32, 6, 79))

# 通过Embedding层的输入、输出构建一个新模型，并通过新模型进行预测，即获取Embedding层的输出
representation_model = Model(inputs=feature_embeddings.input, outputs=feature_embeddings.output)
input_1_emb = representation_model.predict(input_1)
input_1_emb.shape
(32, 6, 79, 32)

input_1
array([[[   0,    0,    0, ...,    0,    0,    0],
        [   0,    0,    0, ...,    0,    0,    0],
        [   0,    0,    0, ...,    0,    0,    0],
        [   0,    0,    0, ...,    0,    0,    0],
        [   0,    0,    0, ...,    0,    0,    0],
        [   3,    0,    0, ..., 1872,    0, 1926]],

       [[   0,    0,    0, ...,    0,    0,    0],
        [   0,    0,    0, ...,    0,    0,    0],
        [   0,    0,    0, ...,    0,    0,    0],
        [   0,    0,    0, ...,    0,    0,    0],
        [   3,    0,    0, ..., 1872,    0, 1922],
        [   3,    0,    0, ..., 1872,    0, 1922]],

       [[   0,    0,    0, ...,    0,    0,    0],
        [   0,    0,    0, ...,    0,    0,    0],
        [   0,    0,    0, ...,    0,    0,    0],
        [   0,    0,    0, ...,    0,    0,    0],
        [   0,    0,    0, ...,    0,    0,    0],
        [   3,    0,   11, ..., 1871,    0, 1922]],

       ...,

       [[   0,    0,    0, ...,    0,    0,    0],
        [   0,    0,    0, ...,    0,    0,    0],
        [   0,    0,    0, ...,    0,    0,    0],
        [   0,    0,    0, ...,    0,    0,    0],
        [   0,    0,    0, ...,    0,    0,    0],
        [   3,    0,    0, ...,    0,    0, 1922]],

       [[   0,    0,    0, ...,    0,    0,    0],
        [   0,    0,    0, ...,    0,    0,    0],
        [   0,    0,    0, ...,    0,    0,    0],
        [   0,    0,    0, ...,    0,    0,    0],
        [   0,    0,    0, ...,    0,    0,    0],
        [   3,    0,    0, ..., 1872,    0, 1926]],

       [[   0,    0,    0, ...,    0,    0,    0],
        [   0,    0,    0, ...,    0,    0,    0],
        [   0,    0,    0, ...,    0,    0,    0],
        [   0,    0,    0, ...,    0,    0,    0],
        [   0,    0,    0, ...,    0,    0,    0],
        [   3,    0,    0, ..., 1871,    0, 1922]]], dtype=int32)

# 查看几个样例输入code
input_1[-1][-1][-1], input_1[-1][-1][-3], input_1[-2][-1][-1], input_1[-2][-1][-3]
(1922, 1871, 1926, 1872)

# 样例code的新构建模型预测值
input_1_emb[-1][-1][-1], input_1_emb[-1][-1][-3], input_1_emb[-2][-1][-1], input_1_emb[-2][-1][-3]
(array([-0.0446717 ,  0.01825527,  0.05349204,  0.03826927,  0.01871787,
         0.05756143,  0.09369965,  0.00549489,  0.09517014,  0.0387467 ,
         0.11492513, -0.1705225 , -0.1031243 ,  0.05413583,  0.01767652,
         0.05579946, -0.02480147, -0.02644608,  0.14885874, -0.02696987,
        -0.12686425,  0.09526629,  0.01541451,  0.13259347, -0.0096681 ,
         0.01018803, -0.0495808 ,  0.04192156,  0.16099697, -0.07137429,
        -0.06296851, -0.01831987], dtype=float32),
 array([ 0.24872316,  0.0130953 ,  0.05293951, -0.1902247 , -0.22053146,
         0.18638588, -0.02532587, -0.31497648, -0.09315658,  0.17802703,
        -0.06038152,  0.03937646, -0.16825318, -0.24178304,  0.1390604 ,
         0.14577578, -0.285094  ,  0.15080766, -0.06105503,  0.06232972,
         0.05032326,  0.2628372 , -0.12729217, -0.20382915,  0.25182387,
        -0.20191091, -0.15617199, -0.05590734, -0.06525838,  0.18725865,
         0.18036032, -0.27386075], dtype=float32),
 array([-0.06849964, -0.32110888,  0.24041066,  0.03665899,  0.445088  ,
         0.57872385,  0.5673283 ,  0.4899758 ,  0.49307835, -0.623345  ,
         0.30415714, -0.27516952, -0.6232489 ,  0.4997082 , -0.51786345,
         0.35926116, -0.14761928, -0.14618266, -0.04085983,  0.17035045,
        -0.38788417, -0.36868128,  0.4614472 ,  0.6494242 ,  0.43324628,
         0.49722824, -0.64769155,  0.27040425,  0.63741785, -0.46497703,
        -0.4874884 , -0.51523954], dtype=float32),
 array([-0.09062219, -0.47664502, -0.10889386, -0.20133594, -0.32111672,
        -0.3181706 , -0.29733503,  0.04952655, -0.29739276,  0.3089955 ,
         0.17998062,  0.30951712,  0.03349508, -0.16417316,  0.28423572,
         0.21494131, -0.21493773,  0.33795354, -0.15083039,  0.18266949,
         0.21642244, -0.18330994, -0.04994079, -0.24898356,  0.03493035,
        -0.33249342, -0.06401964,  0.03615334,  0.4436547 ,  0.26550496,
         0.31052926, -0.2510962 ], dtype=float32))

# 样例code的模型输出值，与Embedding层embeddings对应位置的向量一致
feature_embeddings.embeddings[1922], feature_embeddings.embeddings[1871], feature_embeddings.embeddings[1926], feature_embeddings.embeddings[1872]
# (<tf.Tensor: shape=(32,), dtype=float32, numpy=
#  array([-0.0446717 ,  0.01825527,  0.05349204,  0.03826927,  0.01871787,
#          0.05756143,  0.09369965,  0.00549489,  0.09517014,  0.0387467 ,
#          0.11492513, -0.1705225 , -0.1031243 ,  0.05413583,  0.01767652,
#          0.05579946, -0.02480147, -0.02644608,  0.14885874, -0.02696987,
#         -0.12686425,  0.09526629,  0.01541451,  0.13259347, -0.0096681 ,
#          0.01018803, -0.0495808 ,  0.04192156,  0.16099697, -0.07137429,
#         -0.06296851, -0.01831987], dtype=float32)>,
#  <tf.Tensor: shape=(32,), dtype=float32, numpy=
#  array([ 0.24872316,  0.0130953 ,  0.05293951, -0.1902247 , -0.22053146,
#          0.18638588, -0.02532587, -0.31497648, -0.09315658,  0.17802703,
#         -0.06038152,  0.03937646, -0.16825318, -0.24178304,  0.1390604 ,
#          0.14577578, -0.285094  ,  0.15080766, -0.06105503,  0.06232972,
#          0.05032326,  0.2628372 , -0.12729217, -0.20382915,  0.25182387,
#         -0.20191091, -0.15617199, -0.05590734, -0.06525838,  0.18725865,
#          0.18036032, -0.27386075], dtype=float32)>,
#  <tf.Tensor: shape=(32,), dtype=float32, numpy=
#  array([-0.06849964, -0.32110888,  0.24041066,  0.03665899,  0.445088  ,
#          0.57872385,  0.5673283 ,  0.4899758 ,  0.49307835, -0.623345  ,
#          0.30415714, -0.27516952, -0.6232489 ,  0.4997082 , -0.51786345,
#          0.35926116, -0.14761928, -0.14618266, -0.04085983,  0.17035045,
#         -0.38788417, -0.36868128,  0.4614472 ,  0.6494242 ,  0.43324628,
#          0.49722824, -0.64769155,  0.27040425,  0.63741785, -0.46497703,
#         -0.4874884 , -0.51523954], dtype=float32)>,
#  <tf.Tensor: shape=(32,), dtype=float32, numpy=
#  array([-0.09062219, -0.47664502, -0.10889386, -0.20133594, -0.32111672,
#         -0.3181706 , -0.29733503,  0.04952655, -0.29739276,  0.3089955 ,
#          0.17998062,  0.30951712,  0.03349508, -0.16417316,  0.28423572,
#          0.21494131, -0.21493773,  0.33795354, -0.15083039,  0.18266949,
#          0.21642244, -0.18330994, -0.04994079, -0.24898356,  0.03493035,
#         -0.33249342, -0.06401964,  0.03615334,  0.4436547 ,  0.26550496,
#          0.31052926, -0.2510962 ], dtype=float32)>)

# 对比code=0的模型预测值，及向量输出值，发现结果一致；
input_1_emb[-1][-1][-2], feature_embeddings.embeddings[0]
# (array([ 0.09056077,  0.1091212 , -0.02109013, -0.01617863,  0.01754965,
#          0.000984  ,  0.10190235, -0.00189195,  0.02361282, -0.01916809,
#          0.07159567, -0.04473461, -0.06475332, -0.00940376,  0.02201039,
#          0.02473831, -0.0544999 , -0.03977289,  0.0625231 , -0.04253514,
#          0.03500799,  0.05926352,  0.01921841, -0.10119429, -0.06513655,
#          0.03320842,  0.01135358, -0.08236717, -0.04608197, -0.01866139,
#         -0.03773832,  0.05758953], dtype=float32),
#  <tf.Tensor: shape=(32,), dtype=float32, numpy=
#  array([ 0.09056077,  0.1091212 , -0.02109013, -0.01617863,  0.01754965,
#          0.000984  ,  0.10190235, -0.00189195,  0.02361282, -0.01916809,
#          0.07159567, -0.04473461, -0.06475332, -0.00940376,  0.02201039,
#          0.02473831, -0.0544999 , -0.03977289,  0.0625231 , -0.04253514,
#          0.03500799,  0.05926352,  0.01921841, -0.10119429, -0.06513655,
#          0.03320842,  0.01135358, -0.08236717, -0.04608197, -0.01866139,
#         -0.03773832,  0.05758953], dtype=float32)>)

# 迁移之后的使用示例：
embedding_layer = Embedding(1928, 32, weights=feature_embeddings.embeddings,
                            input_length=input_length,
                            trainable=False)

def main():
    pass


if __name__ == '__main__':
    main()
