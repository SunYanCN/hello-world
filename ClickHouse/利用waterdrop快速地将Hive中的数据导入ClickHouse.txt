
一、准备spark环境：
https://www.apache.org/dyn/closer.lua/spark/spark-2.4.8/spark-2.4.8-bin-hadoop2.7.tgz
https://mirrors.bfsu.edu.cn/apache/spark/spark-2.4.8/spark-2.4.8-bin-hadoop2.7.tgz 
下载解压后，不需要做任何配置即可提交Spark deploy-mode = local模式的任务。


二：安装waterdrop
下载安装包：https://github.com/InterestingLab/waterdrop/releases/download/v1.5.1/waterdrop-1.5.1.zip

下载后，解压：

unzip waterdrop-<version>.zip
ln -s waterdrop-<version> waterdrop

注：上两个压缩文件，解压后，需要给hadoop用户有可读权限，否则后面数据迁移是会报类似下面的错误：
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=root, access=WRITE, inode="/shared/spark-logs":hadoop:supergroup:drwxr-xr-x

tar -xzf spark-2.4.8-bin-hadoop2.7.tgz

[root@SZE-L0403067 waterdrop-1.5.1]# cp -r /root/test_clickhouse/waterdrop-1.5.1 /home/hadoop/
[root@SZE-L0403067 waterdrop-1.5.1]# cp -r /root/test_clickhouse/spark-2.4.8-bin-hadoop2.7 /home/hadoop/
[root@SZE-L0403067 waterdrop-1.5.1]# chown hadoop:hadoop -R /home/hadoop/spark-2.4.8-bin-hadoop2.7
[root@SZE-L0403067 waterdrop-1.5.1]# chown hadoop:hadoop -R /home/hadoop/waterdrop-1.5.1
[root@SZE-L0403067 waterdrop-1.5.1]# su - hadoop

设置spark的路径，即更改文件：waterdrop-1.5.1/config/waterdrop-env.sh
将 SPARK_HOME=${SPARK_HOME:-/opt/spark}
改为：
SPARK_HOME=${SPARK_HOME:/home/hadoop/spark-2.4.8-bin-hadoop2.7}

# 进入waterdrop工作目录，并迁移数据：
[hadoop@SZE-L0403067 ~]$ cd waterdrop-1.5.1/
[hadoop@SZE-L0403067 waterdrop-1.5.1]$ ls
bin  config  docs  lib  LICENSE  plugins  README.md
[hadoop@SZE-L0403067 waterdrop-1.5.1]$ ./bin/start-waterdrop.sh --master local[4] --deploy-mode client --config ./config/hdfs-clickhouse2.conf

# 迁移数据前的准备工作：
# 准备工作1，准备hive数据表：
hive> CREATE TABLE test_hdfs2ch2(
                id int,
                name string,
                create_time timestamp);
# 向hive插入数据：
hive> insert into test_hdfs2ch2 values(1,'zhangsan',' 2020-01-01 01:01:01.000001');
hive> insert into test_hdfs2ch2 values(2,'lisi','2020-01-01 01:01:01.000002');

# 准备工作2，创建clickhouse表，以便导入数据：
curl localhost:8124 -d 'CREATE TABLE tutorial.hdfs2ch2
 (
     `id` Int64,
     `name` String,
     `create_time` DateTime
 )
 ENGINE = MergeTree()
 ORDER BY id
 SETTINGS index_granularity = 8192';

# 准备工作3，编写迁移数据文件：

[hadoop@SZE-L0403067 waterdrop-1.5.1]$ cat config/hdfs-clickhouse2.conf
spark {
  #程序名称
  spark.app.name = "Waterdrop"
  #executor的数量(数据量大可以适当增大)
  spark.executor.instances = 1
  #每个excutor核数(并行度,数据量大可以适当增大到服务器核数一半以下,尽量不要影响clickhouse)
  spark.executor.cores = 1
  #每个excutor内存(不能小于512m)
  spark.executor.memory = "1g"
}

input {
 hdfs {
    result_table_name = "test_source"
    #hive创建表的hdfs路径
    path = "hdfs://12.12.21.22:9001/user/hive/warehouse/test_hdfs2ch2"
    format="text"
 }
}

filter {
  split {
    #根据分隔符切割后给每个列的名字
    fields = ["id", "name","create_time"]
    #这里指的是hive的字段分隔符,不然无法切割
    delimiter = "\\001"
  }
  convert {
    #因为刚切割后所有字段类型为string,如果不转化就会报错
    #可以转化的类型string、integer、long、float、double和boolean
    source_field = "id"
    new_type = "long"
}
  date {
    #指定要进行转换的原字段名
    source_field = "create_time"
    #指定转化结束后的字段名(必须指定)
    target_field = "create_time"
    #大S就是毫秒的表示,如果表示错误,会转化失败,转化失败就会生成当前时间
    source_time_format = "yyyy-MM-dd HH:mm:ss.SSSSSS"
    target_time_format = "yyyy-MM-dd HH:mm:ss"
   }
}
output {
  stdout{
    limit=2
  }
 clickhouse {
    host = "12.12.21.22:8124"
    clickhouse.socket_timeout = 50000
    database = "tutorial"
    table = "hdfs2ch2"
    fields = ["id","name","create_time"]
    username = ""
    password = ""
    bulk_size = 20000
}
}
[hadoop@SZE-L0403067 waterdrop-1.5.1]$
注：hive创建表的hdfs路径，可以通过查hive表得知：
 若有多个字段需要转换数据类型，则在filter下写多个convert即可；
 若不需要有日期格式转换，则将 date 就后面的 {} 都注释掉即可；

[hadoop@SZE-L0403067 waterdrop-1.5.1]$ hive -e "show create table test_hdfs2ch2"|grep -A 1 LOCATION
OK
LOCATION
  'hdfs://12.12.21.22:9001/user/hive/warehouse/test_hdfs2ch2'

# 至此，准备工作都完成，直接导入数据即可：
[hadoop@SZE-L0403067 waterdrop-1.5.1]$ ./bin/start-waterdrop.sh --master local[4] --deploy-mode client --config ./config/hdfs-clickhouse2.conf
# 导入完成后，即可在clickhouse查询到对应的导入数据：

[hadoop@SZE-L0403067 waterdrop-1.5.1]$ curl localhost:8124 -d 'select * from tutorial.hdfs2ch2'
1       zhangsan        2020-01-01 01:01:01
2       lisi    2020-01-01 01:01:01


