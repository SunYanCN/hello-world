
train loss 不断下降，test loss不断下降，说明网络仍在学习;

train loss 不断下降，test loss趋于不变，说明网络过拟合;

train loss 趋于不变，test loss不断下降，说明数据集100%有问题;

train loss 趋于不变，test loss趋于不变，说明学习遇到瓶颈，需要减小学习率或批量数目;

train loss 不断上升，test loss不断上升，说明网络结构设计不当，训练超参数设置不当，数据集经过清洗等问题。

train loss 快速稳定，test loss 快速稳定，如果数据集规模不小的情况下，代表学习过程遇到瓶颈，需要减小学习率（自适应动量优化器小范围修改的效果不明显）。其次考虑修改 batchsize 大小。如果数据集很规模很小的话代表训练稳定。

训练集和验证集的loss一直减少，但是测试集的loss先减小后增大, 到test转折点那里可能已经在训练集上过拟合了;

验证loss比训练loss低的3个原因:
原因1：在训练中应用正则化，但在验证/测试中未应用正则化; 
原因2：训练loss是在每个epoch测量的，而验证loss是在每个epoch后测量的;在整个epoch内，您的训练loss将不断得到报告；但是，仅在当前训练epoch完成后，才根据验证集计算验证指标。这意味着，平均而言，训练loss要提前半个epoch来衡量。
原因3：原因是由于数据本身分布的问题，验证集可能比训练集更容易。
若上述三个原因都不是，那可能是模型过度正则化(over-regularized)了。通过以下方法开始放宽正则化约束：
*降低L2权重衰减强度。
*减少申请的dropout数量。
*增加模型容量（即，使其更深）。
您还应该尝试以更高的学习率进行训练，因为您可能对此过于保守。
 ———————————————— 

原文链接：https://blog.csdn.net/SMF0504/article/details/71698354

# train Loss不降反升的原因:
1.可能是权重初始化不好
2.可能是数据标注有问题
3.可能是学习速度太大了，或者batchsize过小
4.网络结构有问题

训练过程中loss数值为负数？
【原因1】输入的训练数据没有归一化造成
【解决方法】把输入数值通过下面的函数过滤一遍，进行归一化

from keras.layers.normalization import BatchNormalization
x = BatchNormalization(name='normalization')(x)

【原因1】输入的训练数据有异常
【解决方法】除去异常值
比如ner时候，对标签对应填充时候，填充了不恰当的值（正常是用0,1,2...分别代表BIO编码），若填充了-1，计算loss就会出现负数
pad_sequences(batch_ner_tag, input_length, value=-1)
改为：
pad_sequences(batch_ner_tag, input_length, value=0)

acc在训练两三轮之后就开始不变化；或者从一开始就是acc只降不升，说明神经网络不收敛不学习
神经网络不收敛的可能原因:

1、没有对数据进行归一化
2、忘记检查输入和输出
3、没有对数据进行预处理
4、没有对数据正则化
5、使用过大的样本
6、使用不正确的学习率
7、在输出层使用错误的激活函数
8、网络中包含坏梯度
9、初始化权重错误
10、过深的网络
11、隐藏单元数量错误
12、优化算法不对，一般用adam居多。
13、数据随机性太强，同时网络结构太复杂（样本空间太大，同时样本数量不够，网络函数空间也大）
14、学习率过大。网络可能快要收敛了，却忽然在下一个epoch走向了错误的方向，最终导致不收敛。

链接：https://www.jianshu.com/p/bbd11ad4e973
https://zhuanlan.zhihu.com/p/36369878

神经网络不学习的原因

https://blog.csdn.net/hustqb/article/details/78648556#_11

# 验证集Loss震荡的原因可能有如下：
1、数据问题，比如训练集和验证集相差太大，数据量太小；
2、batchsize太小，模型学习的规律不够“普适”；
3、loss函数不合适；
4、学习率太大，模型陷入了局部最优点；
5、模型的网络结构存在问题；

# 训练网络loss出现Nan
nan，是not a number的缩写，就是没有数据
①梯度爆炸
原因：梯度变得非常大，使得学习过程难以继续
现象：观察log，注意每一轮迭代后的loss。loss随着每轮迭代越来越大，最终超过了浮点型表示的范围，就变成了NaN。
措施：
1. 减小学习率learning_rate，至少减小一个数量级。如果有多个loss layer，需要找出哪个损失层导致了梯度爆炸，并减小该层的loss_weight，而非是减小通用的learning_rate。
2. 设置clip gradient(梯度截断)，用于限制过大的diff
②不当的损失函数
原因：有时候损失层中loss的计算可能导致NaN的出现。比如，给InfogainLoss层（信息熵损失）输入没有归一化的值，使用带有bug的自定义损失层，
可能用0作为了除数、可能0或者负数作为自然对数、计算loss的数组越界等等。
现象：观测训练产生的log时一开始并不能看到异常，loss也在逐步的降低，但突然之间NaN就出现了。
措施：看看你是否能重现这个错误，在loss layer中加入一些输出以进行调试。
③不当的输入
原因：输入中就含有NaN。
现象：每当学习的过程中碰到这个错误的输入，就会变成NaN。观察log的时候也许不能察觉任何异常，loss逐步的降低，但突然间就变成NaN了。
措施：重整你的数据集，确保训练集和验证集里面没有损坏的图片。调试中你可以使用一个简单的网络来读取输入层，有一个缺省的loss，并过一遍所有输入，如果其中有错误的输入，这个缺省的层也会产生NaN。
④自定义网络结构问题
如果当前的网络是类似于RNN的循环神经网络的话，出现NaN可能是因为梯度爆炸的原因，一个有效的方式是增加“gradient clipping”（梯度截断来解决）

