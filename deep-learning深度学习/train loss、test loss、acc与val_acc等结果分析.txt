
train loss 不断下降，test loss不断下降，说明网络仍在学习;

train loss 不断下降，test loss趋于不变，说明网络过拟合;

train loss 趋于不变，test loss不断下降，说明数据集100%有问题;

train loss 趋于不变，test loss趋于不变，说明学习遇到瓶颈，需要减小学习率或批量数目;

train loss 不断上升，test loss不断上升，说明网络结构设计不当，训练超参数设置不当，数据集经过清洗等问题。

训练集和验证集的loss一直减少，但是测试集的loss先减小后增大, 到test转折点那里可能已经在训练集上过拟合了;

验证loss比训练loss低的3个原因:
原因1：在训练中应用正则化，但在验证/测试中未应用正则化; 
原因2：训练loss是在每个epoch测量的，而验证loss是在每个epoch后测量的;在整个epoch内，您的训练loss将不断得到报告；但是，仅在当前训练epoch完成后，才根据验证集计算验证指标。这意味着，平均而言，训练loss要提前半个epoch来衡量。
原因3：原因是由于数据本身分布的问题，验证集可能比训练集更容易。
若上述三个原因都不是，那可能是模型过度正则化(over-regularized)了。通过以下方法开始放宽正则化约束：
*降低L2权重衰减强度。
*减少申请的dropout数量。
*增加模型容量（即，使其更深）。
您还应该尝试以更高的学习率进行训练，因为您可能对此过于保守。
 ———————————————— 

原文链接：https://blog.csdn.net/SMF0504/article/details/71698354

# train Loss不降反升的原因:
1.可能是权重初始化不好
2.可能是数据标注有问题
3.可能是学习速度太大了，或者batchsize过小
4.网络结构有问题

训练过程中loss数值为负数？
【原因1】输入的训练数据没有归一化造成
【解决方法】把输入数值通过下面的函数过滤一遍，进行归一化

from keras.layers.normalization import BatchNormalization
x = BatchNormalization(name='normalization')(x)

【原因1】输入的训练数据有异常
【解决方法】除去异常值
比如ner时候，对标签对应填充时候，填充了不恰当的值（正常是用0,1,2...分别代表BIO编码），若填充了-1，计算loss就会出现负数
pad_sequences(batch_ner_tag, input_length, value=-1)
改为：
pad_sequences(batch_ner_tag, input_length, value=0)

acc在训练两三轮之后就开始不变化；或者从一开始就是acc只降不升，说明神经网络不收敛不学习
神经网络不收敛的可能原因:

1、没有对数据进行归一化
2、忘记检查输入和输出
3、没有对数据进行预处理
4、没有对数据正则化
5、使用过大的样本
6、使用不正确的学习率
7、在输出层使用错误的激活函数
8、网络中包含坏梯度
9、初始化权重错误
10、过深的网络
11、隐藏单元数量错误
12、优化算法不对，一般用adam居多。
13、数据随机性太强，同时网络结构太复杂（样本空间太大，同时样本数量不够，网络函数空间也大）
14、学习率过大。网络可能快要收敛了，却忽然在下一个epoch走向了错误的方向，最终导致不收敛。

链接：https://www.jianshu.com/p/bbd11ad4e973
https://zhuanlan.zhihu.com/p/36369878

神经网络不学习的原因

https://blog.csdn.net/hustqb/article/details/78648556#_11



