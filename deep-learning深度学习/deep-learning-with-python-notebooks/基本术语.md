
### 类(class)
    在机器学习中,分类问题中的某个类别叫作类(class)。

### 样本(sample)
    数据点叫作样本(sample)。

### 标签(label)
    某个样本对应的类叫作标签(label)。

### 张量(tensor)
    是一个数据容器。它包含的数据几乎总是数值数据,因此它是数字的容器。你可能对矩阵很熟悉,它是二维张量。
    张量是矩阵向任意维度的推广[注意,张量的维度(dimension)通常叫作轴(axis)]。

### 维度(dimension)
    可以表示沿着某个轴上的元素个数(比如 5D 向量),也可以表示张量中轴的个数(比如 5D 张量),这有时会令人感到混乱。
    对于后一种情况,技术上更准确的说法是 5 阶张量(张量的阶数即轴的个数),但 5D 张量这种模糊的写法更常见。

### 轴(axis)
    轴用来为超过一维的数组定义的属性，二维数据拥有两个轴：第0轴沿着行的垂直往下，第1轴沿着列的方向水平延伸。
    axis取值说明：一维数组时axis=0，二维数组时axis=0，1，维数越高，则axis可取的值越大，数组n维时，axis=0，1，…，n-1。
    在numpy中数组都有着[]标记，则axis=0对应着最外层的[]，axis=1对应第二外层的[]，以此类推，axis=n对应第n+1外层的[]。
    a = np.array([[1,2,3],[4,5,6]]), axis = 0表示对最外层[]里的最大单位块做块与块之间的运算, 即对[1,2,3]与[4,5,6]，做运算；若是做drop，则表示对[1,2,3]，或[4,5,6]，做drop;
    axis = 1, 表示对次外层[]的最大单位块与块之间做运算，或块drop操作。比如，第一行则是 1,2,3之间做运算，drop,则是针对1或2或3做drop，因为其他行也是类似操作，表现出来的效果就是对列进行操作。
    p = pd.DataFrame([[1,2,3], [4,5,6]])
    p.drop(0, axis=1)
    Out[13]: 
       1  2
    0  2  3
    1  5  6
    p.drop(0, axis=0)
    Out[14]: 
       0  1  2
    1  4  5  6
    d = np.array([[1,2,3], [4,5,6]])
    d.sum(axis=0)
    Out[16]: array([5, 7, 9])
    d.sum(axis=0)
    Out[17]: array([5, 7, 9])

### 样本轴
    通常来说,深度学习中所有数据张量的第一个轴(0 轴,因为索引从 0 开始)都是样本轴(samples axis,有时也叫样本维度)

### 批量
    深度学习模型不会同时处理整个数据集,而是将数据拆分成小批量。
    具体来看,下面是 MNIST 数据集的一个批量,批量大小为 128。
    batch = train_images[:128]
    然后是下一个批量。
    batch = train_images[128:256]
    然后是第 n 个批量。
    batch = train_images[128 * n:128 * (n + 1)]
    对于这种批量张量,第一个轴(0 轴)叫作批量轴(batch axis)或批量维度(batch dimension)。

### 向量数据
    2D 张量,形状为 (samples, features) 。
    对于这种数据集,每个数据点都被编码为一个向量,因此一个数据批量就被编码为 2D 张量(即向量组成的数组),其中第一个轴是样本轴,第二个轴是特征轴。
    
### 时间序列数据或序列数据
    3D 张量,形状为 (samples, timesteps, features) 。
    当时间(或序列顺序)对于数据很重要时,应该将数据存储在带有时间轴的 3D 张量中。
    每个样本可以被编码为一个向量序列(即 2D 张量),因此一个数据批量就被编码为一个 3D 张量。
    根据惯例,时间轴始终是第 2 个轴(索引为 1 的轴)。

### 图像
    4D 张量,形状为 (samples, height, width, channels) 或 (samples, channels,height, width) 。
    图像通常具有三个维度:高度、宽度和颜色深度。
    虽然灰度图像(比如 MNIST 数字图像)只有一个颜色通道,因此可以保存在 2D 张量中,但按照惯例,图像张量始终都是 3D 张量,灰度图像的彩色通道只有一维。
    因此,如果图像大小为 256×256,那么 128 张灰度图像组成的批量可以保存在一个形状为 (128, 256, 256, 1) 的张量中,
    而 128 张彩色图像组成的批量则可以保存在一个形状为 (128, 256, 256, 3) 的张量中。
    图像张量的形状有两种约定:通道在后(channels-last)的约定(在 TensorFlow 中使用)和通道在前(channels-first)的约定(在 Theano 中使用)。
    Google 的 TensorFlow 机器学习框架将颜色深度轴放在最后: (samples, height, width, color_depth) 。
    与此相反,Theano将图像深度轴放在批量轴之后: (samples, color_depth, height, width) 。
    如果采用 Theano 约定,前面的两个例子将变成 (128, 1, 256, 256) 和 (128, 3, 256, 256) 。
    Keras 框架同时支持这两种格式。

### 视频
    5D 张量,形状为 (samples, frames, height, width, channels) 或 (samples,frames, channels, height, width) 。
    视频数据是现实生活中需要用到 5D 张量的少数数据类型之一。视频可以看作一系列帧,每一帧都是一张彩色图像。
    由于每一帧都可以保存在一个形状为 (height, width, color_depth) 的 3D 张量中,
    因此一系列帧可以保存在一个形状为 (frames, height, width, color_depth) 的 4D 张量中,
    而不同视频组成的批量则可以保存在一个 5D 张量中,其形状为(samples, frames, height, width, color_depth) 。

### 光学字符识别OCR(optical character recognition)
    OCR （Optical Character Recognition，光学字符识别）是指电子设备（例如扫描仪或数码相机）检查纸上打印的字符，通过检测暗、亮的模式确定其形状，然后用字符识别方法将形状翻译成计算机文字的过程；
    即，针对印刷体字符，采用光学的方式将纸质文档中的文字转换成为黑白点阵的图像文件，并通过识别软件将图像中的文字转换成文本格式，供文字处理软件进一步编辑加工的技术。

### clip
    释义1：视频片段，口语里是指视频 就是一段视频叫clip。
    释义2：CLIP（Contrastive Language-Image Pre-training）模型。在大规模数据集上使用NLP监督预训练图像分类器；
    用4亿对来自网络的图文数据对，将文本作为图像标签，进行训练。进行下游任务时，只需要提供和图上的concepts(概念)对应的文本描述，就可以进行zero-shot transfer。

### 隐空间（Latent Space）
    隐空间（Latent Space）是指在统计学、机器学习和深度学习领域中，一种假设的、通常不可直接观察的多维空间，其中包含数据的潜在特征或属性。
    隐空间可以被视为一种压缩数据的表示，其中的每个点或向量都代表了数据集中的一个潜在模式或特征。
    在机器学习中，模型通过训练学习如何将输入数据映射到隐空间中，同时也学习如何从隐空间中的点重建出原始数据。
    隐空间结构的关键组成部分
    编码：将原始数据转换为隐空间中的表示。
    解码：将隐空间中的表示转换回原始数据空间。
    隐变量：隐空间中的点或向量，通常由随机过程生成。

### zero-shot transfer
    就是零样本迁移到下游任务的意思。
    对于分类问题，Zero-shot就是希望模型能够对其从没见过的类别进行分类，是指对于要分类的类别对象，一次也不学习。
    基于已训练好的大模型，只有推理阶段，没有微调训练阶段，进行直接预测。

### Zero-shot information extraction (零样本信息抽取)
    零样本信息抽取（Information Extraction，IE）旨在从无标注文本中建立IE系统，因为很少涉及人为干预，该问题非常具有挑战性。
    但零样本IE不再需要标注数据时耗费的时间和人力，因此十分重要。
    将零样本IE任务转变为一个两阶段框架的多轮问答问题（Chat IE）：实体关系三元组抽取、命名实体识别和事件抽取。

### Few-shot与One-shot
    如果训练集中，不同类别的样本只有少量，则成为Few-shot，如果参与训练学习，也只能使用较少的样本数。
    如果训练集中，不同类别的样本只有一个，则成为One-shot， 属于Few-shot的一种特殊情况。
    fine-tuning：预训练 + 训练样本计算loss更新梯度，然后预测。会更新模型参数
    zero-shot：预训练 + task description + prompt，直接预测。不更新模型参数
    one-shot：预训练 + task description + example + prompt，预测。不更新模型参数
    few-shot（又称为in-context learning）：预训练 + task description + examples + prompt，预测。不更新模型参数

### 广播(broadcast)
    将一个 2D 张量与一个向量相加。如果将两个形状不同的张量相加,如果没有歧义的话,较小的张量会被广播(broadcast),以匹配较大张量的形状。
    广播包含以下两步。
    (1) 向较小的张量添加轴(叫作广播轴),使其 ndim 与较大的张量相同。
    (2) 将较小的张量沿着新轴重复,使其形状与较大的张量相同。

### 张量点积
    点积运算,也叫张量积(tensor product,不要与逐元素的乘积弄混),是最常见也最有用的张量运算。
    与逐元素的运算不同,它将输入张量的元素合并在一起。
    注意,两个向量之间的点积是一个标量,而且只有元素个数相同的向量之间才能做点积。
    对一个矩阵 x 和一个向量 y 做点积,返回值是一个向量,其中每个元素是 y 和 x的每一行之间的点积。
    a·b=|a|×|b|cosθ；（θ为a,b之间的夹角）

### 张量变形(tensor reshaping)
    张量变形是指改变张量的行和列,以得到想要的形状。变形后的张量的元素总个数与初始张量相同。
    简单的例子可以帮助我们理解张量变形。
    >>> x = np.array([[0., 1.],
                                    [2., 3.],
                                    [4., 5.]])
    >>> print(x.shape)
    (3, 2)
    >>> x = x.reshape((6, 1))
    >>> x
    array([[ 0.],
    [ 1.],
    [ 2.],
    [ 3.],
    [ 4.],
    [ 5.]])
    >>> x = x.reshape((2, 3))
    >>> x
    array([[ 0., 1., 2.],
                [ 3., 4., 5.]])

### 阶(rank)
    张量轴的个数也叫作阶(rank)。
    例如,3D 张量有 3 个轴,矩阵有 2 个轴。这在 Numpy 等 Python 库中也叫张量的 ndim 。
    >>> x = np.random.random((3, 2))
    >>> x
    array([[0.8957804 , 0.96712402],
           [0.73855227, 0.34115149],
           [0.36667734, 0.16249242]])
    >>> x.ndim
    2

### 形状
    这是一个整数元组,表示张量沿每个轴的维度大小(元素个数)。
    例如,前面矩阵示例的形状为 (3, 5) ,3D 张量示例的形状为 (3, 3, 5) 。
    向量的形状只包含一个元素,比如 (5,) ,而标量的形状为空,即 () 。
    >>> x = np.array(3)
    >>> x.shape
    ()
    >>> x = np.array([2,3])
    >>> x.shape
    (2,)

### 数据类型(在 Python 库中通常叫作 dtype )
    这是张量中所包含数据的类型,例如,张量的类型可以是 float32 、 uint8 、 float64 等。
    在极少数情况下,你可能会遇到字符( char )张量。
    注意,Numpy(以及大多数其他库)中不存在字符串张量,因为张量存储在预先分配的连续内存段中,而字符串的长度是可变的,无法用这种方式存储。
    >>> x = np.array([2.0,3])
    >>> x.dtype
    dtype('float64')

### 标量(scalar, 0D 张量)
    仅包含一个数字的张量叫作标量(scalar,也叫标量张量、零维张量、0D 张量)。
    在 Numpy中,一个 float32 或 float64 的数字就是一个标量张量(或标量数组)。
    你可以用 ndim 属性来查看一个 Numpy 张量的轴的个数。标量张量有 0 个轴( ndim == 0 )。
    张量轴的个数也叫作阶(rank)。下面是一个 Numpy 标量。
    >>> import numpy as np
    >>> x = np.array(12)
    >>> x
    array(12)
    >>> x.ndim
    0

### 向量(vector, 1D 张量)
    向量是有顺序的一组数，这“一组数”并不是向量的本质，它只是这个向量的一个表示。
    向量（Vector）本是一个抽象概念。它存在于抽象的向量空间（Vector Space）中。
    如果一组线性独立的向量 vi(i=1,2,…,n)能够线性表出向量空间中的所有向量，则称这组向量为该向量空间的一组基（Basis）。
    一组基中的向量称为基向量（Basis Vector）。一组基的基向量个数 n 称作该向量空间的维数。
    任意向量被特定一组基线性表出的系数是唯一的。这组系数称为这个向量在这组基上的坐标（Coordinate）。
    在 n 维向量空间中可以用坐标表示一个向量。坐标就是 n 个数，而且有顺序。因为它们是向量在每个基向量上的系数，基中基向量的顺序就决定坐标的顺序。
    向量是固定的，但向量的坐标会随着基的不同(选择另外一组基向量)而变。
    （作为一列有序数的）向量是向量在一组特定基上的坐标（表示），矩阵是一个线性变换在两组特定基上的表示。
    数字组成的数组叫作向量(vector)或一维张量(1D 张量)。一维张量只有一个轴。
    下面是一个 Numpy 向量。
    >>> x = np.array([12, 3, 6, 14, 7])
    >>> x
    array([12, 3, 6, 14, 7])
    >>> x.ndim
    1
    这个向量有 5 个元素,所以被称为 5D 向量。不要把 5D 向量和 5D 张量弄混!
    5D 向量只有一个轴,沿着轴有 5 个维度,而 5D 张量有 5 个轴(沿着每个轴可能有任意个维度)。

### 嵌入向量（Embedding Vector）
    嵌入是微分拓扑中的一个概念，简单地可理解为映射。
    对于每一个实体（Item），比如物品、人、词等等，给它分配一个低维的向量（几十维或几百维）。这个向量称为这个实体的嵌入向量。
    嵌入向量是实体的一种表示（Representation），也可以看做从实体中提取的一套特征。
    嵌入是基于数据集中的数据来学习的，这意味着如果你改变数据，你的嵌入很可能会改变。
    实体的高维表示（如：One-Hot）嵌入到了低维空间。线性变换就是“嵌入层”，它本质上就是线性变换层。
    这个线性变换的矩阵称作“嵌入矩阵”，它的所有元素也属于模型参数，在反向传播+梯度下降过程中一并训练。
    训练完成后，我们就得到了所有实体的嵌入向量（嵌入矩阵的所有列）。

### 词嵌入(Word Embedding)
    词嵌入(Word Embedding)是一种将文本中的词转换成数字向量的方法，为了使用标准机器学习算法来对它们进行分析，就需要把这些被转换成数字的向量以数字形式作为输入。
    词嵌入过程就是把一个维数为所有词数量的高维空间嵌入到一个维数低得多的连续向量空间中，每个单词或词组被映射为实数域上的向量，词嵌入的结果就生成了词向量。

### 矩阵(matrix, 2D 张量)
    向量组成的数组叫作矩阵(matrix)或二维张量(2D 张量)。
    矩阵有 2 个轴(通常叫作行和列)。你可以将矩阵直观地理解为数字组成的矩形网格。
    下面是一个 Numpy 矩阵。
    >>> x = np.array([[5, 78, 2, 34, 0],
                                    [6, 79, 3, 35, 1],
                                    [7, 80, 4, 36, 2]])
    >>> x.ndim
    2
    第一个轴上的元素叫作行(row),第二个轴上的元素叫作列(column)。
    在上面的例子中,[5, 78, 2, 34, 0] 是 x 的第一行, [5, 6, 7] 是第一列。

### 3D 张量
    将多个矩阵组合成一个新的数组,可以得到一个 3D 张量,你可以将其直观地理解为数字组成的立方体。
    下面是一个 Numpy 的 3D 张量。
    >>> x = np.random.random((3, 2, 4))
    >>> x
    array([[[0.12988825, 0.77277393, 0.46243809, 0.12377541],
            [0.8033865 , 0.81821495, 0.22307274, 0.52566756]],
           [[0.80553673, 0.79450534, 0.14504729, 0.60237352],
            [0.03084524, 0.12027406, 0.45299607, 0.05332909]],
           [[0.68057101, 0.81951978, 0.65863182, 0.85608584],
            [0.68971186, 0.01891051, 0.17315731, 0.02773519]]])
    >>> x.ndim
    3

### 高维张量
    将多个 3D 张量组合成一个数组,可以创建一个 4D 张量,以此类推。
    深度学习处理的一般是 0D 到 4D 的张量,但处理视频数据时可能会遇到 5D 张量。

### 训练集(training set)
    供机器学习或深度学习模型学习的数据集；并且不是学习一遍就完了，需要多次(epochs)迭代。

### 校验集(validation set)
    每一轮(epoch)训练完后，用于测试模型效果的测试数据。

### 测试集(test set）
    全部轮数训练完后，用于评估模型效果的测试数据。

### 训练(train)和推理(inference)
    深度学习中涉及到训练（Training）和推断（Inference），简单来说：
    1、训练也就是搜索和求解模型最优参数的阶段。
    2、当模型参数已经求解出来，使用和部署模型，则称为推理阶段。
    先看一下训练时的需求。神经网络训练通常使用随机梯度下降算法，显存中除了加载模型参数，
    还需要保存中间状态，主要是梯度信息，相比推理，显存需求要增加几倍，显存要够大才能跑起来；
    要训练好的模型，需要使用大量数据，大量数据要读入显存，显存带宽要够大；
    另外对于当前的大数据量，单卡已经无法满足要求，要用多卡集群训练，集群训练要在多机间通信，
    要交换大量数据，要支持更高的通信带宽，通常还要GPU支持RDMA特性，能够直接在显存和通信卡内存间搬数据。
    总结起来就是训练要求显存大，显存带宽大，和外部通信接口带宽大，算力就不说了，都不是主要考虑问题了。
    推理时的需求就简单了，算力和显存平衡就可了，模型能装的进去，把算力跑慢就可以了，显存和算力越大，推理的并发数越多。
    训练是计算密集型的，以 GPU 资源为主，CPU 主要用于通信，参数更新等低消耗的任务。单次训练任务计算量大，需要用分布式系统才能较快得到结果。训练过程主要关心分布式集群的资源利用率。
    推理过程关注的指标为：访问延迟、吞吐量、模型版本管理等。
    传统机器学习，都会用predict表示预测，而深度学习中往往用inference这个词

### 消融实验（Ablation study）
    消融实验类似于实验方法中的控制变量法。
    在一个实验中，涉及到a,b,c三个部分，不知道那个部分对实验起到效果，如果想知道a部分对整个实验的作用，去掉a部分，从而知道a在实验中起到的效果。
    消融实验方法举例：比如为了提升baseline的性能，给它加了两个模块A,B，加完之后效果果然提高了很多。
    所以为了验证A、B两个模块是不是真的都有用，你需要做ablation study。方法也很简单：
    实验1：在baseline的基础上加上模块A，看效果。
    实验2：在baseline的基础上加上模块B，看效果。
    实验3：在baseline的基础上同时加上模块AB，看效果。
    然后结果可能是，实验1和实验2的结果都不如实验3，那么说明AB都是有用的；然而也有可能你会发现实验1的结果和实验3一样，甚至更好。这就说明:模块B其实并没有起到作用，提升只来自于模块A。
    综上所述，ablation study就是你在同时提出多个思路提升某个模型的时候，为了验证这几个思路分别都是有效的，做的控制变量实验的工作。

### CAP理论
    CAP理论：分布式计算系统不可能同时确保以下三个特性：一致性（consistency）、可用性（availability）和分区容忍性（partition）。
    互联网界的分布式系统便都将CAP理论作为自己的衡量标准。
    后续的分布式系统设计也都按照CAP理论进行设计，但是任何一个系统都只能在三个原则中满足两个，无法三个都满足。

### FLP不可能原理
    在网络可靠，但允许节点失效（即便只有一个）的最小化异步模型系统中，不存在一个可以解决一致性问题的确定性共识算法。
    FLP不可能原理，也不是意味着研究共识算法没有意义，这只是学术界研究的最极端情形。
    科学告诉你什么是不可能的；而工程要做的就是，付出一些代价，可以把它变成可行。

### 一致性（consistency）
    在分布式系统中有多个节点，整个系统对外提供的服务应该是一致的。即用户在不同的系统节点访问数据的时候应该是同样的结果，不能出现1号节点是结果1， 2号节点是结果2这种不一致的情况。
    弱一致性：弱一致性体现的是最终一致性，即如上CAP理论中 ，两个地域的请求分别通过两地的节点写入，可以不用立即进行同步，而是经过一段时间之后两地的用户数据变为一致。这种一致性成为弱一致性，即最终一致性。
    强一致性：强一致性是非常有必要的，比如业界的银行系统，支付系统等对一致性要求非常高。强一致性描述的是一个请求在一个节点写入之后在其他节点读取，则该数据的更新能够被读到。
    共识（Consensus）与一致性（Consistency）的区别：一致性是指数据不同副本之间的差异，而共识是指达成一致性的方法与过程。
    一致性并不代表结果正确与否，而是系统对外呈现的状态一致与否；例如，所有节点都达成失败状态也是一种一致。

### 可用性（availability）
    分布式系统为用户提供服务，需要保证能够在一些节点异常的情况下仍然支持为用户提供服务。

### 分区容错性（partition tolerance）
    网络可能发生分区，即节点之间的通信不可保障。 
    大多数分布式系统都分布在多个子网络。每个子网络就叫做一个区（partition）。
    分布式系统的部署可能跨省，甚至跨国。不同省份或者国家之间的服务器节点是通过网络进行连接，此时如果两个地域之间的网络连接断开，整个分布式系统的体现就是分区容错性了。
    在这种系统出现网络分区的情况下系统的服务就需要在一致性 和 可用性之间进行取舍。
    如果为了在出现网络分区之后提供仍然能够提供可用性，那么一致性必然无法满足。因为可用性的要求是系统仍然能够提供服务，
    但是这个时候分布式系统在两个地域之间无法通信，那么America的请求无法同步到China的服务节点上，整个系统的一致性显然不能满足。
    同理，为了保证一致性，那么可用性也必然不能满足，因为一致性的要求是请求A写入America之后从China中读，一定要能够读到这个请求（强一致性）或者 经过一段时间之后也能够读到（弱一致性）。
    但是两地域已经出现了网络分区，那么请求会阻塞从而降低可用性。所以CAP理论一定是无法全部满足三者，只能满足其中的两者。

### 强一致算法： 主从同步
    主从同步复制：
    1、master接受写请求
    2、master复制日志到slave
    3、master等待直到所有从节点返回
    这是一个强一致性算法的基础，但是这样的模型会出现如下问题：
    比如一个slave节点异常宕机，那么master迟迟等不到该节点的返回，此时client请求便会阻塞。
    这样的模型就是强一致模型，但是系统的可用性就会非常差，一个节点挂掉会导致整个系统不可用。

### 强一致性算法：多数派
    为了避免主从同步那样一个节点异常，整个集群不可用的情况，推出了多数派。即每次写入只需要保证写入节点数大于N/2 个，读节点数保证大于N/2个节点，此时即可认为能够向客户端返回了。
    这样既能够保证一致性，又提高了可用性。但是这个情况也只是针对单个请求写入的情况下，而我们实际生产环境中会存在大量的并发场景。
    比如：并发这个分布式系统的不同节点执行不同的操作，在满足多数派的情况下可能会出现顺序的问题，不同的节点因为请求同步的时间问题可能出现请求的顺序不一致。

### ACID原则
    ACID原则指的是：Atomicity（原子性）、Consistency（一致性）、Isolation（隔离性）、Durability（持久性）。
    这是比较有名的描述一致性的原则，通常出现在分布式数据库领域。满足一致性需求，但是允许付出可用性的代价。
    A：每次操作是原子的，要么成功，要么不执行；
    C：数据库的状态是一致的，无中间状态；
    I：各种操作之间互不影响；主要用于实现并发控制, 隔离能够确保并发执行的事务能够顺序一个接一个执行，通过隔离，一个未完成事务不会影响另外一个未完成事务。
    D：状态的改变是持久的，不会失效。一旦一个事务被提交，它应该持久保存，不会因为和其他操作冲突而取消这个事务。
    与ACID相对的一个原则是BASE（Basic Availability， Soft-state， Eventual Consistency）原则，牺牲对一致性约束（但实现最终一致性），来换取一定的可用性。

### BASE理论
    BASE理论是：
    Basic Availability： 基本业务可用性，接受响应时间变慢或者非关键模块宕机等意外情况。
    Soft state： 柔性状态（软状态），指的是允许系统中的数据存在中间状态，并认为该状态不影响系统的整体可用性。
    Eventual consistency： 最终一致性，如字面含义，允许有延迟，即开始时候不一致，但最终一致。
    BASE 理论 是对 CAP 的妥协。对于普遍的互联网场景而言，保证可用性的要求要高于一致性（金融支付类的除外），故一致性的约束降级为最终一致性。
    考虑到用户体验，这个最终一致的时间窗口，要尽可能的对用户透明。最常见的实现最终一致性的系统是DNS（域名系统Domain Name System）。
    一个域名更新操作根据配置的形式被分发出去，并结合有过期机制的缓存；最终所有的客户端可以观察到这个更新。
    另外，和 ACID 代表的刚性事务相比，BASE 代表的柔性事务面向的是大型高可用可扩展的分布式系统，通过牺牲强一致性来获得可用性。

### 权重(weight)或可训练参数(trainable parameter)
    权重是利用随机梯度下降学到的一个或多个张量,其中包含网络的知识。

### 层(layer)
    神经网络的核心组件是层(layer),它是一种数据处理模块,你可以将它看成数据过滤器。
    进去一些数据,出来的数据变得更加有用。具体来说,层从输入数据中提取表示——我们期望这种表示有助于解决手头的问题。
    大多数深度学习都是将简单的层链接起来,从而实现渐进式的数据蒸馏(data distillation)。
    深度学习模型就像是数据处理的筛子,包含一系列越来越精细的数据过滤器(即层)。
    层是一个数据处理模块,将一个或多个输入张量转换为一个或多个输出张量。有些层是无状态的,但大多数的层是有状态的,即层的权重。

### 密集层(dense layer, Dense 层)
    密集连接(也叫全连接)的神经层。
    密集连接层[densely connected layer,也叫全连接层(fully connected layer)或密集层(dense layer),对应于 Keras 的 Dense 类]来处理。

### 全连接层(fully connected layers,FC)
    全连接（Full Connect）的核心操作就是矩阵向量乘积 y = Wx；
    本质就是由一个特征空间线性变换到另一个特征空间。
    目标空间的任一维——也就是隐层的一个 cell——都认为会受到源空间的每一维的影响。
    全连接层中的每个神经元与其前一层的所有神经元进行全连接．
    全连接层（fully connected layers，FC）在整个卷积神经网络中起到“分类器”的作用。
    如果说卷积层、池化层和激活函数层等操作是将原始数据映射到隐层特征空间的话，全连接层则起到将学到的“分布式特征表示”映射到样本标记空间的作用。
    全连接层参数冗余

### 循环层(recurrent layer,比如 Keras 的 LSTM 层)

### 二维卷积层(Keras 的 Conv2D )

### softmax 层
    softmax函数，又称归一化指数函数。它是二分类函数sigmoid在多分类上的推广，目的是将多分类的结果以概率的形式展现出来。
    softmax把一些输入映射为0-1之间的实数，并且归一化保证和为1，因此多分类的概率之和也刚好为1。
    顾名思义，softmax由两个单词组成，其中一个是max。
    另外一个单词为soft(柔和)。如果将max看成一个分类问题，就是非黑即白，最后的输出是一个确定的变量。
    更多的时候，我们希望输出的是取到某个分类的概率，或者说，我们希望分值大的那一项被经常取到，而分值较小的那一项也有一定的概率偶尔被取到，所以我们就应用到了soft的概念，即最后的输出是每个分类被取到的概率。
    假设有一个数组V， Vi表示V中的第i个元素，那么这个元素的softmax值为:Si = e^i/∑j e^j ;
    该元素的softmax值，就是该元素的指数与所有元素指数和的比值。
    softmax层只是对神经网络的输出结果进行了一次换算，将输出结果用概率的形式表现出来。将网络输出值的每一维映射成（0，1）之间的概率值，且所有维的概率值之和等于1。

### 哈夫曼树（Huffman Tree）
    给定N个权值作为N个叶子结点，构造一棵二叉树，若该树的带权路径长度达到最小，称这样的二叉树为最优二叉树，也称为哈夫曼树。
    
### 分层softmax(Hierachical Softmax)
    Hierachical Softmax的基本思想就是首先将词典中的每个词按照词频大小构建出一棵Huffman树，保证词频较大的词处于相对比较浅的层，
    词频较低的词相应的处于Huffman树较深层的叶子节点，每一个词都处于这棵Huffman树上的某个叶子节点；
    第二，将原本的一个|V|分类问题变成了log(V)次的二分类问题，做法简单说来就是，使用的是普通的softmax，
    势必要求词典中的每一个词的概率大小，为了减少这一步的计算量，在Hierachical Softmax中，
    将计算当前词在其上下文中的概率大小，变成在Huffman树中的路径预测问题就可以了，
    因为当前词在Huffman树中对应到一条路径，
    这条路径由这棵二叉树中从根节点开始，经过一系列中间的父节点，最终到达当前这个词的叶子节点而组成，
    那么在每一个父节点上，都对应的是一个二分类问题（本质上就是一个LR分类器），而Huffman树的构造过程保证了树的深度为log(V)，
    所以也就只需要做log(V)次二分类便可以求得概率的大小，这相比原来|V|次的计算量，已经大大减小了。
    
### 负采样
    负采样每遍历到一个目标词，为了使得目标词的概率最大，让其他非目标词的概率最小，
    普通softmax的计算量太大就是因为它把词典中所有其他非目标词都当做负例了，
    而负采样的思想，就是每次按照一定概率随机采样一些词当做负例，从而就只需要计算这些负采样出来的负例了，
    将原来的|V|分类问题变成了K分类问题，这便把词典大小对时间复杂度的影响变成了一个常数项。

### 采样率(SampleRate)
    采样频率，也称为采样速度或者采样率，定义了单位时间内从连续信号中提取并组成离散信号的采样个数，它用赫兹（Hz）来表示。采样频率的倒数是采样周期或者叫作采样时间，它是采样之间的时间间隔。

### 编译(compile)

### 拟合(fit)

### 过拟合(overfit)
    过拟合是指机器学习模型在新数据上的性能往往比在训练数据上要差

### 标签平滑（Label Smoothing）
    标签平滑是一种损失函数的修正。在训练样本中，我们并不能保证所有的样本标签都标注正确，如果某个样本的标注是错误的，那么在训练时，该样本就有可能对训练结果产生负面影响。
    在每次迭代时，并不直接将(xi,yi)（其中yi是样本xi的标签，为0或1）放入训练集，而是设置一个错误率ε，以1-ε的概率将(xi,yi)代入训练，以ε的概率将(xi,1-yi)代入训练。
    这样，模型在训练时，既有正确标签输入，又有错误标签输入，可以想象，如此训练出来的模型不会“全力匹配”每一个标签，而只是在一定程度上匹配。
    这样，如果真的出现错误标签，模型受到的影响就会更小。
    也就是说，当标签为0时，我们并不把0直接放入训练，而是将其替换为一个比较小的数ε。
    同样的，如果标签为1，我们也将其替换为较接近的数1-ε。
    这样，在交叉熵模型中，模型输出永远不可能达到0和1，因此模型会不断增大w,使得预测输出尽可能逼近0或1，
    而这个过程与正则化是矛盾的，或者说，有可能过拟合。
    如果我们把标签0和1分别替换成ε和1-ε，模型的输出在达到这个值之后，就不会继续优化。
    因此，所谓平滑，指的就是把两个极端值0和1变成两个不那么极端的值。
    但在知识蒸馏教师模型中，不要使用标签平滑。尽管使用标签平滑化训练提高了教师的最终准确性，但与使用“硬”目标训练的教师(没有标签平滑化)相比，它未能向学生网络传递足够多的知识。
    标签平滑“擦除”了在hard目标训练中保留的一些细节。这样的泛化有利于教师网络的性能，但是它传递给学生网络的信息更少。
    丢失的信息最终会对它教授新学生模型的能力产生负面影响。因此，准确性更高的老师并不能更好地向学生提炼信息。

### 损失( loss )

### 损失函数(loss function)
    网络如何衡量在训练数据上的性能,即网络如何朝着正确的方向前进。
    该函数也叫目标函数(objective function)。
    损失函数的输入是网络预测值与真实目标值(即你希望网络输出的结果),然后计算一个距离值,衡量该网络在这个示例上的效果好坏。
    损失函数(目标函数)——在训练过程中需要将其最小化。它能够衡量当前任务是否已成功完成。

### 回归损失函数
    * 均方差 Mean Squared Error (MSE) 损失是机器学习、深度学习回归任务中最常用的一种损失函数，也称为 L2 Loss。其基本形式如下：
    Jmse = sum(math.pow(yi-yi', 2))/N
    事实上，在模型输出与真实值的误差服从高斯分布的假设下，最小化均方差损失函数与极大似然估计本质上是一致的，
    因此在这个假设能被满足的场景中（比如回归），均方差损失是一个很好的损失函数选择；
    当这个假设没能被满足的场景中（比如分类），均方差损失不是一个好的选择。
    Keras中的用法:
    model.compile(loss='mean_squared_error', optimizer ='sgd')
    
    * 平均绝对误差（Mean Absolute Error，MAE）
    平均绝对误差 Mean Absolute Error (MAE) 是另一类常用的损失函数，也称为 L1 Loss。其基本形式如下：
    Jmae = sum(abs(yi-yi'))/N
    模型预测与真实值之间的误差服从拉普拉斯分布 Laplace distribution（“即当拉普拉斯分布与高斯分布方差相等时，
    相比于高斯分布，拉普拉斯分布在均值附近和远离均值处有更高概率密度，也就是说随机采样时，拉普拉斯分布更容易抽样到均值附近和远离均值的样本”），
    最小化平均绝对误差损失函数与极大似然估计本质上是一致的。
    Keras中的用法：
    model.compile(loss='mean_absolute_error', optimizer='sgd')
    MAE与MSE区别
    MAE 和 MSE 作为损失函数的主要区别是：MSE 损失相比 MAE 通常可以更快地收敛，但 MAE 损失对于 outlier（离群值，也称逸出值） 更加健壮，即更加不易受到 outlier 影响。
    
    * Huber loss
    MSE 损失收敛快但容易受 outlier 影响，MAE 对 outlier 更加健壮但是收敛慢，
    Huber Loss 则是一种将 MSE 与 MAE 结合起来，取两者优点的损失函数，也被称作 Smooth Mean Absolute Error Loss 。
    其原理很简单，就是在误差接近 0 时使用 MSE，误差较大时使用 MAE。
    Huber Loss 结合了 MSE 和 MAE 损失，在误差接近 0 时使用 MSE，使损失函数可导并且梯度更加稳定；
    在误差较大时使用 MAE 可以降低 outlier 的影响，使训练对 outlier 更加健壮。缺点是需要额外地设置一个超参数δ。
    在[-δ, δ]的区间内就是MSE损失，在(-∞, -δ)和(δ, ∞)的区间内未MAE损失。
    Keras 中的用法：
    model.compile(loss='huber_loss', optimizer='sgd')

### 分类损失函数
    * 交叉熵损失函数（Cross Entropy Loss， CE）
    对于分类问题，最常用的损失函数是交叉熵损失函数 Cross Entropy Loss。
    Keras中的用法:
    model.compile(loss='categorical_crossentropy', optimizer='sgd')
    当使用 categorical_crossentropy 损失时，你的目标值应该是分类格式 (即，如果你有 10 个类，每个样本的目标值应该是一个 10 维的向量，这个向量除了表示类别的那个索引为 1，其他均为 0)。
    一般而言，在Keras中使用‘categorical_crossentropy’处理多分类问题，针对于二分类问题，有专门的’binary_crossentropy’
    
    * 合页损失 （Hinge loss）
    合页损失 Hinge Loss 是另外一种二分类损失函数，适用于 maximum-margin 的分类，支持向量机 Support Vector Machine (SVM) 模型的损失函数本质上就是 Hinge Loss + L2 正则化。
    Hinge loss专用于二分类问题，标签值y=±1，预测值y^∈R。该二分类问题的目标函数的要求如下： 
    当y^大于等于+1或者小于等于-1时，都是分类器确定的分类结果，此时的损失函数loss为0；
    而当预测值y^∈(−1,1)时，分类器对分类结果不确定，loss不为0。显然，当y^=0时，loss达到最大值。
    对于输出y=±1，当前y^的损失为：loss(y) = max(0, 1-y*y^)
    Hinge loss是一个凸函数(convex function)，所以适用所有的机器学习凸优化方法。 
    虽然Hinge loss函数不可微，但我们可以求它的分段梯度;
    Keras中的使用：model.compile(loss='hinge', optimizer='sgd')

### 梯度调和机制（gradient harmonized mechanism, GHM, GHM-C Loss）
    将不同梯度对损失函数的影响，进行基于密度的平衡。
    样本的梯度分布中，梯度小的表示已经被模型学习到的了（容易样本），梯度大的表示模型很难学到（困难样本）。这两种样本，梯度密度都比较大，主导了整个模型的训练方向。
    而该论文作者认为，学到的了可以不用学了，没有学到的可能是异常样本，也不用学了；我们这时候应该提升模型，学习中间那段梯度密度较小、还有信息可以学习的样本。
    求解出整个样本集梯度的概率密度的调和曲线（1/概率密度），来调和原始的梯度（也就是与样本的梯度分布相乘），得到最终的模型回传梯度。
    论文通过两个机制来近似这个梯度密度：
    将梯度取值区间(0, 1)切割为多个bin，统计不同bin的梯度数量R，作为梯度密度；
    用逐batch的指数加权移动平均 (EMA) 来近似总样本下的梯度密度。
    它是先将梯度模长划分为bins个范围，然后计算每个区域的边界，接着就根据边界判断梯度模长落在哪个区间里了，然后按照公式计算对应区间权重乘以原来的交叉熵损失即可（实现思路和上面的Focal Loss差不多，都是对原本的交叉熵加权）

### 优化器(optimizer)
    基于训练数据和损失函数来更新网络的机制。
    利用网络预测值与真实目标值的距离值作为反馈信号来对权重值进行微调,以降低当前示例对应的损失值。
    优化器——决定如何基于损失函数对网络进行更新。它执行的是随机梯度下降(SGD)的某个变体。

### 指标(metric)

### 轮次(epoch)
    在所有训练数据上迭代一次叫作一个轮次(epoch)

### 模型的深度(depth)
    数据模型中包含多少层,这被称为模型的深度(depth)。

# reduce_sum
    是从维度上去考虑元素相加(求和)。
    前缀reduce_，是“对矩阵降维”的含义，下划线后面的部分(sum)就是降维的方式，在reduce_sum()中就是按照求和的方式对矩阵降维。
    同理，reduce_mean()就是按照某个维度求平均值。

### logistic 回归(logistic regression,简称 logreg, LR)
    logreg 是一种分类算法,而不是回归算法。
    Logistic回归分析也用于研究影响关系，即X对于Y的影响情况。Y为定量数据，X可以是定量数据或定类数据。
    Logistic回归和线性回归最大的区别在于，Y的数据类型。线性回归分析的因变量Y属于定量数据，而Logistic回归分析的因变量Y属于分类数据。
    Logistic回归在进一步细分，又可分为二元Logit（Logistic）回归(分类数据有且仅有两类)、
    多元无序Logit（Logistic）回归(分类数据超过两类，类别之间没有对比意义)，
    多元有序Logit（Logistic）回归(分类数据超过两类，类别之间具有对比意义，如：非常满意，满意，不满意，非常不满意)。
    在CTR模型的基本框架下的LR,是将特征加权求和并经过sigmoid即得到CTR值。

### 因子分解机(Factorization Machine, FM)
    因子分解机(Factorization Machine, FM)是一种基于矩阵分解的机器学习算法。目前，被广泛的应用于广告预估模型中，相比LR而言，效果强了不少。
    FM主要目标：解决数据稀疏的情况下，特征怎样组合的问题。
    是一种不错的CTR(Click-Through-Rate,点击通过率)预估模型，也是我们现在在使用的广告点击率预估模型，比起著名的Logistic Regression, FM能够把握一些组合的高阶特征，因此拥有更强的表现力。
    在逻辑回归(LR,Logistic Regression)的基础上引入二阶特征交叉，为每一维特征训练得到相应的隐向量，通过隐向量的内积得到特征交叉权重。
    用两个隐向量的内积来表示特征组合的权重。这种通过更新隐向量来更新组合权重的方式不需要保证该组合存在，从而使模型具有一定的泛化性。
    相比逻辑回归，具备了二阶特征交叉能力，模型的表达能力增强；但只能建模二阶的特征交叉，不容易扩展到更高阶的特征交叉。

### AFM(Attention neural Factorization Machine, 注意力因子分解机)
    与LR相比，FM增加了二阶项的信息，通过穷举所有的二阶特征（一阶特征两两组合）并结合特征的有效性（特征权重）来预测点击结果，FM的二阶特征组合过程可拆分成Embedding和内积两个步骤。
    两个特征内积的过程如下：两个特征向量做哈达玛积(Hadamard product)得到1*k的二阶组合特征向量，再将这个向量沿嵌入维度求和（sum by dimension）得到一个实数值。
    AFM通过注意力网络学习二阶组合特征的重要性，将所有的二阶组合特征向量进行加权求和作为Attention Net部分的输出。
    AFM首先利用FM解决稀疏特征问题及浅层交互特征，同时利用深度注意力网络获取深层交互特征。
    模型的核心是注意力层（Attention-based Pooling Layer）通过关注不同的交叉特征和目标之间的关系，得到不同程度的贡献分数，然后加权求和通。
    同时利用MLP进一步处理训练数据中未出现样本的的评估问题，从而达到泛化模型的目的。

### IAFM（Interaction-aware FM）
    从特征层面和特征组层面共同影响二阶组合特征的重要性。其中在特征组层面，通过网络学习特征所在特征组之间的重要性向量。
    最后将二阶特征向量和特征组向量做哈达玛积(Hadamard product), 再求和得到Attention Net部分的输出。

### NFM(Neural Factorization Machines)
    NFM使用深度神经网络(DNNs)取代FM中的二阶交叉特征(通过两个隐向量的内积来表示特征组合的权重)部分，让DNNs来负责特征交叉的建模；
    使得网络能够突破阶数的限制，学习更加高阶的交叉特征。
    同时，为了防止网络层数过深导致的退化问题，还设计了一种新的特征交叉网络层——Bi-Interaction Layer。
    将LR、MLP和Quadratic Layer(二阶特征组合层)串连可得到NFM，注意这里的Quadratic Layer和原始FM模型里有些许不同。
    NFM采用了Bi-interaction Pooling方法将原本输出为数值的FM变为输出为一个k维的embedding,然后放入DNN中进行更高维度的交叉。
    NFM将FM得到的交互特征用于DNN层的输入，并使用Bi-interaction Pooling操作对二阶交叉特征进行处理，解决传统FM作为线性模型表达有限的问题和对高阶交叉特征学习不充分的问题

### DeepFM
    将LR、MLP和Quadratic Layer并联可得到DeepFM，注意到MLP和Quadratic Layer共享Group Embedding。
    DeepFM模型分为两部分，分别为FM部分与DNN部分。
    FM部分的又分为Linear部分以及Second-order Interaction(二阶交互特征)部分。
        在Linear部分我们需要分别对dense feature和sparse feature进行处理。
        dense_inputs用来存储所有的dense输入，在得到dense特征的列表后，我们将其进行concat，得到concat_dense_inputs；紧接着连接一个全连接层即可，这时我们得到了dense部分特征进行线性加和的结果。
        对于每一个 sparse 特征，一般都是进行one-hot以后再转化为embedding特征，但实际上由于稀疏性的存在，很多位置的x取0时，对应的 w*x 也为0。
        因此，可以将 sparse 特征 embedding 到 1维，然后通过 embedding lookup 的方式来找到对应的w，即对sparse线性部分系数进行处理。
        至此，完成了对 Dense 特征与 Sparse 特征的处理，接下来就是将二者的结果再求和。
        具体求和过程包括：1.将所有sparse 的embedding拼接起来，得到(n,k)的矩阵，其中n为特征数，k为embedding大小；2.先求和再平方；3.先平方再求和；4.将2和3的结果相减除以2.
    构造完线性部分后，我们对FM的二阶交叉部分进行构造，同样的我们需要对sparse部分进行k维的embedding，如，k=8。
    代码部分与sparse线性部分处理类似，只不过这里的embedding output维度由原先的1变成了8。
    得到embedding后，就需要进行二阶交叉系数的计算。
    DeepFM中的DNN部分与FM部分是share embedding layer的，因此直接将FM部分的sparse embedding进行flatten以后接入多个fully-connected layer即可。
    在构造完FM与DNN部分后，我们将FM的线性部分、二阶交叉部分、DNN部分进行加和，再使用sigmoid激活函数得到最终的预估值。

### CrossNet(Cross网络，Cross Net)
    CrossNet输入/输出都是向量，输出维度固定，等于网络输入的向量维度；每层包含上一层输出信息；

### CIN(Compression Interaction Network, 压缩交互网络)
    CIN也是一个堆叠型网络，该部分的初始输入是一个(f,k)的矩阵。每层计算过程如下：输入矩阵(Hi, k)和初始输入矩阵沿嵌入维度方向做Cartesian product得到(Hi, f, k)的三维矩阵，再重新投影成(Hi+1,k)矩阵。
    CIN的最后一层：将CIN中间层的输出矩阵沿嵌入维度方向做sum pooling得到(H1,1),(H2,1)...(Hl,1)的向量，再将这些向量concat起来作为CIN网络的输出。
    CIN输入/输出都是矩阵，输出维度行不固定，列固定为嵌入向量维度；每层不包含上一层输出信息；
    CIN第k层的第i个特征向量是由第k-1层的每一个特征向量与第0层的每一个特征向量进行Hadamard乘积然后乘上一个系数矩阵最后全部相加得到的。所以说特征交叉是显示的。

### xDeepFM
    将LR、MLP和CIN并联可得到xDeepFM

### FFM(域感知因子分解机, Field-aware Factorization Machine, FFM)
    在FM模型的基础上，加入特征域的概念，使每个特征在与不同域的特征交叉时采用不同的隐向量；相比FM模型，进一步提升了特征交叉能力，但模型的训练开销增加，效果提升有限。

### GBDT+LR(梯度提升决策树(Gradient Boosting Decision Tree) +逻辑回归)
    利用GBDT自动提取特征组合，将原始特征转化为组合后的离散特征，最后输入到逻辑回归模型。
    模型自动学习特征组合，具备了更高阶特征组合的能力；但GBDT无法实时训练，更新时间长。

### LS-PLM(大规模分片线性模型,Large Scale Piece-wise Linear Model, LS-PLM)
    首先对样本进行“分片”,在每个“分片”内部构建逻辑回归模型，将每个样本的各“分片”概率与逻辑回归的得分进行加权平均，得到最终的预估值。
    模型结构类似三层神经网络，具备了较强的表达能力；模型结构相比深度学习模型仍比较简单，有进一步提高的空间。

### MLR
    利用结构性先验，对样本分片建模，在每个分片内部构建逻辑回归模型，最后将分片概率和逻辑回归得分进行加权平均。
    模型适应性强，能解决解决用户分组、位置bias不同问题；但模型结果相对简单，表达能力有限。

### PNN(Product-based Neural Network)
    将Inner/Outer Product Layer和MLP串连可得到PNN模型。

### OENN（Order-aware Embedding Neural Network for CTR Prediction）
    OENN相同特征在不同阶交互时应当使用不同嵌入向量，对于大于3阶的交互过程则使用CIN替代。

### OANN（Operation-aware Neural Networks for User Response Prediction）
    OANN认为相同特征在交互过程中执行不同的操作应当使用不同嵌入向量，例如一共有f个特征，每个特征与其他特征做Inner Product会执行(f-1)次操作，
    加上不交互的嵌入向量，即一个特征需要有f个对应的嵌入向量。

### FGCNN（Feature Generation by Convolutional Neural Network）
    FGCNN是在IPNN的基础上串连了一个Feature Generation Layer。
    Feature Generation由Convolutional Layer+MaxPooling Layer+FC组成，CNN提取useful neighbor feature patterns，
    将MaxPooling得到的特征组信息拍平了通过一个FC可提取global feature interactions.

### FiBiNET
    FiBiNET引入CV中的SENET和设计了Bilinear-Interaction。SENET是一个比较有效的特征提取方法，共分为三个部分：Squeeze，Excitation和Re-Weight。
    Binear-Interaction则是在特征之间加入(k,k)的权重参数矩阵进行计算，
    设置了三种模式：共享（Field-All）、特征组共享（Field-Each）、特征独享（Field-Interaction），
    分别要训练1个、f个、f*(f-1)/2个维度为(k,k)的权重参数矩阵。

### AutoInt
    AutoInt可以看做将MLP的FC部分替换成Multi-head Self-Attention。
    模型主要组成是嵌入层(Embedding Layer)，交互层(Interacting Layer),输出层(Output Layer)。
    AutoInt首先使用不同的嵌入层将输入的离散数据和连续数据分别映射到同一低维空间。
    交互层(Interacting Layer)是AutoInt的核心，它使用了Multi-head Self-Attention来构造特征间的高阶组合。
    对于每个从Embedding Layer取得的向量, 使用不同的注意力函数(Attention Head) ,计算向量对间的相似度; 
    最后把不同注意力函数的输出作拼接，然后使用了残差连接作为Interacting Layer的最终输出。
    输出层(Output Layer)把所有在Interacting Layer学到的向量连接其来，然后把它作为全连接层(fully connected layer,FC)的输入，再通过激活函数sigmoid给出预测的点击率。

### WDL(Wide & Deep Learning for Recommender Systems)
    将LR和MLP并联即可得到Wide&Deep模型，可同时学习一阶特征和高阶特征。
    WDL模型主要包含两部分，Wide侧是一个线性模型LR；Deep侧是一个DNN模型。
    最终，输出层将Wide部分和Deep部分组合起来，形成统一的输出。
    Google经典CTR预估模型。WDL模型巧妙的将传统特征工程与深度模型进行了强强联合。
    WDL模型中的Wide部分主要作用是让模型具有较强的记忆能力，Deep部分的主要作用是让模型具有泛化能力，这样的特点使得WDL能够兼容复杂的人工交叉特征，同时学习到更复杂的高阶交叉。
    在早期的推荐模型中，由于表达能力有限，需要设计大量的人工交叉特征，这部分特征已经具备了很强的交叉能力，因此模型只要“记忆”这部分特征。而对于未挖掘的潜在模式，需要模型自动组合特征，模型需要具备很强的泛化能力。
    DNN和逻辑回归联合训练，利用Wide部分加强模型的记忆能力，利用Deep部分加强模型的泛化能力；开创了深度学习在推荐系统的应用；但Wide部分依赖人工进行特征选择。

### AutoRec(自编码器推荐)
    AutoRec是一个新型的基于自动编码器的协同过滤模型。
    AutoRec模型将自编码器(AutoEncoder)的思想与协同过滤(Collaborative Filter)的思想结合起来，提出了一种单隐层的简单神经网络推荐模型。
    基本原理是利用协同过滤中的共现矩阵，完成物品向量或用户向量的自编码。再利用自编码的结果得到用户对物品的预测评分，再进行排序，得到Top K。
    AutoRec模型使用一个单隐层的AutoEncoder泛化用户或物品评分，使模型具有一定的泛化和表达能力。由于AutoRec模型的结构比较简单，使其存在一定的表达能力不足的问题。

### DCN(Deep&Cross Network)
    DCN网络与DeepFM类似，都采用双路结构，DCN一路是CrossNet，用来捕捉显式高阶交叉特征；一路是DNN用来捕捉隐式交叉特征。
    对WDL模型的Wide部分进行改进，使用交叉能力更强的Cross网络；解决了WDL模型人工组合特征的问题，但Cross网络设计过于简单，效果有限；
    将LR、MLP和Cross Net并联可得到DCN。Cross Net是一个堆叠型网络，该部分的初始输入是将f个(1,k)的特征组向量concat成一个(1,f*k)的向量。
    每层计算过程：输入向量和初始输入向量做Cartesian product得到(f*k,f*k)的矩阵，再重新投影成(1,k)向量，每一层输出都包含输入向量。
    在DCN网络中，由下到上主要包括五种类型的层，第一种是Embedding层，对于CTR问题的输入，通常是由一些离散的特征和连续的特征组成，
    对于离散的特征的处理方法通常是利用one-hot编码对其离散化处理，但是处理后的特征通常是较为稀疏的，如类别特征，假设通过one-hot编码后得到的特征为[0 , 1 , 0]。
    这样的特征不适合DNN处理，通常需要通过Embedding将其转换成连续的向量表示，这便是Embedding层的作用。
    第二种是Stacking层，用于组合Embedding层的输出；这里的Embedding特征包括了离散特征的Embedding结果以及原始的连续特征。
    Stacking层的作用是将Embedding的输出特征和数值型特征拼接在一起，形成新的包含全部特征的特征向量。
    第三种是Cross network的层，用于对Stacking后的特征进行学习；其作用是利用深度神经网络充分挖掘特征中的交叉特征。
    通过多层残差网络对特征向量各个维度进行充分的交叉组合，使模型能够抓取到更多的非线性特征和组合特征的信息，进而使深度学习模型在表达能力上较传统机器学习模型大为增强。
    残差层由残差单元构造而成。Deep Crossing简单的修改了残差单元，不适用卷积核。残差单元的独特之处在于两个，
    (1)它是将原输入特征通过两层以ReLU为激活函数的全连接层后，生成输出向量 。
    (2) 输入可以通过一个短路通路直接与输出向量进行元素加操作，生成最终的输出向量。
    在这样的结构下，残差单元中的两层ReLU网络其实拟合的是输出和输入之间的“残差”,这就是残差神经网络名称的由来。
    第四种是Deep network的层，deep network是一个传统的全连接前向传播神经网络，用来学习高维非线性特征交叉组合。
    此外，Cross network的层和Deep network的层是并行的两个过程；
    第五种是输出层，即“Combination output layer”（组合层），它结合了cross network和deep network两个网络的输出。
    经过Cross network的层和Deep network的层后，组合两者的输出做最后的计算。
    Combination Layer把Cross Network和Deep Network的输出拼接起来，然后经过加权求和后得到logits，然后经过sigmoid函数得到最终的预测概率。

### DCN-v2, DCNMIX, CrossNetMix
    对DCN的Cross网络进行改进，将交叉层的权重从n维向量变成n×n的矩阵，同时引入MOE结构增强不同子空间特征交叉能力；
    加强的Cross网络的特征组合能力，使用MOE结构进一步提升了特征交叉能力，但模型复杂度显著增加。

### DIN(Deep Interest Network,深度兴趣网络)
    使用注意力机制建模用户动态兴趣，计算目标广告和历史点击的相关性，最后加权和得到用户动态向量；用户兴趣的动态表达，根据目标广告的不同，进行更有针对性的推荐。
    但没有充分利用历史行为的序列信息。

### DIEN(Deep Interest Evolution Network, 深度兴趣进化网络)
    使用序列模型模拟用户行为或用户兴趣的演化趋势。
    对DIN模型进行改进，使用注意力机制筛选和目标广告相关的兴趣演化路径，使用序列模型模拟用户兴趣的迁移。序列模型增强了对用户兴趣变迁的表达能力，使模型具备了预测“下一次点击”的能力。
    但序列模型增加了计算开销，线上服务延迟显著增加。

### BST
    使用Transformer建模用户行为序列，序列信息中引入类别特征解决冷启动；引入Transformer，提升了序列模型的表达能力；但相比DIEN，只是将序列模型从GRU换成了Transformer.

### DSIN
    优化DIEN不能处理更长序列的问题，将用户行为序列切分成不同session,对新的session序列进行建模；根据用户行为特点，引入session概念，能够处理更长的行为序列；
    但依赖于数据分布，用户行为不具备session结构的，效果会受影响。

### MIMN
    将用户行为划分为多个通道，对每个通道生成用户兴趣向量；更精准把握兴趣变迁过程，避免不同兴趣的相互干扰；但在处理更大规模的序列长度时，容易被噪声数据干扰，导致效果不理想。

### SIM
    用户超长序列建模方案，一阶段基于搜索提取和目标广告相关的新闻，二阶段使用更复杂模型建模子序列；将用户序列长度从1000级别扩展到10万级别；需要离线构建索引，存储开销显著增加。

### CAN
    建模特征之间的协同关系(co-action),把co-action希望建模的两个ID，一端作为输入，另一端作为MLP的参数，用MLP输出表示co-action信息；
    能够达到笛卡尔积交叉的效果，同时具备一定的信息共享，模型参数显著增加。

### 监督学习(supervised learning)
    监督学习是目前最常见的机器学习类型。给定一组样本(通常由人工标注),它可以学会将输入数据映射到已知目标[也叫标注(annotation)]。
    其目标是学习训练输入与训练目标之间的关系。

### 序列生成(sequence generation)
    给定一张图像,预测描述图像的文字。序列生成有时可以被重新表示为一系列分类问题,比如反复预测序列中的单词或标记。

### RAG (Retrieval-Augmented Generation)：检索增强型生成
    检索增强型生成是一种方法，通过将外部数据源整合到大型语言模型（LLMs）中，来增强模型的能力。它使用稀疏或密集的检索器来实现，允许使用私有数据而不需要重新训练或微调语言模型；
    RAG允许语言模型更自由地结合其内在知识和检索到的知识来生成文本。
    RAG可能会将检索到的知识与语言模型的内在知识更紧密地结合，这可能导致生成的文本中包含混合的信息源。
    由于RAG在生成文本时可能会混合使用检索到的知识与模型的内在知识，这可能导致在某些情况下生成的信息不够准确或出现“幻觉”（hallucinations）。
    RAG中会混用外部检索到的上下文以及大模型内部的知识。

### ROG (Retrieval-OFF Generation)：检索关闭型生成
    在检索关闭型生成方法中，检索器被完全禁用，语言模型仅依赖其自身的知识库来生成响应；

### RCG (Retrieval-Centric Generation)：检索中心型生成
    检索中心型生成区分了大型语言模型（LLMs）和检索器在上下文解释和知识记忆方面的角色。通过将上下文解释与知识记忆分开，这种方法可能提高生成AI系统的性能和可解释性。
    RCG方法强调了检索器在知识记忆方面的重要性，并将上下文解释的任务更多地交给了语言模型。
    RCG方法强调在语言模型和检索器之间进行明确的角色分离。语言模型主要负责上下文解释，而检索器负责知识记忆和提供相关信息。
    RCG则更倾向于使用检索到的知识作为生成文本的主要信息源，语言模型使用这些信息来生成回答，从而保持了知识的清晰来源。
    RCG通过明确区分上下文解释和知识记忆，有助于减少幻觉风险，因为它限制了语言模型生成的范围，更多地依赖于检索到的确切信息。
    RCG完全使用外部检索的上下文。
    基于大模型来做RCG，其实很难实现，很难从技术上将RCG方法中所提到的在语言模型和检索器之间进行明确的角色分离进行实现，模型内部的参数化知识本身就很难与外部知识进行解耦。

### 语法树预测(syntax tree prediction)
    给定一个句子,预测其分解生成的语法树。

### 目标检测(object detection)
    给定一张图像,在图中特定目标的周围画一个边界框。这个问题也可以表示为分类问题(给定多个候选边界框,对每个框内的目标进行分类)或分类与回归联合问题(用向量回归来预测边界框的坐标)。

### 图像分割(image segmentation)
    给定一张图像,在特定物体上画一个像素级的掩模(mask)

### 降维(dimensionality reduction)
    降维（Dimensionality Reduction） 是机器学习中的一种重要的特征处理手段，它可以减少计算过程中考虑到的随机变量（即特征）的个数，
    其被广泛应用于各种机器学习问题中，用于消除噪声、对抗数据稀疏问题。它在尽可能维持原始数据的内在结构的前提下，得到一组描述原数据的，低维度的隐式特征（或称主要特征）。
    两个常用的降维方法：
    1、奇异值分解（Singular Value Decomposition，SVD）
    2、主成分分析（Principal Component Analysis，PCA）

### 聚类(clustering)
    聚类 (Clustering) 是按照某个特定标准 (如距离)把一个数据集分割成不同的类或簇，使得 同一个簇内的数据对象的相似性尽可能大，同时不在同一个簇中的数据对象的差异性也尽可能地大 。

### LSH算法 LSH（locality sensitivity Hashing，局部敏感性哈希）
    LSH（locality sensitivity Hashing，局部敏感性哈希）算法是一种海量数据中进行相似性搜索的算法。

### 样本(sample)或输入(input)
    进入模型的数据点。

### 预测(prediction)或输出(output)
    从模型出来的结果。

### 目标(target)
    真实值。对于外部数据源,理想情况下,模型应该能够预测出目标。

### 预测误差(prediction error)或损失值(loss value)
    模型预测与目标之间的距离。

### 误差(error)
    样本误差是指样本对母本(无法观察到的)均值及真实值的均值的偏离. 
    误差:即观测值与真实值的偏离;
    误差与测量有关，误差大小可以衡量测量的准确性，误差越大则表示测量越不准确。
    误差分为两类：系统误差与随机误差。其中，系统误差与测量方案有关，通过改进测量方案可以避免系统误差。
    随机误差与观测者，测量工具，被观测物体的性质有关，只能尽量减小，却不能避免。
    误差: 所有不同样本集的均值,与真实总体均值的偏离.由于真实总体均值通常无法获取或观测到,因此通常是假设总体为某一分布类型,则有N个估算的均值; 表征的是观测/测量的精确度;
    误差大,由异常值引起.表明数据可能有严重的测量错误;或者所选模型不合适,;

### 残差(Residuals)
    残差则是指样本和观察值(样本总体)或回归值(拟合)的差额. 
    残差:观测值与拟合值的偏离.
    残差――与预测有关，残差大小可以衡量预测的准确性。残差越大表示预测越不准确。残差与数据本身的分布特性，回归方程的选择有关。
    残差: 某样本的均值与所有样本集均值的均值的偏离; 表征取样的合理性,即该样本是否具代表意义;
    残差大,表明样本不具代表性,也有可能由特征值引起.
    反正要看一个模型是否合适,看误差;要看所取样本是否合适,看残差;

### 类别(class)
    分类问题中供选择的一组标签。例如,对猫狗图像进行分类时,“狗”和“猫”就是两个类别。

### 标签(label)
    分类问题中类别标注的具体例子。比如,如果 1234 号图像被标注为

### 真值(ground-truth)或标注(annotation)
    数据集的所有目标,通常由人工收集。
### 二分类(binary classification)
    一种分类任务,每个输入样本都应被划分到两个互斥的类别中。

### 多分类(multiclass classification)
    一种分类任务,每个输入样本都应被划分到两个以上的类别中,比如手写数字分类。

### 多标签分类(multilabel classification)
    一种分类任务,每个输入样本都可以分配多个标签。
    举个例子,如果一幅图像里可能既有猫又有狗,那么应该同时标注“猫”标签和“狗”标签。每幅图像的标签个数通常是可变的。

### 标量回归(scalar regression)
    目标是连续标量值的任务。预测房价就是一个很好的例子,不同的目标价格形成一个连续的空间。

### 向量回归(vector regression)
    目标是一组连续值(比如一个连续向量)的任务。如果对多个值(比如图像边界框的坐标)进行回归,那就是向量回归。

### 小批量(mini-batch)或批量(batch)
    模型同时处理的一小部分样本(样本数通常为 8~128)。
    样本数通常取 2 的幂,这样便于 GPU 上的内存分配。
    训练时,小批量用来为模型权重计算一次梯度下降更新。

### 特征图(feature map)
    对于包含两个空间轴(高度和宽度)和一个深度轴(也叫通道轴)的 3D 张量,其卷积也叫特征图(feature map)。
    应该是特征图的意思，是指每个卷积核和输入卷积后形成的特征图，特征图的个数和卷积核的个数相同

### 填充(padding)
    填充(padding)是指在输入高和宽的两侧填充元素(通常是 0 元素)。

### 步幅(stride)
    在卷积神经网络中，卷积窗口从输入数组的最左上方开始,按从左往右、从上往下的顺序,依次在输入数组上滑动。我们将每次滑动的行数和列数称为步幅(stride)。

### 范数(norm)
    距离的定义是一个宽泛的概念，只要满足非负、自反、三角不等式就可以称之为距离。
    范数是一种强化了的距离概念，它在定义上比距离多了一条数乘的运算法则。
    有时候为了便于理解，我们可以把范数当作距离来理解。
    范数是把一个事物映射到非负实数，且满足非负性、齐次性、三角不等式，符合以上定义的都可以称之为范数
    范数包括向量范数和矩阵范数
    不同的范数表示不同的度量方法，就好比米和光年都可以来度量远近一样；

### 向量范数
    向量范数表征向量空间中向量的大小.
    一种非严密的解释就是，对应向量范数，向量空间中的向量都是有大小的，这个大小如何度量，就是用范数来度量的，
    不同的范数都可以来度量这个大小，就好比米和光年都可以来度量远近一样；
    
    L0范数： L0范数表示向量中非零元素的个数。
    1-范数：，即向量元素绝对值之和; np.linalg.norm([1,2,3], ord=1)
    2-范数：，Euclid范数（欧几里得范数，常用计算向量长度），即向量元素绝对值的平方和再开方;
                np.linalg.norm([1,2,3], ord=2)
                Out[13]: 3.7416573867739413
                math.pow(1*1+2*2+3*3, 0.5)
                Out[9]: 3.7416573867739413
    ∞-范数：，即所有向量元素绝对值中的最大值
                np.linalg.norm([1,2,3], ord=np.inf)
                Out[16]: 3.0
    -∞-范数：，即所有向量元素绝对值中的最小值。
                np.linalg.norm([1,2,3], ord=-np.inf)
                Out[17]: 1.0
    p-范数：，即向量元素绝对值的p次方和的1/p次幂。

### 矩阵特征值；特征向量
    设A是n阶方阵，如果数λ和n维非零列向量x使关系式Ax=λx成立，那么这样的数λ称为矩阵A特征值，非零向量x称为A的对应于特征值λ的特征向量。
    式Ax=λx也可写成( A-λE)X=0。这是n个未知数n个方程的齐次线性方程组，它有非零解的充分必要条件是系数行列式| A-λE|=0。
    np.linalg.eig(np.array([[1, 0, 0],
       [0, 5, 0],
       [0, 0, 9]]))
    Out[43]: 
    (array([1., 5., 9.]), array([[1., 0., 0.],
            [0., 1., 0.],
            [0., 0., 1.]]))

### 特征穿越（标签泄漏，label leakage）
    当您要预测的信息直接或间接出现在训练数据集中时，就会发生标签泄漏或目标泄漏。   
    它会导致模型夸大其泛化误差，并极大地提高了模型的性能，但模型对于任何实际应用都毫无用处。  
    特征泄漏不仅可以通过训练特征作为标签的间接表示来实现。 也可能是因为来自验证或测试数据的某些信息保留在训练数据中，或者使用了来自将来的历史记录。
    特征穿越的本质对于使用过去以及当下信息来预测未来的AI算法模型，特征穿越本质上是，特征中包含了未来的信息。
    对于线上推理过程，构建特征所使用的信息只能来自当下或过去，自然不存在特征穿越问题。
    而对于线下训练过程，构建特征时可能会误引入样本发生时刻之后的信息，导致特征穿越。

### 矩阵范数
    矩阵范数表征矩阵引起变化的大小。
    对于矩阵范数，通过运算AX=BAX=B，可以将向量X变化为B，矩阵范数就是来度量这个变化大小的。
    不同的范数都可以来度量这个大小，就好比米和尺都可以来度量远近一样；
    一个集合（向量），通过一种映射关系（矩阵），得到另外一个集合（另外一个向量）。
    矩阵的范数，就是表示这个变化过程的大小的一个度量。矩阵范数反映了线性映射把一个向量映射为另一个向量，向量的“长度”缩放的比例。
    1-范数：， 列和范数，即所有矩阵列向量绝对值之和的最大值，
                np.linalg.norm([[1,2,3], [4, 5, 6]], ord=1)
                Out[19]: 9.0
    2-范数：，谱范数，即AT·A矩阵的最大特征值的开平方。
                np.linalg.norm([[1,2,3], [4, 5, 6]], ord=2)
                Out[31]: 9.508032000695724
                x = np.array([[1,2,3], [4, 5, 6]])
                x.T
                Out[36]: 
                array([[1, 4],
                       [2, 5],
                       [3, 6]])
                np.dot(x.T, x)
                Out[37]: 
                array([[17, 22, 27],
                       [22, 29, 36],
                       [27, 36, 45]])
                ### 计算特征值及特征向量       
                np.linalg.eig(np.dot(x.T, x))
                Out[38]: 
                (array([9.04026725e+01, 5.97327474e-01, 7.23299057e-16]),
                 array([[-0.42866713, -0.80596391,  0.40824829],
                        [-0.56630692, -0.11238241, -0.81649658],
                        [-0.7039467 ,  0.58119908,  0.40824829]]))
                math.pow(max(np.linalg.eig(np.dot(x.T, x))[0]), 0.5)
                Out[39]: 9.508032000695724
    
    ∞-范数：，行和范数，即所有矩阵行向量绝对值之和的最大值。
    F-范数：，Frobenius范数，即矩阵元素绝对值的平方和再开平方。

### 向量模长
    向量模长； 即向量元素绝对值的平方和再开方;
    向量的第二范数为传统意义上的向量长度
    向量的模，sum(vector**2)**0.5
    math.sqrt(sum(vec**2 for vec in vector))

### 矩阵乘积
    矩阵乘积; 矩阵相乘最重要的方法是一般矩阵乘积。
    它只有在第一个矩阵的列数（column）和第二个矩阵的行数（row）相同时才有意义  。
    np.dot([[1,2,3], [4, 5,6]], [[1,2], [3,4], [5,6]])
    Out[63]: 
    array([[22, 28],
           [49, 64]])

### 点乘运算
    就是对这两个向量对应位一一相乘之后求和的操作; 即：若a=(x1,y1),b=(x2,y2)，则a·b=x1·x2+y1·y2
    np.dot([1,2,3], [4,5,6])
    Out[4]: 32
    1*4 + 2*5 + 3*6
    Out[5]: 32

### np.dot
    1.对于一维数组，其作用同inner
    np.dot([1,2], [3,4])
    Out[75]: 11
    np.inner([1,2], [3,4])
    Out[76]: 11
    
    2.对于二维数组，其作用是矩阵乘法
    np.dot([[1,2,3], [4,5,6]], [[1,0,0], [1,2,3], [4,5,6]])
    Out[69]: 
    array([[15, 19, 24],
           [33, 40, 51]])
     
    对于多维数组，其作用是将数组a的最后轴上的所有元素与数组b的倒数第二轴上的所有元素的乘积和
    np.dot([[    [0, 2, 0],
                 [1,2,3]],
                [[1,2,3], 
                 [4,5,6]] ], 
            [[[1,0,0], 
              [1,1,1], 
              [0, 0, 0]],
            [[1,0,0], 
             [1,2,3], 
             [4,5,6]]])
    Out[80]: 
    array([[[[ 2,  2,  2],
             [ 2,  4,  6]],
            [[ 3,  2,  2],
             [15, 19, 24]]],
           [[[ 3,  2,  2],
             [15, 19, 24]],
            [[ 9,  5,  5],
             [33, 40, 51]]]])

### 点积
    点积，又叫点乘，向量内积、数量积,dot product; scalar product,标量积
    公式：a * b = |a| * |b| * cosθ = a1*b1 + a2*b2 + a3*b3 + ... + an*bn 
    这里要求一维向量a和向量b的行列数相同。
    点乘又叫向量的内积、数量积，是一个向量和它在另一个向量上的投影的长度的乘积；是标量。 
    点乘反映着两个向量的“相似度”，两个向量越“相似”，它们的点乘越大。
    对两个向量执行点乘运算，就是对这两个向量对应位一一相乘之后求和的操作，点乘的结果是一个标量。
    向量内积,向量a和b的长度之积再乘以它们之间的夹角的余弦；向量内积的几何解释就是一个向量在另一个向量上的投影的积
    内积指的是一个向量(在另一个向量上)的投影乘上另一个向量的模(可以理解为向量的长度)，如果内积为零，意思是互相之间没有投影。
    np.inner([1, 0], [1, 2])
    Out[52]: 1
    math.sqrt(sum(vec**2 for vec in [1, 0])) 
    Out[53]: 1.0
    math.sqrt(sum(vec**2 for vec in [1, 2])) 
    Out[54]: 2.23606797749979
    对于一维数组，np.dot 其作用同np.inner
    
    ### 对于两个二维数组的inner，相当于按X和Y的最后顺序的轴方向上取向量，
    ### 然后依次计算内积后组成的多维数组
    np.inner([[1,2,3], [4,5,6]], [[1,0,0], [1,2,3], [4,5,6]])
    Out[67]: 
    array([[ 1, 14, 32],
           [ 4, 32, 77]])
    
    [[np.dot([1,2,3], [1,0,0]), np.dot([1,2,3],  [1,2,3]), np.dot([1,2,3], [4,5,6])], 
    [np.dot([4,5,6], [1,0,0]), np.dot([4,5,6], [1,2,3]), np.dot([4,5,6], [4,5,6])]]
    Out[71]: [[1, 14, 32], 
              [4, 32, 77]]

### 哈达玛积(Hadamard product)
    矩阵A和矩阵B都是m行n列，将两个矩阵对应位置元素相乘，得到一个m×n矩阵C.
    称矩阵C为矩阵A与B的哈达玛(Hadamard)积，记作 A○B=C。

### 正交基(Orthogonal Basis)
    三个向量两两之间互相的内积等于零，于是这三个向量就是一组简单的正交基。

### 标准正交基
    每组正交基中的向量，其模(可以认为是长度)的大小都是1，这样的情况称为标准正交基。

### 叉乘
    两个向量的叉乘，又叫向量积、外积、叉积，叉乘的运算结果是一个向量而不是一个标量。并且两个向量的叉积与这两个向量组成的坐标平面垂直。
    向量的叉乘，即求同时垂直两个向量的向量，即c垂直于a，同时c垂直于b（a与c的夹角为90°，b与c的夹角为90°）
    c =  a×b = （a.y*b.z-b.y*a.z , b.x*a.z-a.x*b.z  , a.x*b.y-b.x*a.y）
    在二维空间中，叉乘还有另外一个几何意义就是：aXb等于由向量a和向量b构成的平行四边形的面积。
    在三维几何中，向量a和向量b的叉乘结果是一个向量，更为熟知的叫法是法向量，该向量垂直于a和b向量构成的平面。
    两个向量的外积，又叫叉乘、叉积向量积，其运算结果是一个向量而不是一个标量。并且两个向量的外积与这两个向量组成的坐标平面垂直。
    定义：向量a与b的外积a×b是一个向量，其长度等于|a×b| = |a||b|sin∠(a,b)，其方向正交于a与b。并且，(a,b,a×b)构成右手系。 
    特别地，0×a = a×0 = 0.此外，对任意向量a，a×a=0。
    向量的叉乘：a ∧ b
    a ∧ b = |a| * |b| * sinθ 
    向量积被定义为： 
    模长：（在这里θ表示两向量之间的夹角(共起点的前提下)（0° ≤ θ ≤ 180°），它位于这两个矢量所定义的平面上。） 
    方向：a向量与b向量的向量积的方向与这两个向量所在平面垂直，且遵守右手定则。
    若坐标系是满足右手定则的，当右手的四指从a以不超过180度的转角转向b时，竖起的大拇指指向是c的方向。c = a ∧ b
    np.outer只对一维数组进行计算，如果传入的是多维数组，则先将此数组展平为一维数组之后再进行算。
    outer计算列向量和行向量的矩阵乘积，即结果为矩阵
    np.outer([1,2,3], [4,5,6])
    Out[72]: 
    array([[ 4,  5,  6],
           [ 8, 10, 12],
           [12, 15, 18]])
           
    [row * np.array([4,5,6]) for row in [1,2,3]]
    Out[74]: [array([4, 5, 6]), array([ 8, 10, 12]), array([12, 15, 18])]

### 张量积np.tensordot
    tensordot()将两个多维数组a和b指定轴上的对应元素相乘并求和，它是最一般化的乘积运算函数。
    np.tensordot(Z,X,axes=[[0],[1]]) #指定Z的0轴与X的1轴进行乘积求和
    a = np.arange(60.).reshape(3,4,5)
    b = np.arange(24.).reshape(4,3,2)
    c = np.tensordot(a,b, axes=([1,0],[0,1]))
    
    对于多维数组的dot乘积，相当于tensordot(a,b,axes=[[-1],[-2]])
    np.dot(X,Z) == np.tensordot(X,Z,axes=[[-1],[-2]])

### 卷积核

### 互相关（cross-correlation）
    互相关
    设两个函数分别是f(t)和g(t)，则互相关函数定义为：
    它反映的是两个函数在不同的相对位置上互相匹配的程度。
    对卷积不要求倒序操作，也就是互相关；互相关不满足交换律；

### 卷积
    卷积，就是信号B(数据B)与信号A(数据A)错开时间或空间的内积和，错开的时间或空间长度就是卷积结果的自变量。
    或者说，离散域的卷积就是数据A与数据B里面的一段数据逐个相乘，然后再加起来。
    这里的“一段”就是卷积核的长度，当然也可以是无限长。连续域就是把加起来换成积分。
    卷积的方式有三种：
    如果自始至终卷积核都在“信号内”，则最后得到的结果长度会小于待卷积信号长度。假设待卷积信号的长度是n,卷积核大小是m,则结果长度为n-m+1;这种卷积的方式称为 valid;
    如果卷积核的中心刚好从待卷积信号的第一个元素滑到最后一个元素，则需要把原来的信号扩展长度；一般来说扩展的方式是在原来信号的边缘添加0元素，这个过程通常称为零填充(zero padding);
    通过零填充，卷积结果的长度和待卷积信号长度一样，这种卷积的方式称为same;
    如果通过零填充把卷积核能够划过的位置扩展到最大，则结果长度为 n+m-1;这种方式称为full.

### 编码器（encoder）
    编码器（encoder）将输入数据转换为一种不同的表示
    编码器用于将高维的输入压缩成低维的特征，解码器将低维的特征还原为原始信号。

### 降噪自动编码器(Denoising Auto-encoder, dAE)
    当采用无监督的方法分层预训练深度网络的权值时，为了学习到较鲁棒的特征，可以在网络的可视层（即数据的输入层）引入随机噪声，这种方法称为Denoise Autoencoder(简称dAE)。
    Denoising Autoencoder（降噪自动编码器）就是在Autoencoder的基础之上，为了防止过拟合问题而对输入的数据（网络的输入层）加入噪音，使学习得到的编码器W具有较强的鲁棒性，从而增强模型的泛化能力。
    通过与非破损数据（未引入噪音数据）训练的对比，破损数据（加噪数据）训练出来的Weight噪声比较小。降噪因此得名。

### 解码器（decoder）
    解码器函数将新的表示（经编码器处理后的表示）转换到原来的形式

### 自动编码器，自编码(Auto Encoder，AE)
    自回归语言模型只能根据上文预测下一个单词，或者反过来，只能根据下文预测前面一个单词。
    相比而言，Bert通过在输入X中随机Mask掉一部分单词，然后预训练过程的主要任务之一是根据上下文单词来预测这些被Mask掉的单词，
    那些被Mask掉的单词就是在输入侧加入的所谓噪音。类似Bert这种预训练模式，被称为DAE LM。
    这种DAE LM的优缺点正好和自回归LM反过来，它能比较自然地融入双向语言模型，同时看到被预测单词的上文和下文，这是好处。
    缺点：主要在输入侧引入[Mask]标记，导致预训练阶段和微调(Fine-tuning)阶段不一致的问题，因为Fine-tuning阶段是看不到[Mask]标记的。
    DAE就要引入噪音，[Mask] 标记就是引入噪音的手段，这个正常。
    Autoencoding Language Modeling，又叫自编码语言。通过上下文信息来预测当前被mask的token，代表有BERT ，Word2Vec(CBOW)。
    缺点： 由于训练中采用了 [MASK] 标记，导致预训练与微调阶段不一致的问题。此外对于生成式问题， AE 模型也显得捉襟见肘，这也是目前 BERT 为数不多实现大的突破的领域。
    优点： 能够很好的编码上下文语义信息， 在自然语言理解(NLU)相关的下游任务上表现突出。

### 自编码器(AE)
    自编码器（autoencoder）=编码器（encoder）+解码器（decoder）。
    自编码器是一种网络类型,其目的是将输入编码到低维潜在空间,然后再解码回来。
    自编码器学习对原始输入进行重新构建。通过对编码器的输出施加各种限制,我们可以让自编码器学到比较有趣的数据潜在表示。
    自编码器分成两个部分，第一个部分是encoder，一般是多层网络，将输入的数据压缩成为一个向量，变成低维度，而该向量就称之为瓶颈。第二个部分是decoder，灌之以瓶颈，输出数据，我们称之为重建输入数据。
    我们的目的是要让重建数据和原数据一样，以达到压缩还原的作用。
    缺点：低维度的瓶颈显然丢失了很多有用的信息，重建的数据效果并不好。

### 去噪自编码器(DAE)
    降噪自动编码器DAE是在自动编码器的基础上，训练数据加入噪声，来训练整个网络，以和AE相同的方式去训练，得到的网络模型便是DAE。
    在实际的测试数据中，噪声是不可避免的，采用有噪声的训练数据训练网络，神经网络就能够学习到不加噪声的输入特征和噪声的主要特征。能够使网络在测试数据中有更强的泛化能力。

### 添加噪声
    高斯噪声或白噪声（Gaussian noise, or white noise） 的平均值为零，标准差（std）为1，可以根据需要使用伪随机数生成器生成。
    传统上，将高斯噪声添加到神经网络的输入中称为抖动（jitter） 或随机抖动。
    添加的噪声量（例如，扩展或标准偏差）是可配置的超参数。噪声太小没有影响，而噪声太大使映射功能难以学习。通常是通过在将输入数据喂给网络之前在输入数据上添加一个随机向量来完成的，因此，在训练中同样的数据在不同训练批次中，也会添加不同的随机向量。
    随机噪声的标准差可以根据每个输入变量的大小进行调整。如果输入变量的标度已首先进行标准化，则配置起来会更容易。
    噪声仅在训练期间添加。在模型评估期间或使用模型对新数据进行预测时，不会添加任何噪声。
    噪声的添加也是自动特征学习的重要部分，例如在自动编码器的情况下，所谓的降噪自动编码（Denoising Autoencoders）器明确要求模型在存在输入噪声的情况下学习鲁棒特征。
    如何添加噪声：
    1、在数据中添加噪声是最常见且最广泛的方法；
    2、给激活添加噪声，即每层的输出。
    3、给权重添加噪声，即迭代输入。
    4、给梯度（即更新权重的方向）添加噪声。
    5、给输出（即标签或目标变量）添加噪声。
    在激活中中增加噪声，从而可以在网络的任何数据使用噪声。这对于非常深的网络可能是有益的。将噪声添加到层输出中，很可能是通过使用噪声激活函数来实现的。
    在权重中添加噪声，可以使该方法以一致的方式在整个网络中使用，而不是在输入和层激活中添加噪声。这在递归神经网络中特别有用。
    向梯度添加噪声，可以使模型更多地集中在提高优化过程本身的鲁棒性上，而不是输入空间的结构上。噪声的数量可能在训练开始时就开始很高，并且随着时间的流逝而降低，这很像学习速率的下降。
    将噪声添加到激活，权重或梯度都提供了一种更通用的添加噪声的方法，该噪声对于提供给模型的输入变量的类型是不变的。
    如果认为或预期问题域带有错误标记的示例，则在类别标签上添加噪声可以提高模型对此类错误的鲁棒性。但这很容易偏离学习过程。
    在回归或时间序列预测中，将噪声添加到连续目标变量中，非常类似于将噪声添加到输入变量中，并且可能是更好的方法。

### 变分自编码器(VAE,variational autoencoder)
    VAE和AE，DAE不同的是，原先编码器是映射成一个向量，现在是映射成两个向量，一个向量表示分布的平均值，另外一个表示分布的标准差，两个向量都是相同的正态分布。现在从两个向量分别采样，采样的数据灌给解码器。
    VAE 不是将输入压缩成潜在空间中的固定编码,而是将转换为统计分布的参数,即平均值和方差。
    本质上来说,这意味着我们假设输入是由统计过程生成的,在编码和解码过程中应该考虑这一过程的随机性。
    然后,VAE 使用平均值和方差这两个参数来从分布中随机采样一个元素,并将这个元素解码到原始输入。
    这个过程的随机性提高了其稳健性,并迫使潜在空间的任何位置都对应有意义的表示,即潜在空间采样的每个点都能解码为有效的输出。
    变分自动编码器的训练过程与自动编码器的训练过程非常类似，无非是损失函数不一样罢了。这里的损失函数也非常直观，就是在自动编码器中的损失上增加了 KL 散度正则项。
    数据降维的目的不在于单纯的降维，而在于在降维的同时还要保证包含原始信号的主要信息能通过最简单的结构给表达出来。
    一个变分自动编码器是一种特殊的自动编码器，它的训练过程加入了正则项，避免训练过程的过拟合并且能确保其隐空间具有良好的能生成新样本的性质。变分自动编码器对数据在隐空间中以某种特定分布进行编码。
    一般来说 VAE （Variational AutoEncoder） 可以进行异常检测、去噪和生成合成数据。
    异常检测：异常检测可以关于识别显着偏离大多数数据和不符合明确定义的正常行为概念的样本。
    去噪：去噪是从信号中去除噪声的过程。 我们可以应用 VAE 对大多数偏离的特征进行降噪。 去噪转换噪声特征，一般情况下我们会将异常检测出的样本标记为噪声样本。
    生成合成数据：使用 VAE，我们可以从正态分布中采样并将其传递给解码器以获得新的样本。

### 去耦变分自编码器(β-VAE)
    β-VAE是VAE的变体，增强了VAE模型表示解耦(Disentanglement，解纠缠)的能力。仅需在loss function中加上一个β即可达到目的。
    loss函数中，β就是一个超参数，当β为1的时候，它就是标准的VAE。
    一个较高的β值，就使得前变量空间z表示信息的丰富度降低，但同时模型的解纠缠能力增加。所以β可以作为表示能力和解纠缠能力之间的平衡因子。

### 扩散模型( Diffusion Models )
    扩散是指将一个结构化的信号（一张图像）逐步转化为噪声的过程。通过模拟扩散，我们可以从我们的训练图像中生成带有噪声的图像，并训练一个神经网络尝试去噪。
    使用训练好的网络，我们可以模拟扩散的逆过程，即反向扩散，从随机噪声直接生成图片的过程。
    扩散模型（diffusion model）是一类生成模型，运用了物理热力学扩散思想，主要用于对复杂数据分布进行建模和采样。
    给定目标分布q(x)中的一些观测数据x，生成模型的目标是学习一个生成过程，从q(x)产生新样本。通过注入高斯噪声逐步扰动观测数据，然后应用一个可学习的转换核心进行逆过程以恢复数据。
    定义了扩散步骤的马尔可夫链，以缓慢地将随机噪声添加到数据中，然后学习逆向扩散过程以从噪声中构造所需的数据样本。
    扩散模型的基本思想是正向扩散过程来系统地扰动数据中的分布，然后通过学习反向扩散过程恢复数据的分布，这样就了产生一个高度灵活且易于计算的生成模型。
    
### DDPM(去噪扩散概率模型, Denoising Diffusion Probabilistic Models)
    扩散模型是一类生成式模型，从随机噪声直接生成图片。
    DDPMs 是一种最具代表性的扩散模型，通过逐步去噪的方法实现数据生成。DDPM把图像通过n步压缩成噪声，噪声再通过n步去噪成图像。
    DDPM通过模拟一个逐步加入噪声的过程，然后学习一个去噪过程来逆转这个噪声过程，最终生成新的数据点。
    这个过程可以被分解为两个主要部分：扩散过程（前向的加噪声过程）和反向去噪过程（逆向过程）。
    扩散(加噪声)：针对输入的图片，之后的每一步在前一步的基础上加上高斯噪声，最终变成了几乎纯粹的高斯噪声。
    去噪过程: 去噪过程与加噪声过程相反，从标准正太分布中采样得到一个噪声样本，再一步步地去噪，最后得到数据分布中的一个样本。
    这两个过程的中间态是一个和输入图像相同尺寸的高斯噪声。
    加噪声的过程类似于物理世界中的粒子随机扩散过程。

### DDIM （Denoising Diffusion Implicit Models）
    DDPM 有一个非常明显的问题：采样过程很慢。因为 DDPM 的反向过程利用了马尔可夫假设，所以每次都必须在相邻的时间步之间进行去噪，而不能跳过中间步骤。
    原始论文使用了 1000 个时间步，所以我们在采样时也需要循环 1000 次去噪过程，这个过程是非常慢的。
    为了加速 DDPM 的采样过程，DDIM 在不利用马尔可夫假设的情况下推导出了 diffusion 的反向过程，最终可以实现仅采样 20～100 步的情况下达到和 DDPM 采样 1000 步相近的生成效果，也就是提速 10～50 倍。  
    DDIM和DDPM有相同的训练目标，但是它不再限制扩散过程必须是一个马尔卡夫链，这使得DDIM可以采用更小的采样步数来加速生成过程，DDIM的另外是一个特点是从一个随机噪音生成样本的过程是一个确定的过程（中间没有加入随机噪音）。

### Improved DDPM（Improved Denoising Diffusion Probabilistic Models）
    Improved DDPM 主要是针对 DDPM 的训练过程进行改进，主要从两个方面进行改进：
    1、不使用 DDPM 原有的固定方差，而是使用可学习的方差；
    2、改进了加噪过程，使用余弦形式的 Scheduler，而不是线性 Scheduler。

### Unet model
    U-Net网络结构它是一种用于语义分割的深度神经网络模型。
    U-Net的基本结构是一个U形网络，其中包含编码器（downsampling path）和解码器（upsampling path）两部分，具有对称结构。
    编码器用于提取输入图像的特征信息，通过多个卷积层和池化层将输入图像逐步缩小，得到一个语义表达。
    解码器则将此语义表达映射回原始图像大小，通过多个反卷积和上采样层恢复分辨率，并将特征与对应的编码器层的特征进行融合，从而还原输出目标的位置、形状和语义信息。
    UNet从本质上来说也属于一种全卷积神经网络模型，它的取名来源于其架构形状：模型整体呈现U形。
    UNet网络结构，最主要的两个特点是：U型网络结构和Skip Connection跳层连接。
    Unet通过跳接的U形网络结构结合了浅层特征与深层特征，用于最后的语义分割图生成。
    与FCN不同的是，UNet以拼接方式来结合浅层特征与深层特征；
    而FCN则是以相加方式来结合浅层特征与深层特征；
    U形网络架构能够更充分地融合浅层特征和深层特征，这也是UNet性能优于FCN的主要原因。
    浅层特征图更倾向于表达例如点、线、边缘轮廓等基本特征单元；蕴含的空间信息更多。 
    深层特征图更倾向于表达图像的语义信息；蕴含的空间信息更少，语义特征更多。
    UNet的主干分为对称的左右两部分：
    左边为特征提取网络（编码器，encoder），原始输入图像通过卷积-最大池化进行四次下采样，获得四层级的特征图；
    右边为特征融合网络（解码器，decoder），各层级特征图与经过反卷积获得的特征图通过跳接方式进行特征融合；
    最后一层通过与标签计算损失进行语义图预测。

### 自回归(Auto Regressive，AR)
    根据上文内容预测下一个可能跟随的单词，就是常说的自左向右的语言模型任务，或者反过来也行，就是根据下文预测前面的单词，这种类型的LM被称为自回归语言模型。
    （GPT,ELMO）GPT 就是典型的自回归语言模型。
    ELMO尽管看上去利用了上文，也利用了下文，但是本质上仍然是自回归LM，这个跟模型具体怎么实现有关系。
    ELMO是做了两个方向（从左到右以及从右到左两个方向的语言模型），但是是分别有两个方向的自回归LM，然后把LSTM的两个方向的隐节点状态拼接到一起，来体现双向语言模型这个事情的。
    所以其实是两个自回归语言模型的拼接，本质上仍然是自回归语言模型。
    自回归语言模型有优点有缺点，缺点是只能利用上文或者下文的信息，不能同时利用上文和下文的信息，
    当然，貌似ELMO这种双向都做，然后拼接看上去能够解决这个问题，因为融合模式过于简单，所以效果其实并不是太好。
    它的优点，其实跟下游NLP任务有关，比如生成类NLP任务，比如文本摘要，机器翻译等，在实际生成内容的时候，就是从左向右的，自回归语言模型天然匹配这个过程。
    而Bert这种DAE模式，在生成类NLP任务中，就面临训练过程和应用过程不一致的问题，导致生成类的NLP任务到目前为止都做不太好。
    Transformer模型属于自回归模型，也就是说后面的token的推断是基于前面的token的。Decoder端的Mask的功能是为了保证训练阶段和推理阶段的一致性。
    在推理阶段，token是按照从左往右的顺序推理的。也就是说，在推理timestep=T的token时，decoder只能“看到”timestep < T的 T-1 个Token, 
    不能和timestep大于它自身的token做attention（因为根本还不知道后面的token是什么）。
    为了保证训练时和推理时的一致性，所以，训练时要同样防止token与它之后的token去做attention。
    Aotoregressive Lanuage Modeling，又叫自回归语言模型。它指的是，依据前面(或后面)出现的tokens来预测当前时刻的token，代表模型有ELMO、GTP等。
    缺点：它只能利用单向语义而不能同时利用上下文信息。ELMO 通过双向都做AR 模型，然后进行拼接，但从结果来看，效果并不是太好。
    优点： 对自然语言生成任务(NLG)友好，天然符合生成式任务的生成过程。这也是为什么 GPT 能够编故事的原因。

### 编码器-解码器模型
    现有的预训练框架可以分为三类：包括自编码模型（例如BERT）、自回归模型（例如GPT）和编码器-解码器模型（例如T5）。
    自回归模型，只有decoder，如GPT，学习从左到右的语言模型。虽然它们在长文本生成方面取得了成功，并且在扩展到数十亿个参数时显示出few-shot能力，但其固有的缺点是单向注意力机制，无法完全捕捉NLU任务中上下文词之间的依赖性。
    自编码模型，只有encoder，如BERT，通过去噪目标学习双向上下文编码器，如Masked Language Model（MLM）。编码器产生适合自然语言理解任务的上下文化表示，但不能直接应用于文本生成。
    编码器-解码器模型，同时有encoder-decoder，对编码器采用双向注意力，对解码器采用单向注意力，并在它们之间进行交叉注意力。它们通常部署在条件生成任务中，例如文本摘要和响应生成。

### dense prediction
    英语直译是密集预测，dense prediction 实际上是给图像上的每个像素预测一个类别（预测每个像素的label），由于一般图片的像素都很多，直观上看很密集，所以这种任务叫做dense prediction。

### patch
    patch在图像分类中，可以通俗地理解为图像块,当需要处理的图像分辨率太大而资源受限(比如显存、算力等)时,就可以将图像划分成一个个小块,这些小的图像块就是patch。
    将图像分解为较小的切片（patch）。
    为何要划分patch而不使用resize缩小分辨率呢？通常情况下，resize没有太大问题。
    但在处理图像分割问题时，由于是dense prediction，属于像素级的预测，因此会尽量要求精确。
    而resize操作大多是对图像进行插值处理，本质上一种滤波，在像素级别上会造成损失(对传统图像处理有了解的应该知道某些滤波效果会使图像变得模糊)，
    即：某些位置上的像素值是通过多个位置加权计算出来的，从而限制了模型预测结果的上限。
    因为你给的源图像本来就是不精确的，基于这不精确的源信号作为监督，训练出来的模型性能自然就局限在那里了。
    相对地，划分patch只是把原来的大图分成一个个小图，而这些小图依然是原图的部分，像素值没有改动，因而在理论上，训练出来模型的上限能够比基于resize得到的图像训练来的高。

### UniLM(统一语言模型，Uniﬁed Language Model )
    UniLM也是一个多层Transformer网络，跟bert类似，但是UniLM能够同时完成三种预训练目标，
    使用三种特殊的Mask的预训练目标，从而使得模型可以用于NLG，同时在NLU任务获得和BERT一样的效果。 模型使用了三种语言模型的任务：
    unidirectional prediction、bidirectional prediction、seuqnece-to-sequence prediction
    1、Unidirectional LM
    单向训练语言模型，mask词的语境就是其单侧的words，左边或者右边。
    x1 x2 [MASK] x4 对于MASK的预测，只能使用token1和token2以及自己位置能够被使用，使用的就是一个对角矩阵的。同理从右到左的LM也类似。
    2、Bidirectional LM
    双向训练语言模型，mask词的语境就是左右两侧的words。
    对于双向的LM，只对padding进行mask。
    3、Seq2Seq LM
    Seq-to-Seq语言模型，左边的seq我们称source sequence，右边的seq我们称为target sequence，我们要预测的就是target sequence，所以其语境就是所有的source sequence和其左侧已经预测出来的target sequence。
    在训练的时候，一个序列由[SOS]S_1[EOS]S_2[EOS]组成，其中S1是source segments，S2是target segments。
    随机mask两个segment其中的词，其中如果masked是source segment的词的话，则它可以attend to 所有的source segment的tokens，
    如果masked的是target segment，则模型只能attend to 所有的source tokens 以及target segment 中当前词（包含）和该词左边的所有tokens。
    这样的话，模型可以隐形地学习到一个双向的encoder和单向decoder。
    总的loss是三种LM的loss之和。
    我们在一个训练的batch中，1/3的时间训练bidirection LM，1/3的时间训练sequence-to-sequence LM objective， 1/6的时间训练left-to-right 和 1/6的时间训练 right-to-left LM。
    混合训练方式：对于一个batch，1/3时间采用双向(bidirectional)语言模型的目标，1/3的时间采用seq-to-seq语言模型目标，最后1/3平均分配给两种单向学习的语言模型，也就是left-to-right和right-to-left方式各占1/6时间。
    masking 方式：总体比例15%，其中80%的情况下直接用[MASK]替代，10%的情况下随机选择一个词替代，最后10%的情况用真实值。
    还有就是80%的情况是每次只mask一个词，另外20%的情况是mask掉bigram(二元分词)或者trigram(三元分词)。
    Attention 控制：不同的训练方式，其关注的语境是不一样的，
    不让当前预测词看掉的信息就采用掩码隐藏掉，只留下能让当前词可看的信息，换句话说，使用了掩码来控制在计算基于上下文的表征时 token 应该关注的上下文的量。

### 语言模型（Language Model）
    语言模型简单来说就是一串词序列的概率分布。

### Prefix LM（前缀语言模型）
    Prefix LM：前缀语言模型是一种生成模型，它在生成每个词时都可以考虑之前的上下文信息。
    在生成时，前缀语言模型会根据给定的前缀（即部分文本序列）预测下一个可能的词。这种模型可以用于文本生成、机器翻译等任务。

### Causal LM（因果语言模型）
    Causal LM：因果语言模型是一种自回归模型，它只能根据之前的文本生成后续的文本，而不能根据后续的文本生成之前的文本。
    在训练时，因果语言模型的目标是预测下一个词的概率，给定之前的所有词作为上下文。这种模型可以用于文本生成、语言建模等任务。

### 神经网络语言模型（Neural Language Model，简称NLM）
    神经网络语言模型是一种基于神经网络的语言模型，它的主要优点是可以学习到词的分布式表示（Distributed Representation），从而克服了N-gram模型的稀疏性问题。
    常见的神经网络语言模型有循环神经网络（RNN）、长短时记忆网络（LSTM）和门控循环单元（GRU）等。

### 预训练语言模型（PLM，Pre-trained Language Model）
    预训练语言模型(PLMs)是在大规模语料库上以自监督方式进行预训练的语言模型。
    PLM的工作原理可以分为两个阶段：预训练和微调。
    在预训练阶段，PLM利用大量的无监督数据（如互联网上的文本）进行训练，以学习语言的通用特征。这些特征可以表现为词汇、语法、语义等方面的知识。
    在微调阶段，PLM针对具体的NLP任务，利用少量的有监督数据进行训练，以调整模型的参数，使其更好地适应特定任务。
    PLM的优点在于其具备广泛的语言理解能力。由于预训练阶段所使用的数据量庞大，PLM得以学习到丰富的语言知识，这使得它在处理各种NLP任务时表现出色。

### 大型语言模型（LLM,Large Language Model）
    近些年，研究人员通过在大规模语料库上预训练 Transformer 模型产生了预训练语言模型（PLMs），并在解决各类 NLP 任务上展现出了强大的能力。
    并且研究人员发现模型缩放可以带来性能提升，因此他们通过将模型规模增大进一步研究缩放的效果。
    有趣的是，当参数规模超过一定水平时，这个更大的语言模型实现了显著的性能提升，并出现了小模型中不存在的能力，比如上下文学习。
    为了区别于 PLM，这类模型被称为大型语言模型（LLMs）。
    通常，大型语言模型（LLM）是指包含数千亿（或更多）参数的语言模型，这些参数是在大量文本数据上训练的，例如模型 GPT-3、PaLM、Galactica 和 LLaMA。
    作为主要区别，LLM 在很大程度上扩展了模型大小、预训练数据和总计算量（扩大倍数）。他们可以更好地理解自然语言，并根据给定的上下文（例如 prompt）生成高质量的文本。
    这种容量改进可以用标度律进行部分地描述，其中性能大致遵循模型大小的大幅增加而增加。
    然而根据标度律(标度律是指在某些物理系统中,当系统的大小发生变化时,某些物理量的变化规律具有一定的统一性)，某些能力（例如，上下文学习）是不可预测的，只有当模型大小超过某个水平时才能观察到。
    LLM 的涌现能力被正式定义为「在小型模型中不存在但在大型模型中出现的能力」，这是 LLM 与以前的 PLM 区分开来的最显著特征之一。
    当出现这种新的能力时，它还引入了一个显著的特征：当规模达到一定水平时，性能显著高于随机的状态。
    这里简要介绍了 LLM 的三种代表性的涌现能力：
    1、上下文学习。GPT-3 正式引入了上下文学习能力：假设语言模型已经提供了自然语言指令和多个任务描述，它可以通过完成输入文本的词序列来生成测试实例的预期输出，而无需额外的训练或梯度更新。 
    2、指令遵循。通过对自然语言描述（即指令）格式化的多任务数据集的混合进行微调，LLM 在微小的任务上表现良好，这些任务也以指令的形式所描述。这种能力下，指令调优使 LLM 能够在不使用显式样本的情况下通过理解任务指令来执行新任务，这可以大大提高泛化能力。
    3、循序渐进的推理。对于小语言模型，通常很难解决涉及多个推理步骤的复杂任务，例如数学学科单词问题。同时，通过思维链推理策略，LLM 可以通过利用涉及中间推理步骤的 prompt 机制来解决此类任务得出最终答案。据推测，这种能力可能是通过代码训练获得的。

### LMMs（Large Multimodal Models，多模态大模型）
    基于深度学习的大规模机器学习模型，它们能够处理和理解多种不同类型的数据输入，例如文本、图像、音频和视频。
    模型通过大规模的数据训练，学习如何联合理解和生成跨多种模式的信息。
    设计用来理解和处理多种类型的数据输入，包括文本、图像、音频、视频，有时还包括感觉数据等。它们的关键能力是整合并理解这些不同的数据格式。
    除了文本数据，训练模型还需要图像、音频、视频等数据。数据收集更为复杂，因为它涉及内容的多样性以及不同的格式和模态。

### 模态
    模态（modal）是事情经历和发生的方式，我们生活在一个由多种模态(Multimodal)信息构成的世界，包括视觉信息、听觉信息、文本信息、嗅觉信息等

### 多模态大语言模型(Multimodal Large Language Model , MLLM）
    多模态大语言模型（Multimodal Large Language Model, MLLM）主要是指那些能够处理和整合多种模态信息（比如文本、图像和音频）的大语言模型。
    多模态大语言模型（MLLMs）可以无缝地集成视觉和文本模态，既作为输入又作为输出，同时提供基于对话的界面和指令遵循的能力。
    多模态大语言模型的训练过程主要包括两个阶段：视觉-语言对齐预训练和视觉指令微调。
    视觉-语言对齐预训练
    为了训练多模态大语言模型，一般重用已有的视觉编码器和大语言模型。
    由于视觉模型和语言模型之间存在较大的语义空间差异，因此视觉-语言对齐预训练旨在利用大规模“图像-文本对”（简称图文对）进行端到端训练，进而对齐两种不同的语义空间。
    视觉指令微调
    视觉指令微调，旨在提高多模态大语言模型遵循指令和解决任务的能力。一般来说，视觉指令微调的输入包括一张图像和一段任务描述文本，输出是对应的文本回复。
    为了构造高质量的视觉指令数据，可以将图像自带的描述文本输入给大语言模型，通过特定的提示（如“根据图像描述生成一段图像相关的对话”）来引导大语言模型自动化地合成视觉指令；
    或者基于已有的视觉-语言任务数据集，利用特定的问题模板将原有任务数据转化为视觉指令。

### 生成式人工智能（Artificial Intelligence Generated Content, AIGC）
    AIGC(AI Generated Content)即人工智能生成内容,又称“生成式AI”(Generative AI),被认为是继专业生产内容(PGC)、用户生产内容(UGC)之后的新型内容创作方式。
    利用人工智能技术来生成内容的一种新型技术,通俗讲是用人工智能进行产品的内容创作。

### 智能问答 (Intelligent Question Answering, IQA) 
    智能问答是自然语言处理（NLP）中的一个核心子领域，旨在设计和开发可以解析、理解并回答用户提出的自然语言问题的系统。 
    这些系统的目标不仅仅是返回与问题相关的文本，而是提供精确、凝练且直接的答案。

### 机器阅读理解(Machine Reading Comprehension, MRC)
    机器阅读理解(Machine Reading Comprehension, MRC)作为自然语言处理领域中的一个基本任务,要求模型就给定的一段文本和与文本相关的问题进行作答。
    机器阅读理解(Machine Reading Comprehension,MRC)就是给定一篇文章,以及基于文章的一个问题, 让机器在阅读文章后对问题进行作答。
    机器阅读理解(MRC)任务通过问答的形式来衡量模型是否理解了自然语言文本，然而深度学习模型终究只是统计模型，当前的MRC模型本质上仅仅是通过复杂的函数来拟合文本中的统计线索，从而预测答案而已。
    比如三短一长选最长，参差不齐就选C，以及数学考试中常见的排除法、特值法、估算法等。但我们在学习知识的过程中并不会采用这些技巧，因为这些技巧并不是真正的知识。

### Mixture-of-Experts(MoE)模型,混合专家模型
    混合专家（Mixture of Experts，简称MoE）是一种集成学习方法，它通过将多个专业化的子模型（即“专家”）组合起来，形成一个整体模型，每一个“专家”都在其擅长的领域内做出贡献。
    而决定哪个“专家”参与解答特定问题的，是一个称为“门控网络”的机制。每个专家模型可以专注于解决特定的子问题，而整体模型则能够在复杂的任务中获得更好的性能。
    混合专家模型（MoE）是一种稀疏门控制的深度学习模型，由两个关键组成部分构成：门控网络（GateNet）和专家网络（Experts）。
    门控网络：负责根据输入数据的特征，动态地决定哪个专家模型应该被激活以生成最佳预测。
    专家网络：是一组独立的模型，每个模型都负责处理某个特定的子任务。
    
### 大模型的“幻觉”（Hallucination）
    有人将 LLM 的幻觉定义为“生成的内容与提供的源内容不符或没有意义”。
    幻觉可以分为几种类型：
    逻辑谬误：模型在进行推理时出现了错误，提供错误的答案。
    捏造事实：模型自信地断言不存在的事实，而不是回答“我不知道”。
    数据驱动的偏见：由于某些数据的普遍存在，模型的输出可能会偏向某些方向，导致错误的结果。例如：自然语言处理模型中发现的政治偏见。
    为何会产生幻觉？
    因为，模型往往通过采样的方式，决定token的产出结果，而不是固定取softmax算出的最大概率token。
    当我们压缩训练数据时，模型难免会产生一定的幻觉。
    压缩的关键在于，生成模型存储的是输入（文本或图像像素点）之间关系（概率）的数学表征（mathematical representation），而不是输入的内容本身。
    更重要的是，这种表征能够让我们通过抽样或提交queries/prompts来提取知识。
    这种压缩方法降低了保真度（fidelity）。模型倾向于不完美地“填补空白”或产生幻觉，这是为了获得那些虽然被压缩但是仍然有用的知识表征而做出的权衡。 
    这种权衡的结果是，模型会在缺少信息的情况下进行猜测，从而产生不准确的输出。
    当LLMs的训练数据集中关于所提出问题的信息受限、过时或具有矛盾时，它们也会产生幻觉。 
    模型没有充分的信息支持准确回答问题，所以输出也就不可靠。

### LLMs 复读机问题
    LLMs复读机问题指的是大型语言模型（LLMs）在生成文本时出现的一种现象，即模型倾向于无限地复制输入的文本或者以过度频繁的方式重复相同的句子或短语。
    这种现象使得模型的输出缺乏多样性和创造性，给用户带来了不好的体验。
    复读机问题可能出现的原因包括：
    1、数据偏差：大型语言模型通常是通过预训练阶段使用大规模无标签数据进行训练的。如果训练数据中存在大量的重复文本或者某些特定的句子或短语出现频率较高，模型在生成文本时可能会倾向于复制这些常见的模式。  
    2、训练目标的限制：大型语言模型的训练通常是基于自监督学习的方法，通过预测下一个词或掩盖词来学习语言模型。 
    这样的训练目标可能使得模型更倾向于生成与输入相似的文本，导致复读机问题的出现。  
    3、缺乏多样性的训练数据：虽然大型语言模型可以处理大规模的数据，但如果训练数据中缺乏多样性的语言表达和语境，模型可能无法学习到足够的多样性和创造性，导致复读机问题的出现。  
    4、模型结构和参数设置：大型语言模型的结构和参数设置也可能对复读机问题产生影响。例如，模型的注意力机制和生成策略可能导致模型更倾向于复制输入的文本。
    为了缓解LLMs复读机问题，可以尝试以下方法： 
    1、多样性训练数据：在训练阶段，使用多样性的语料库来训练模型，避免数据偏差和重复文本的问题。这可以包括从不同领域、不同来源和不同风格的文本中获取数据。  
    2、引入噪声：在生成文本时，引入一些随机性或噪声，例如通过采样不同的词或短语，或者引入随机的变换操作，以增加生成文本的多样性。这可以通过在生成过程中对模型的输出进行采样或添加随机性来实现。  
    3、温度参数调整：温度参数是用来控制生成文本的多样性的一个参数。通过调整温度参数的值，可以控制生成文本的独创性和多样性。较高的温度值会增加随机性，从而减少复读机问题的出现。  
    4、Beam搜索调整：在生成文本时，可以调整Beam搜索算法的参数。Beam搜索是一种常用的生成策略，它在生成过程中维护了一个候选序列的集合。通过调整Beam大小和搜索宽度，可以控制生成文本的多样性和创造性。  
    5、后处理和过滤：对生成的文本进行后处理和过滤，去除重复的句子或短语，以提高生成文本的质量和多样性。可以使用文本相似度计算方法或规则来检测和去除重复的文本。  
    6、人工干预和控制：对于关键任务或敏感场景，可以引入人工干预和控制机制，对生成的文本进行审查和筛选，确保生成结果的准确性和多样性。

### LoRA(LOW-RANK ADAPTATION, 低秩适配)
    在模型(一般是大型语言模型)的Linear层，的旁边，增加一个“旁支”，这个“旁支”的作用，就是代替原有的参数矩阵W进行训练。
    输入x，输出具有维度d。这个输出可能是embedding的输出，也有可能是上一层transformer layer的输出，而d一般就是768或者1024。
    而在LORA的策略下，增加了“旁支”，也就是先用一个Linear层A，将数据从d 维降到r，这个r也就是LORA的秩，是LORA中最重要的一个超参数。
    一般会远远小于d，尤其是对于现在的大模型，d 已经不止是768或者1024，例如LLaMA-7B，每一层transformer有32个head，这样一来d 就达到了4096.
    接着再用第二个Linear层B，将数据从r变回d维。最后再将左（原有的模型部分）右（lora旁支）两部分的结果相加融合，就得到了输出的hidden_state。
    在Albert中，作者考虑到词表的维度很大，所以将Embedding矩阵分解成两个相对较小的矩阵，用来模拟Embedding矩阵的效果，这样一来需要训练的参数量就减少了很多。
    LORA也是类似的思想，并且它不再局限于Embedding层，而是所有出现大矩阵的地方，理论上都可以用到这样的分解。
    但是与Albert不同的是，Albert直接用两个小矩阵替换了原来的大矩阵，而LORA保留了原来的矩阵W，但是不让W参与训练，所以需要计算梯度的部分就只剩下旁支的A和B两个小矩阵。

### Parameter-Efficient Fine-Tuning (PEFT)
    PEFT 方法仅微调少量 (额外) 模型参数，同时冻结预训练 LLM 的大部分参数，从而大大降低了计算和存储成本。
    这也克服了灾难性遗忘的问题，这是在 LLM 的全参数微调期间观察到的一种现象。PEFT 方法也显示出在低数据状态下比微调更好，可以更好地泛化到域外场景。
    PEFT 方法还有助于提高轻便性，其中用户可以使用 PEFT 方法调整模型，以获得与完全微调的大型检查点相比，大小仅几 MB 的微小检查点。例如， bigscience/mt0-xxl 占用 40GB 的存储空间，全参数微调将导致每个下游数据集有对应 40GB 检查点。
    而使用 PEFT 方法，每个下游数据集只占用几 MB 的存储空间，同时实现与全参数微调相当的性能。来自 PEFT 方法的少量训练权重被添加到预训练 LLM 顶层。因此，同一个 LLM 可以通过添加小的权重来用于多个任务，而无需替换整个模型。
    以LORA为例，PEFT模型的使用非常方便，只需要按照原本的方式实例化模型，然后设置一下LORA的config，调用一下get_peft_model方法，就获得了在原模型基础上的PEFT模型，对于LORA策略来讲，就是在某些参数矩阵W的基础上增加了矩阵分解的旁支。
    # 创建基础transformer模型
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)
    # 加入PEFT策略
    model = PeftModel.from_pretrained(model, peft_model_name_or_path)

### 灾难性遗忘（Catastrophic Forgetting）
    灾难性遗忘（Catastrophic Forgetting）是指在模型微调过程中，当模型在新任务上进行训练时，可能会忘记之前学习到的知识，导致在旧任务上的性能下降。
    这种现象常见于神经网络模型的迁移学习或连续学习场景中。
    在微调大语言模型时，灾难性遗忘可能出现的原因包括：
    1、数据分布差异：微调过程中使用的新任务数据与预训练数据或旧任务数据的分布存在差异。如果新任务的数据分布与预训练数据差异较大，模型可能会过度调整以适应新任务，导致旧任务上的性能下降。
    2、参数更新冲突：微调过程中，对新任务进行训练时，模型参数可能会被更新，导致之前学习到的知识被覆盖或丢失。新任务的梯度更新可能会与旧任务的梯度更新发生冲突，导致旧任务的知识被遗忘。
    为了解决灾难性遗忘问题，可以尝试以下方法：
    1、经验回放（Replay Buffer）：在微调过程中，使用一个缓冲区来存储旧任务的样本，然后将旧任务的样本与新任务的样本一起用于训练。这样可以保留旧任务的知识，减少灾难性遗忘的发生。
    2、弹性权重共享（Elastic Weight Consolidation）：通过引入正则化项，限制模型参数的变动范围，以保护之前学习到的知识。这种方法可以在微调过程中平衡新任务和旧任务之间的重要性。
    3、增量学习（Incremental Learning）：将微调过程分为多个阶段，每个阶段只微调一小部分参数。这样可以逐步引入新任务，减少参数更新的冲突，降低灾难性遗忘的风险。
    4、多任务学习（Multi-Task Learning）：在微调过程中，同时训练多个相关任务，以提高模型的泛化能力和抗遗忘能力。通过共享模型参数，可以在不同任务之间传递知识，减少灾难性遗忘的影响。
    综上所述，灾难性遗忘是在模型微调过程中可能出现的问题。通过合适的方法和技术，可以减少灾难性遗忘的发生，保留之前学习到的知识，提高模型的整体性能。

### checkpoint
    tensorflow中的检查点checkpoint，检查点checkpoint中存储着模型model所使用的的所有的 tf.Variable 对象，它不包含任何关于模型的计算信息，因此只有在源代码可用，
    也就是我们可以恢复原模型结构的时候，checkpoint才有用，否则不知道模型的结构，仅仅只知道一些Variable是没有意义的。
    Checkpoints文件是一个二进制文件，它把变量名映射到对应的tensor值 。本质上是存储的各个变量的值，并没有网络结构信息。
    Checkpoint是用于描述在每次训练后保存模型参数（权重）的惯例或术语。
    checkpoint的4个文件简介，checkpoint的一般格式如下：
    （1）meta文件
    .meta文件：一个协议缓冲，保存tensorflow中完整的graph、variables、operation、collection；这是我们恢复模型结构的参照；
    meta文件保存的是图结构，通俗地讲就是神经网络的网络结构。当然在使用低层PAI编写神经网络的时候，本质上是一系列运算以及张量构造的一个较为复杂的graph，这个和高层API中的层的概念还是有区别的，但是可以这么去理解，整个graph的结构就是网络结构。一般而言网络结构是不会发生改变，所以可以只保存一个就行了。我们可以使用下面的代码只在第一次保存meta文件。
    saver.save(sess, 'my_model.ckpt', global_step=step, write_meta_graph=False)
    在后面恢复整个graph的结构的时候，并且还可以使用
    tf.train.import_meta_graph(‘xxxxxx.meta’)
    能够导入图结构。
    （2）data文件
    keypoint_model.ckpt-9.data-00000-of-00001：数据文件，保存的是网络的权值，偏置，操作等等。
    （3）index文件
    keypoint_model.ckpt-9.index  是一个不可变得字符串字典，每一个键都是张量的名称，它的值是一个序列化的BundleEntryProto。 每个BundleEntryProto描述张量的元数据，所谓的元数据就是描述这个Variable 的一些信息的数据。 “数据”文件中的哪个文件包含张量的内容，该文件的偏移量，校验和，一些辅助数据等等。
    Note: 以前的版本中tensorflow的model只保存一个文件中。
    （4）checkpoint文件——文本文件
    checkpoint是一个文本文件，记录了训练过程中在所有中间节点上保存的模型的名称，首行记录的是最后（最近）一次保存的模型名称。
    checkpoint是检查点文件，文件保存了一个目录下所有的模型文件列表。

### 困惑度 (perplexity，PPL)
    在自然语言处理中，对于一个语言模型，一般用困惑度来衡量它的好坏，困惑度越低，说明语言模型面对一句话感到困惑的程度越低，语言模型就越好。PPL越小越好.
    Perplexity可以认为是average branch factor（平均分支系数），即预测下一个词时可以有多少种选择。
    说模型的PPL下降到90，可以直观地理解为，在模型生成一句话时下一个词有90个合理选择，可选词数越少，我们大致认为模型越准确。这样也能解释，为什么PPL越小，模型越好。

### 错误L2范数(ErrorL2-Norm)
    ErrorL2-Norm（EL2N）得分用于确定哪些样本对学习是重要的，它利用模型的早期学习信号来衡量每个样本的重要性。
    文本序列上的EL2N分数定义为误差向量的平均L2准则。
    EL2N分数较低的例子通常是模型在早期训练阶段学习到的，这可能是因为它们相对较容易。相反，EL2N分数较高的示例则表明模型在学习这些示例时仍会遭受重大损失，因此可能需要更多的迭代来学习。

### 记忆化(memorization)
    z是一个数据点，zˆ是参考模型预测的标记序列，1(.)是一个指示函数。参考模型使用数据点z的前M个标记来计算记忆分数。
    然后，再贪婪地生成N个额外的token，即zˆ。记忆分数是贪婪生成的N个标记（zˆM:M+N）中与原始数据点（zM:M+N）完全匹配的部分。
    记忆分数越高，说明模型逐字再现的文本越多。

### 掩码(mask)
    掩码(Mask)表示屏蔽某些值，以便在更新参数时它们不起作用。
    变换器(Transformer)模型中有两种掩码:填充掩码和序列掩码。
    填充掩码用于所有缩放的点积注意，序列掩码仅用于解码器的自注意。
    填充掩码解决了输入序列具有可变长度的问题。具体来说，在较短的序列后填0。
    但是如果输入序列太长，则会截取左侧的内容，并直接丢弃多余的内容。因为这些填充的位置实际上没有意义，注意机制不应该集中在这些位置，所以需要做一些处理。
    具体方法是在这些位置的值上加一个非常大的负数（负无穷大），这样这些位置的概率在softmax计算之后将接近0！
    填充掩码实际上是一个张量，每个值都是一个布尔值，false值指想要处理的值。
    padding mask: 处理非定长序列，区分 padding 和非 padding 部分，如在 RNN 等模型和 Attention 机制中的应用等；
    sequence mask: 防止标签泄漏 如: Transformer decoder 中的mask 矩阵，bert中的【Mask】位，XLNet中的mask矩阵等。
    Transformer 是包括 Encoder和 Decoder的，Encoder中 self-attention 的 padding mask就是用于处理非定长序列，
    而 Decoder 还需要防止标签泄露，即在t时刻不能看到t时刻之后的信息，因此在上述 padding mask的基础上，还要加上 sequence mask。
    在NLP中，文本一般是不定长的，所以在进行 batch训练之前，要先进行长度的统一，过长的句子可以通过truncating(截断)到固定的长度，过短的句子可以通过 padding(填充)增加到固定的长度，
    但是 padding 对应的字符只是为了统一长度，并没有实际的价值，因此希望在之后的计算中屏蔽它们，这时候就需要 Mask。
    在 Attention 机制中，同样需要忽略 padding 部分的影响，这里以Transformer encoder中的self-attention为例：self-attention中，Q和K在点积之后，需要先经过mask再进行softmax，
    因此，对于要屏蔽的部分，mask之后的输出需要为负无穷，这样softmax之后输出才为0。
    在语言模型中，常常需要从上一个词预测下一个词，但如果要在LM中应用 self attention 或者是同时使用上下文的信息，要想不泄露要预测的标签信息，就需要 mask 来“遮盖”它。
    sequence mask 一般是通过生成一个上三角矩阵来实现的，上三角区域对应要mask的部分。
    在Transformer 的 Decoder中，先不考虑 padding mask，一个包括四个词的句子[A,B,C,D]在计算了相似度scores之后，将scores的上三角区域mask掉，即替换为负无穷，
    再做softmax得到第三幅图。这样，比如输入 B 在self-attention之后，也只和A，B有关，而与后序信息无关。
    因为在softmax之后的加权平均中: B’ = 0.48A+0.52B，而 C，D 对 B’不做贡献。
    对于 decoder 的 self-attention，里面使用到的 scaled dot-product attention，同时需要padding mask 和 sequence mask 作为 attn_mask，具体实现就是两个mask相加作为attn_mask。
    其他情况，attn_mask 一律等于 padding mask。

### 掩码语言模型（Masked Language Model, MLM）
    MLM 随机遮蔽模型输入中的一些 token，目标在于仅基于遮蔽词的语境来预测其原始词汇 id。
    实际上就是一个完形填空任务，随机 Mask 掉文本中的某些字词，然后要模型去预测被 Mask 的字词。
    与从左到右的语言模型预训练不同，MLM 目标允许表征融合左右两侧的语境，从而预训练一个深度双向 Transformer。

### 句子预测(Next Sentense Prediction, NSP)
    在训练过程中，对于 sentenceA 和 sentenceB，随机选择 50% 对，
    将其中 sentenceB 置换成其他任意文本，然后标签为 0/1，从而构造成了二值分类任务。
    最终，pre-training 过程中，training loss 为平均 NSP 似然之和。
    NSP任务实际上是一个二分类，训练数据的正样本是通过采样同一个文档中的两个连续的句子，而负样本是通过采用两个不同的文档的句子。该任务主要是希望能提高下游任务的效果，例如NLI自然语言推理任务。
    但是后续的研究发现该任务效果并不好，主要原因是因为其任务过于简单。NSP其实包含了两个子任务，主题预测与关系一致性预测，但是主题预测相比于关系一致性预测简单太多了。
    这里提一下为啥包含了主题预测，因为正样本是在同一个文档中选取的，负样本是在不同的文档选取的，假如我们有2个文档，一个是娱乐相关的，一个是教育相关的，那么负样本选择的内容就是不同的主题，
    而正样都在娱乐文档中选择的话预测出来的主题就是娱乐，在教育的文档中选择的话就是后者这个主题了。


### 句子顺序预测(sentence-order prediction ,SOP)
    句子间顺序预测, 也就是给模型两个句子,让模型去预测两个句子的前后顺序 。
    
### 排列语言模型(Permutation Language Model, PLM)
    又称，乱序语言模型。
    PLM的本质就是LM联合概率的多种分解机制的体现；
    将LM的顺序拆解推广到随机拆解，但是需要保留每个词的原始位置信息（PLM只是语言模型建模方式的因式分解/排列，并不是词的位置信息的重新排列！）
    如果遍历 𝑇! 种分解方法，并且模型参数是共享的，PLM就一定可以学习到各种双向上下文；
    换句话说，当我们把所有可能的𝑇! 排列都考虑到的时候，对于预测词的所有上下文就都可以学习到了！
    由于遍历 𝑇! 种路径计算量非常大（对于10个词的句子，10!=3628800）。因此实际只能随机的采样𝑇!里的部分排列，并求期望；

### ELMo，(Embeddings from Language Models)
    ELMo顾名思义是从Language Models得来的embeddings，确切的说是来自于Bidirectional language models。
    利用语言模型来获得一个上下文相关的预训练表示，称为ELMo。
    与word2vec相比ELMo使上下文无关的静态(static)向量变成上下文相关的动态(dynamic)向量,一定程度解决了一词多义的问题。
    每一个词语的表征都是整个输入语句的函数。具体做法就是先在大语料上以language model为目标训练出bidirectional LSTM模型，然后利用LSTM产生词语的表征。
    在进行有监督的NLP任务时，可以将ELMo直接当做特征拼接到具体任务模型的词向量输入或者是模型的最高层表示上。
    不像传统的词向量，每一个词只对应一个词向量，ELMo利用预训练好的双向语言模型，然后根据具体输入从该语言模型中可以得到上下文依赖的当前词表示（对于不同上下文的同一个词的表示是不一样的），
    再当成特征加入到具体的NLP有监督模型里。
    引入双向语言模型，其实是2个单向语言模型（前向和后向）的集成；通过保存预训练好的2层biLSTM，通过特征集成或finetune(微调)应用于下游任务；
    缺点：本质上为自回归语言模型，只能获取单向的特征表示，不能同时获取上下文表示；LSTM不能解决长距离依赖。
    为什么不能用biLSTM构建双向语言模型？不能采取2层biLSTM同时进行特征抽取构建双向语言模型，否则会出现标签泄漏的问题；因此ELMO前向和后向的LSTM参数独立，共享词向量，独立构建语言模型；
    ELMo 和 BERT 的区别：ELMo 模型是通过语言模型任务得到句子中单词的 embedding 表示，以此作为补充的新特征给下游任务使用。
    因为 ELMO 给下游提供的是每个单词的特征形式，所以这一类预训练的方法被称为“Feature-based Pre-Training”。
    而 BERT 模型是“基于 Fine-tuning 的模式”，下游任务需要将模型改造成 BERT 模型，才可利用 BERT 模型预训练好的参数。

### Feature-based
    又称feature-extraction 特征提取。就是用预训练好的网络在新样本上提取出相关的特征，然后将这些特征输入一个新的分类器，从头开始训练的过程。
    也就是说在训练的过程中，网络的特征提取层是被冻结的，只有后面的密集链接分类器部分是可以参与训练的。

### 微调(Fine-tuning, finetune)
    冻结预训练模型的部分卷积层（通常是靠近输入的多数卷积层，因为这些层保留了大量底层信息）甚至不冻结任何网络层，
    训练剩下的卷积层（通常是靠近输出的部分卷积层）和全连接层。
    只要分阶段训练，模型稍有不同，都可以叫fine-tune(微调)
    fine-tuning: 微调。和feature-based的区别是，训练好新的分类器后，还要解冻特征提取层的顶部的几层，然后和分类器再次进行联合训练。
    之所以称为微调，就是因为在预训练好的参数上进行训练更新的参数，比预训练好的参数的变化相对小，这个相对是指相对于不采用预训练模型参数来初始化下游任务的模型参数的情况。
    也有一种情况，如果你有大量的数据样本可以训练，那么就可以解冻所有的特征提取层，全部的参数都参与训练，但由于是基于预训练的模型参数，所以仍然比随机初始化的方式训练全部的参数要快的多。
    对于作者团队使用BERT模型在下游任务的微调时，就采用了解冻所有层，微调所有参数的方法。

### 判别性微调（Discr）
    由于不同的层捕获不同类型的信息，它们应该在不同程度上进行微调。 
    不是对模型的所有层使用相同的学习速率，而是区分性微调允许我们以不同的学习速率调整每个层。

### 倾斜的三角学习率（STLR）
    它首先线性地增加学习率，然后根据训练时间线性衰减它; 具有短期增长和长衰减期

### Warmup学习率
    Warm-up的策略就是初期用一个逐渐递增的学习率去初始化网络，渐渐初始化到一个更优的搜索空间。
    Warmup是在ResNet论文中提到的一种学习率预热的方法，它在训练开始的时候先选择使用一个较小的学习率，
    训练了一些epoches或者steps(比如4个epoches,10000steps),再修改为预先设置的学习来进行训练。
    由于刚开始训练时,模型的权重(weights)是随机初始化的，此时若选择一个较大的学习率,可能带来模型的不稳定(振荡)，
    选择Warmup预热学习率的方式，可以使得开始训练的几个epoches或者一些steps内学习率较小,
    在预热的小学习率下，模型可以慢慢趋于稳定,等模型相对稳定后再选择预先设置的学习率进行训练,使得模型收敛速度变得更快，模型效果更佳。
    总之，使用Warmup预热学习率的方式,即先用最初的小学习率训练，然后每个step增大一点点，直到达到最初设置的比较大的学习率时（注：此时预热学习率完成），
    采用最初设置的学习率进行训练（注：预热学习率完成后的训练过程，学习率是衰减的），有助于使模型收敛速度变快，效果更佳。
    constant warmup: 是从一个很小的学习率一下变为比较大的学习率，但这可能会导致训练误差突然增大。
    gradual warmup: 从最初的小学习率开始，每个step增大一点点学习率，直到达到最初设置的比较大的学习率时，采用最初设置的学习率进行训练。
    warmup:学习率热身。规定前多少个热身步骤内，对学习率采取逐步递增的过程。热身步骤之后，会对学习率采用衰减策略。这样训练初期可以避免震荡，后期可以让loss降得更小。
    
### 通用语言模型微调(Universal Language Model Fine-tuning for Text Classification,ULMFiT)
    要点：三阶段训练：LM预训练+精调特定任务LM+精调特定分类任务；特征抽取：3层AWD-LSTM；精调特定分类任务：逐层解冻；
    ULMFiT由三个阶段组成：
    a）LM在一般领域语料库上进行训练，以捕获不同层次语言的一般特征。 
    b）使用判别性微调（'Discr'）和倾斜三角学习率（STLR）对目标任务数据进行微调，以学习任务特定的功能。 
    c）使用逐渐解冻，'Discr'和STLR对目标任务进行微调，以保留低级表示并适应高级表示

### SiATL(Simple Approach for Transfer Learning)
    SiATL要点：
    二阶段训练：LM预训练+特定任务精调分类任务（引入LM作为辅助目标，辅助目标对于小数据有用，与GPT相反）；
    特征抽取：LSTM+self-attention；
    精调特定分类任务：逐层解冻；

### GPT(Generative Pre-trained Transformer)
    GPT（Generative Pre-trained Transformer）是一种基于 Transformer 架构的预训练语言模型。它由一系列的 Transformer 编码器组成，其中每个编码器都是一个包含自注意力机制的神经网络结构。
    GPT 的训练过程分为两个阶段：预训练和微调。在预训练阶段，GPT 使用大量的文本数据来学习语言模型，其中包括单词的出现顺序、上下文信息和语法结构等。在微调阶段，GPT 被用于特定的自然语言处理任务，如文本分类、问答系统和机器翻译等。
    GPT 的底层原理是基于 Transformer 架构的，它使用了自注意力机制来处理输入数据。自注意力机制可以学习输入数据中的上下文信息，并将其应用于模型的预测中。在 GPT 中，这种机制被用于在输入数据中识别语言模式和上下文关系，并生成下一个单词的概率分布。
    除了自注意力机制外，GPT 还使用了掩码语言模型（Masked Language Modeling，MLM）和下一句预测（Next Sentence Prediction，NSP）等技术来提高模型的性能和泛化能力。
    在 MLM 中，GPT 随机遮盖一些单词，然后要求模型根据上下文预测这些单词的正确形式。在 NSP 中，GPT 被要求根据两个句子的上下文判断它们是否是连续的两句话。
    使用的是标准的语言模型目标函数，即通过前k个词预测当前词，但是在语言模型网络上他们使用了Transformer解码器作为语言模型。
    Transformer模型主要是利用自注意力（self-attention）机制的模型
    具体NLP任务有监督微调时，与ELMo当成特征的做法不同，OpenAI GPT不需要再重新对任务构建新的模型结构，
    而是直接在transformer这个语言模型上的最后一层接上softmax作为任务输出层，然后再对这整个模型进行微调。
    GPT1.0要点：
        2018年，OpenAI 公司推出了具有1.17亿个参数的GPT-1（Generative Pre-training Transformers）模型。
        基本思想：先在没有标签的数据集上训练预训练语言模型，再在子任务上微调（自监督学习）。与之前的任务（word2vec也是在没有标签的数据集上预训练语言模型）不同，微调时只需要改变模型输入的形式，而不需要对模型结构进行改变。
        模型结构选用的是12层的Transformer的decoder。
        无监督的预训练过程：给定前k kk个词，预测当前词。
        有监督的微调阶段：有标签的数据集C上每个样本包含一个句子X和对应的标签y。将X输入预训练模型，获取decoder最后一层的对应的编码，将它传入一个额外的线性输出层来预测y。
        拥有 1.5 亿个参数。
        采用Transformer进行特征抽取，首次将Transformer应用于预训练语言模型；
        finetune阶段引入语言模型辅助目标（辅助目标对于大数据集有用，小数据反而有所下降，与SiATL相反），解决finetune过程中的灾难性遗忘；
        预训练和finetune一致，统一二阶段框架；
        GPT-1 最核心的能力是语言模型，它可以根据前面的文本内容预测下一个单词或词组的概率。
        GPT-1 的核心是利用深度学习技术和大规模文本语料库，通过自我训练得到了能够理解语言规律和抽象概念的模型，从而可以生成自然流畅的文本。
        GPT-1范式：预训练 + finetune(微调)；也是自监督预训练 (语言模型)+微调的范式。预训练：用的是标准的语言模型的目标函数，即似然函数，根据前k个词预测下一个词的概率。
        微调：用的是完整的输入序列+标签。目标函数=有监督的目标函数+λ*无监督的目标函数。
        通过在序列前后添加 [Start] 和 [Extract] 特殊标识符来表示开始和结束，序列之间添加必要的 [Delim] 标识符来表示分隔】，接上对应下游任务的层，就可实现不同下游任务。
    GPT2.0要点：
        2019年，OpenAI 公布了一个具有15亿个参数的模型：GPT-2。
        GPT-2也是基于transformer的decoder结构。但和gpt-1有所区别，区别在于layer-norm的位置有所调整，也就是将layer-nore放在了每个残差块的里面即输入位置。反正呢，这样的操作在实验中证明是为了减少预训练过程中各层之间的方差变化，使梯度更加稳定。
        GPT-2 的模型规模比 GPT-1 大了好几倍，包含了更多的参数。
        相比于 GPT-1，GPT-2 在模型规模、预训练任务、生成能力和零样本学习（zero-shot）能力等方面都有很大的提升.
        无监督的预训练阶段：同GPT1.0。
        zero-shot的下游任务：下游任务转向做zero-shot而放弃微调，相较于GPT1.0，出现一个新的问题：样本输入的构建不能保持GPT的形态，因为模型没有机会学习Start，Delim，Extract这些特殊token。
        因此，GPT-2使用一种新的输入形态：增加文本提示，后来被称为prompt。
        没有针对特定模型的精调流程：GPT2.0认为预训练中已包含很多特定任务所需的信息。
        生成任务取得很好效果，使用覆盖更广、质量更高的数据；
        缺点：依然为单向自回归语言模型，无法获取上下文相关的特征表示；
        GPT-2范式：预训练 + zero-shot
        GPT-2可以在zero-shot设定下实现下游任务，即不需要用有标签的数据再微调训练。
        为实现zero-shot，下游任务的输入就不能像GPT那样在构造输入时加入开始、中间和结束的特殊字符，这些是模型在预训练时没有见过的，而是应该和预训练模型看到的文本一样，更像一个自然语言。
        可以通过做prompt的方式来zero-shot。例如机器翻译和阅读理解，可以把输入构造成，“请将下面的一段英语翻译成法语，英语，法语”。
        从一个尽可能大且多样化的数据集中一定能收集到不同领域不同任务相关的自然语言描述示例，数据集里就存在展示了这些prompt示例，所以训练出来就自然而然有一定zero-shot的能力了。
    GPT3.0要点：
        2020年，OpenAI推出了GPT-3 模型——它有1750亿个参数（未开源）。GPT-3 模型架构与 GPT-2 类似，但是规模大了整整两个数量级。
        GPT-3 的训练集也比前两款 GPT 模型要大得多：经过基础过滤的全网页爬虫数据集（4290亿个词符）、维基百科文章（30亿词符）、两个不同的书籍数据集（670亿词符）。
        GPT3.0及其之后版本均未开源；
        GPT-3是相较于GPT-2更大规模的模型，参数量达到了1.75万亿个，拥有更强的语言理解和生成能力，可以执行更多复杂的任务。
        GPT-3的可学习参数达到1750亿，是之前的非稀疏语言模型的10倍以上，并在few-shot的设置上测试它的性能。对于所有子任务，GPT-3不做任何的梯度更新或者是微调。GPT-3的模型和GPT-2一样。
        few-shot：GPT-3是不做梯度更新的few-shot。将一些有标签的样例放在预测文本的上下文。
        GPT-3模型和GPT-2一样，但GPT-3应用了Sparse Transformer中的attention结构。
        GPT-3范式：预训练 + few-shot
    InstructGPT, 可以理解成是“Flan-GPT”：
        2022年3月，OpenAI推出了基于 GPT-3 模型进一步微调的 InstructGPT 模型。
        InstructGPT 的模型训练中加入了人类的评价和反馈数据，而不仅仅是事先准备好的数据集，从而训练出更真实、更无害，且更好地遵循用户意图的语言模型。
        使用用户指令（Instruct）和偏好答案来微调GPT-3模型，让模型生成的内容更符合用户的意图，更真实、更有用（Alignment，对齐过程）。
        这么做的出发点是面向一种经典的应用场景，用户给一条指令声明意图，期望模型生成有用、无害的内容，但使用大量网页数据训练的大语言模型GPT无法直接满足这种诉求，因此需要微调。
    GPT3.5(chatGPT)要点：
        2022年11月，ChatGPT 横空出世，它是基于 GPT-3.5 架构开发的对话AI模型，是 InstructGPT 的兄弟模型。但两者在训练模型的数据量上，以及数据收集、数据如何设置用于训练方面有所不同。
        GPT-3.5是在GPT-3的基础上进一步优化了模型架构和训练技术，提高了模型的效率和泛化能力，同时减少了对大量数据和计算资源的依赖。
        具体来说，GPT-3.5引入了一种新的“分组稀疏注意力”（Grouped Sparse Attention）的架构，可以在不影响模型性能的情况下减少计算量，同时还采用了“标准化知识蒸馏”（Normalized Knowledge Distillation）等技术来进一步提高模型的效率和精度。
        ChatGPT，一个基于GPT-3.5架构的大型语言模型，可以进行自然语言处理任务，如对话生成、文本摘要、语言翻译等。
        ChatGPT 是基于人类反馈的强化学习的指令微调 (Instruction tuning with Reinforcement Learning from Human Feedback RLHF)。
    GPT4.0要点：
        GPT4的模型容量更大。
        GPT4内置了更多核心功能，能够利用机器学习处理多种任务，而GPT3仅可用于对话导向问题，文本生成和历史数据分析。
        GPT4具有更强的对接性，可以集成不同的数据源，进行基于多种任务的训练，而GPT3仅可进行单一任务的特定训练。
    GPT和BERT的区别：
        ①GPT使用的Transformer的Decoder层（目标函数为标准的语言模型，每次只能看到当前词之前的词，需要用到Decoder中的Masked attention），BERT使用的Transformer的Encoder层（目标函数为带[Mask]的语言模型，通过上下文的词预测当前词，对应Encoder）；
    为什么GPT(这里指的是GPT1、GPT2，GPT3及之后实现了反超)的性能比BERT差：
        ①GPT预训练时的任务更难（BERT的base就是为了和GPT对比，参数设定几乎一样）；
        ②BERT预训练用的数据集大小几乎是GPT的四倍；

### 对齐(Alignment)
    所谓的“对齐”，是指“模型与用户对齐”。再进一步展开，“模型与用户对齐”指的是“模型与用户的意图对齐”。
    对正常用户来说，没人希望得到的回答是虚假、有害或无用的，所以模型输出这些内容就被作者们称为“与用户未对齐”。
    像GPT这一类模型（大语言模型）在训练时的训练目标。整个GPT系列，它们最重要的训练目标是：给定一个句子，用句子前面的词去预测下一个词（这种方法被称为自回归）。
    这个自回归的训练目标与用户希望的“真实、有用”的目标之间没有任何直接关系。所以，这是“模型与用户未对齐”的问题根源。
    InstructGPT解决“未对齐”问题的方案是基于人类反馈的强化学习（Reinforcement Learning from Human Feedback，RLHF)。
    ChatGPT是InstructGPT的产品化产物。
    InstructGPT属于一问一答型的模型，并非Chat类产品。所以ChatGPT面临的主要问题是“对话类需求与训练数据未对齐的问题”。
    简单来说，ChatGPT的目的是对话，但之前的所有数据都是一问一答型。所以之前的数据支撑不了ChatGPT的需求。
    解决方案很简单，人工把数据重新整理一遍，将模型单轮的回答也输入模型，形成多轮对话数据集。
    将多轮对话数据集与 InstructGPT 数据集混合，一同训练模型。

### 基于人类反馈的强化学习（Reinforcement Learning from Human Feedback，RLHF)
    RLHF过程如下：
    步骤一：有监督微调GPT的输入一般被称为prompt（提示），其实就是一串文本。ChatGPT的输出也是一串文本。
        为了来调整GPT的行为，作者们从OpenAI开放的API接口接收到的prompt中随机采样一部分prompt，然后让标注员手工去写这些prompt对应的正确答案。
        这样实际上形成了许多样本对：（prompt，Answer），其中Answer是人类认为正确的答案。然后拿着这些样本对（prompt，Answer）让模型继续训练。
        这是一个有监督过程，因为对于任意的输入prompt，我们都向模型提供了正确的答案Answer。参与上面训练过程的模型是GPT3（之前预训练好的）。
        经过这一步调整得到的模型被称为supervised policy。之所以叫policy，而不叫model，仅仅是因为在强化学习的语境中，大家更习惯使用policy。
    步骤二：训练激励模型（Reward Model）
        激励模型的本质是一个评分函数。我们可以输入一个样本对（prompt，Answer），让激励模型来评价Answer作为prompt回答的合理性。
        该合理性是一个分值，分越高，表示越合理。为了训练激励模型，首先随机采样一个prompt，然后让上一步训练好的supervised policy根据prompt输出多个答案，比如A、B、C、D四个答案。
        然后让标注人员人为对这四个答案的合理性进行排序。比如排序结果是D > C > A = B。
        为什么要让模型输出多个答案而不是两个？这是因为对于D > C > A = B这样的一个排序结果，它实际隐含了大量两两关系，比如D > C，D > A， C > B等等。
        如果我们把每一个两两关系看作一个样本，那么人工一次标注多个结果的相对关系，实际上能产生大量两两关系。而这些两两关系正是激励模型所需要的输入。
        激励模型可以通过排序学习（Learning-to-Rank）来实现。简单说，我们通过输入给模型类似D > C，C > B这样的样本，模型最后能输出一个评分函数。
        虽然这个过程不是特别直觉（因为我们输入的是两个样本与它们之间的关系，并不包含评分），但像RankNet这样的经典方法都能实现这样的效果。
    步骤三：强化学习
        当训练好激励模型RM后，可以使用RM来替代人的作用。比如，采样一个prompt，然后让supervised policy输出一个结果Answer。
        然后用激励模型RM对Answer进行评分。并且将这个评分作为反馈再送进supervised policy让它学习。
        这样相当于，supervised policy每做一次输出，都会收到一个反馈。因此它能基于这个反馈来持续调整自己的行为。
        最后经这三步训练得到的模型就称为InstructGPT，也被称为GPT3.5。RM的反馈的质量决定了最后强化学习的成败。InstructGPT的成功表明这样的方法是有效的。
    所有的能力都是模型本来就有的，而不是通过 RLHF 注入的。RLHF 的作用是触发 / 解锁突现能力。这个论点主要来自于数据量大小的比较：因为与预训练的数据量相比，RLHF 占用的计算量 / 数据量要少得多。
    模型知道它不知道什么不是通过编写规则来实现的，而是通过 RLHF 解锁的。这是一个非常令人惊讶的发现，因为 RLHF 的最初目标是让模型生成复合人类期望的回答，这更多是让模型生成安全的句子，而不是让模型知道它不知道的内容。

### BLOOM（BigScience Large Open-science Open-access Multilingual Language Model）
    BLOOM-176B 是在46种自然语言和13种编程语言上训练的1760亿参数语言模型，其是由数百名研究人员合作开发和发布的。
    BLOOM是在一个称为ROOTS的语料上训练的，其是一个由498个Hugging Face数据集组成的语料。共计1.61TB的文本，包含46种自然语言和13种编程语言。
    对BLOOM预训练之后，应用相同的大规模多任务微调（指令微调），使BLOOM具有多语言zero-shot任务泛化能力。称得到的模型为BLOOMZ。
    BLOOM模型是自回归结构，具体包含了70层transformer decoder。
    BLOOM 模型也是 Decoder-only 架构，但和原始 transformer-decoder 结构有所区别，具体来说:
    1、ALiBi Positional Embeddings。ALiBi 位置嵌入不是向嵌入层添加位置信息，而是根据键和查询的距离直接衰减注意力分数。它也能带来更平滑的训练和更好的下游性能。
    2、Embedding LayerNorm。在 embedding 层之后立即添加额外的归一化层（layer norm 层）。这样可以显著提高训练的稳定性。

### OPT(Open Pretrained Transformer)
    OPT-175B, 由Meta AI 2022年5月开源的，OPT-175B：1750亿参数大模型.
    GPT-3 也具有 1750 亿参数.
    需要注意的是，所有公开的(OPT-175B、BLOOM-176B、GLM-130B)对 GPT-3 的复现都失败；
    这里称之为 “失败”，是指训练得出模型有接近 GPT-3 或者更大的参数量，但仍无法与 GPT-3 原始文献中报告的性能所匹配。
    在这一标准下，GPT-3 和 PaLM 是 “成功” 的，但这两个模型都不是公开的。而所有的公开模型（例如：OPT-175B 和 BLOOM-176B）都在一定程度上 “失败” 了。
    尽管它背后的模型可能使用了指令微调（instruction tuning， 正如 InstructGPT 那样），类似的使用了指令微调的 OPT 版本（OPT-IML）和 BLOOM 版本（BLOOMZ）也仍然远比 InstructGPT 和 FLAN-PaLM（PaLM 的指令微调版本）要差得多。
    预训练数据(如“GPT-3 和 PaLM ”有一部分高质量训练数据未公开)和训练策略(如“GPT-3 和 PaLM ”有一部分训练策略未公开)等原因，也是导致“OPT-175B 和 BLOOM-176B”，相比“GPT-3 和 PaLM ”效果差的原因；

### PaLM（Pathways Language Model）
    Google的超大模型PaLM；该模型不是公开的;
    PaLM 540B 使用6144 个TPU v4芯片训练，模型540 B参数，780 B高质量token，密集激活，Transformer 语言模型。
    PaLM与GPT-3模型一样，只使用Decoder结构。
    当PaLM模型从62B变为540B时，模型效果有了跨越式的提升。
    另外，PaLM540B在多数评测中高于人类的平均水平，有35%低于人类平均。

### 指令微调 (Flan，Finetuning language models) 
    使用指令(Instruct)来微调(Fine-tune)一个预训练模型,我们称之为指令微调(FLAN)。
    在具有多种指令模板类型的数据源集合上，对预训练模型进行指令微调。
    将该微调过程称为FLAN（Finetuning language models），并将“Flan”附加到最终需要微调的模型名前（例如Flan-PaLM）。
    实验表明，FLAN在几种模型和结构上都表现良好。
    对非CoT和CoT数据进行联合微调。这种联合微调使CoT性能大幅提高，同时保持在非CoT任务上的性能，允许单一模型在所有评估中表现良好。
    CoT联合微调一个大型模型提高了CoT任务的性能，同时保持了非CoT任务的性能改进。
    指令微调在不同架构模型中具有普遍性。
    指令微调提高了可用性，经过微调的模型产生的输出结果更符合人类的偏好。
    指令微调的计算效率相对较高，尽管扩大语言模型的规模已被证明可以可靠地提高性能，但它需要大量的计算。
    指令微调以相对较少的计算量提高了模型的性能。例如，对于PaLM 540B来说，指令微调只需要0.2%的预训练计算量，但却使整个评估基准的标准化平均值提高了9.4%。
    此外，使用指令微调的较小的模型有时会比没有指令微调的较大的模型表现更好。
    指令微调不会为模型注入新的能力 —— 所有的能力都已经存在了。指令微调的作用是解锁 / 激发这些能力。这主要是因为指令微调的数据量比预训练数据量少几个数量级（基础的能力是通过预训练注入的）。
    指令微调将 GPT-3.5 的分化到不同的技能树。有些更擅长上下文学习，如 text-davinci-003，有些更擅长对话，如 ChatGPT。
    指令微调通过牺牲性能换取与人类的对齐（alignment）。OpenAI 的作者在他们的指令微调论文中称其为「对齐税」 (alignment tax)。

### Flan-T5
    谷歌的AI团队2022年10月开源的基于思维链进行大模型微调的预训练模型Flan-T5。
    对Flan-T5模型进行了指令调整（从80M到11B）。这些检查点具有强大的zero-shot、few-shot和CoT能力，远远超过之前的公共检查点，如T5。
    例如，Flan-T5比基础T5有两位数的提高，甚至在一些具有挑战性的BIG-Bench任务上超过了PaLM 62B。

### LLama（Large Language Model Meta AI）
    Meta 的 LLaMA 是“大型语言模型 Meta AI” （Large Language Model Meta AI）的缩写。
    由facebook（即Meta AI）2023年3月，提供的7B～65B的模型，13B的模型效果可超GPT-3（175B），65B的模型效果直逼谷歌的PaLM（540B）；
    训练模型只用了开源数据集，trillions of token。
    训练语料中维基百科和图书领域不包括中文。
    多项任务上SOTA，并且开源了所有模型权重。
    训练数据集（主要是英文，因此中文和中文微调效果堪忧）
    模型结构：和GPT一样，同样是Transformer Decoder架构；
    训练过程：7B和13B的模型在1T的token上进行训练；33B和65B的模型则在1.4T的token上进行了训练。
    LLaMA模型不允许发布调优后的完整模型权重，但是可以发布原始的模型的diff。因此，基于LLaMA针对中文的指令调优模型，网上难找到直接下载的模型，但可以也可以还原出调优后的模型。
    原始LLama模型下载地址参见：https://github.com/facebookresearch/llama/pull/73 或 https://huggingface.co/nyanko7/LLaMA-7B

### Alpaca
    2023年3月，斯坦福基于LLama做指令微调得到了Alpaca模型，效果和GPT-3.5差不多。
    训练方式：使用指令微调的方式，在LLama 7B模型上训练；训练数据规模为5.2万；
    目前开放了：测试Demo、训练数据集、训练数据的生成过程、训练代码；预训练的权重暂未开放；
    斯坦福基于 LLaMA 7B 微调出一个具有 70 亿参数的新模型 Alpaca，他们使用了 Self-Instruct 论文中介绍的技术生成了 52K 条指令数据，
    同时进行了一些修改，在初步的人类评估中，Alpaca 7B 模型在 Self-Instruct 指令评估上的表现类似于 text-davinci-003（GPT-3.5）模型。
    但遗憾的是，Alpaca 的种子任务都是英语，收集的数据也都是英文，因此训练出来的模型未对中文优化。

### BELLE（Bloom-Enhanced Large Language model Engine）
    BELLE 基于 开源预训练大语言模型（如 BLOOM、LLaMA），对中文做了优化，模型调优仅使用由 ChatGPT 生产的数据（不包含任何其他数据）。
    基于BLOOMZ-7B1-mt优化后的模型：BELLE-7B-0.2M，BELLE-7B-0.6M，BELLE-7B-1M，BELLE-7B-2M
    基于Meta LLaMA实现调优的模型：BELLE-LLaMA-7B-0.6M-enc , BELLE-LLaMA-7B-2M-enc , BELLE-LLaMA-7B-2M-gptq-enc , BELLE-LLaMA-13B-2M-enc , 
    BELLE-on-Open-Datasets 以及基于LLaMA做了中文词表扩充的预训练模型BELLE-LLaMA-EXT-7B。
    开源中文对话大模型70 亿参数的 BELLE。
    在数据方面，该项目参考Stanford Alpaca 的数据收集代码，开源了基于这段代码生成了约 100 万条中文数据，5 万条英文数据，在 BLOOMZ-7B 模型训练得到的 checkpoint 上传在 Hugging Face。

### ColossalChat
    ColossalChat是HPC-AI Tech开源的一个聊天机器人大模型，其基础模型来自于Meta开源的LLaMA。
    ColossalChat的主要特点如下：
    有一个在线演示的网站，完全免费且不需要注册。
    训练代码完全开源，且是全球第一个公开RLHF训练代码的模型，包括70亿参数和130亿参数预训练结果。
    开源了一个10.4万条中英文数据。
    仅需要4GB的GPU显存即可运行4-bit量化的70亿参数模型。
    就性能方面来说，ColossalChat只需要不到100亿个参数就能达到中英文双语水平，通过在大语言模型基础上进行RLHF微调，达到与ChatGPT和GPT-3.5相当的效果。
    但实测(https://chat.colossalai.org/)中文效果并不理想。

### GLM
    GLM-130B是在GPT-3之后，清华的大语言模型方向的尝试。不同于 BERT、GPT-3 以及 T5 的架构，GLM-130B是一个包含多目标函数的自回归预训练模型。
    GLM-130B在2022年8月开放，有一些独特的优势：
    双语：同时支持中文和英文。
    高精度（英文）：在公开的英文自然语言榜单 LAMBADA、MMLU 和 Big-bench-lite 上优于 GPT-3 175B（API: davinci，基座模型）、OPT-175B 和 BLOOM-176B，但效果不如最近的OpenAI GPT-3 的 API 表现。
    高精度（中文）：在7个零样本 CLUE 数据集和5个零样本 FewCLUE 数据集上明显优于 ERNIE TITAN 3.0 260B 和 YUAN 1.0-245B。
    快速推理：首个实现 INT4 量化的千亿模型，支持用一台 4 卡 3090 或 8 卡 2080Ti 服务器进行快速且基本无损推理。
    可复现性：所有结果（超过 30 个任务）均可通过我们的开源代码和模型参数复现。
    跨平台：支持在国产的海光 DCU、华为昇腾 910 和申威处理器及美国的英伟达芯片上进行训练与推理。

### ChatGLM
    ChatGLM参考ChatGPT 的设计思路，在千亿基座模型 GLM-130B中注入了代码预训练，通过有监督微调（Supervised Fine-Tuning）等技术实现人类意图对齐。
    清华仅开源 ChatGLM-6B 模型。ChatGLM-6B 是一个具有62亿参数的中英双语语言模型。通过使用与 ChatGLM（http://chatglm.cn）相同的技术，ChatGLM-6B 初具中文问答和对话功能，并支持在单张 2080Ti 上进行推理使用。
    ChatGLM-6B 有如下特点：
    充分的中英双语预训练：ChatGLM-6B 在 1:1 比例的中英语料上训练了 1T 的 token 量，兼具双语能力。
    优化的模型架构和大小：吸取 GLM-130B 训练经验，修正了二维 RoPE 位置编码实现，使用传统FFN结构。6B（62亿）的参数大小，也使得研究者和个人开发者自己微调和部署 ChatGLM-6B 成为可能。
    较低的部署门槛：FP16 半精度下，ChatGLM-6B 需要至少 13GB 的显存进行推理，结合模型量化技术，这一需求可以进一步降低到 10GB（INT8） 和 6GB（INT4）， 使得 ChatGLM-6B 可以部署在消费级显卡上。
    更长的序列长度：相比 GLM-10B（序列长度1024），ChatGLM-6B 序列长度达 2048，支持更长对话和应用。
    人类意图对齐训练：使用了监督微调（Supervised Fine-Tuning）、反馈自助（Feedback Bootstrap）、人类反馈强化学习（Reinforcement Learning from Human Feedback） 等方式，使模型初具理解人类指令意图的能力。

    ChatGLM-6B 也有相当多已知的局限和不足：
    模型容量较小：6B 的小容量，决定了其相对较弱的模型记忆和语言能力。在面对许多事实性知识任务时，ChatGLM-6B 可能会生成不正确的信息；她也不擅长逻辑类问题（如数学、编程）的解答。
    可能会产生有害说明或有偏见的内容：ChatGLM-6B 只是一个初步与人类意图对齐的语言模型，可能会生成有害、有偏见的内容。
    较弱的多轮对话能力：ChatGLM-6B 的上下文理解能力还不够充分，在面对长答案生成，以及多轮对话的场景时，可能会出现上下文丢失和理解错误的情况。
    英文能力不足：训练时使用的指示大部分都是中文的，只有一小部分指示是英文的。因此在使用英文指示时，回复的质量可能不如中文指示的回复，甚至与中文指示下的回复矛盾。
    易被误导：ChatGLM-6B 的“自我认知”可能存在问题，很容易被误导并产生错误的言论。即使该模型经过了1万亿标识符（token）左右的双语预训练，并且进行了指令微调和人类反馈强化学习（RLHF），但是因为模型容量较小，所以在某些指示下可能会产生有误导性的内容。

### 量化（Quantization）
    正如其它模型压缩方法一样，对模型的量化基于一个共识。那就是复杂的、高精度表示的模型在训练时是必要的，因为我们需要在优化时捕捉微小的梯度变化，然而在推理时并没有必要。
    也就是说，网络中存在很多不重要的参数，或者并不需要太细的精度来表示它们。另外，实验证明神经网络对噪声鲁棒，而量化其实也可看作是噪声。
    这就意味着我们在部署模型前可以将之化简，而表示精度降低就是化简的重要手段之一。我们知道，大多深度学习训练框架默认下模型的参数是32位浮点的，计算也是32位浮点的。
    模型量化的基本思想就是用更低精度（如8位整型）来代替原浮点精度，达到节省内存 and/or 加速计算的效果。
    量化最核心的挑战是如何在减少表示精度的同时不让模型的准确度掉下来，即在压缩率与准确率损失间作权衡(trade-off)。
    衍生出很多子问题，比如量化对象是什么（weight，activation，gradient），量化到几位（8位，4位，2位，1位），量化参数（如step size，clipping value）如何选择，
    量化参数是否可以自动优化，不同层是否需要不同的量化参数，如何在量化后恢复准确率或者在训练时考虑量化，等等。
    在线性量化中，对原值域和量化后值域都不要求关于0对称，故称为非对称（Asymmetric）量化。如果将原值域中的0在量化后的值(zero point)设为0，则称为对称（Asymmetric）量化。
    可以说对称量化是非对称量化的特例。另外，如果非对称量化中整个值域都落在正区间，那就可以用无符号整型表示，否则用有符号整型。
    量化方法还分为均匀（Uniform）量化与非均匀（Non-uniform）量化。最简单就是让量化的等级间距离相等，这类量化称为均匀量化。
    但直觉上这样会有更多的信息丢失，因为一般来说dynamic range中肯定有些区域点密集，有些稀疏。相应地，有均匀量化就有非均匀量化，它是指量化等级间不等长，如log quantization。
    再高级一点，那就是通过学习来得到更大自由度的映射（可通过查找表表示）。直觉上，非均匀量化似乎能达到更高的准确率，但它的缺点是不利于硬件加速。
    根据量化后的目标区间，可以分为二值量化（1, -1）、三值量化（-1, 0, 1）、定点数量化（INT4, INT8）、2 的指数量化。
    根据量化节点的分布，可以分为均匀量化和非均匀量化。非均匀量化可以根据待量化参数的概率分布计算量化节点。如果某一个区域参数取值较为密集，就多分配一些量化节点，其余部分少一些。这样量化精度较高，但计算复杂度也高。
    LLM 主要采用的是均匀量化，它又可以分为对称量化、非对称量化。
    根据量化的时机，我们有量化感知训练和训练后量化两条路径。
    对于量化位数,有float16, int8, int4等。
    按量化过程中是否需要进行训练可分为两类PTQ和QAT。
    大模型ChatGLM的int8量化方案中，只是对权重weight进行了量化和反量化(weightOnly)，并没有完整的对weight和activation进行量化。
    chatGLM的量化并没有加速推理的能力，只有降低显存的能力。其量化过程：加载模型的fp16权重，采用min_max对weight_fp16进行int8量化，得到Qweight_int8；在推理的时候，
    把Qweight_int8进行反量化得到新的权重Rweight_fp16，然后由这个新权重和输入input_fp16完成后面的前向计算；
    前向计算过程中，相比没有量化的时候，多了一次反量化的计算，而计算精度仍然是fp16之间的计算，所以整体耗时会增加。
    全流程量化指的是包含了激活值和权重两部分量化，在保证模型性能降低的不多的同时，还能减少推理时间。当然要取得这样的效果，需要做好量化和反量化的算法优化，以及适配不同硬件的优化。

### LLM-int8
    transformers 包自带量化方法，使用bnb.nn.Linear4bit或者bnb.nn.Linear8bit模块把一个模型里面所有nn.Linear模块，全部替换掉。
    当然，也不是也有例外，就是不能把模型的lm_head层替换掉。

### 量化感知训练
    Quantization Aware Training (QAT) 量化感知训练：首先正常预训练模型，然后在模型中插入“伪量化节点”，继续微调。
    所谓“伪量化节点”，就是对权重和激活先量化，再反量化。这样引入了量化误差，让模型在训练过程中“感知”到量化操作，在优化 training loss 的同时兼顾 quantization error.

### 训练后量化
    QAT 虽好，但插入“伪量化节点”后微调大大增加了计算成本，尤其是面对超大规模的 LLM。
    目前针对 LLM 的量化研究都集中在 Post-training quantization (PTQ) ——训练后量化。
    像是 LLM.int8(), SmoothQuant, GPT-Q 都属于这一范畴，所以之后的文章讨论的都是 PTQ。
    对于权重而言，我们可以在推理前事先计算好量化系数，完成量化。但是对于激活（即各层的输入），它们事先是未知的，取决于具体的推理输入，会更加棘手。
    根据对激活的量化，分为动态与静态量化。
    训练后量化方案：LLM.in8()、SmoothQuant、GPT-Q、QLoRA

### LLM.int8()
    LLM.in8() 论文中发现：激活中存在一些离群值(outliers)，它们的绝对值明显更大；并且这些离群值分布在少量的几个特征 (features) 中，称为离群特征 (Emergent Features)。
    LLM.in8() 的思路是，既然只有少量的特征包含离群值，那就把这些特征拿出来单独fp16计算，只对剩余特征做int8量化。
    LLM.in8()说起来可以分成三步：
    对激活提取离群值，找出哪些是离群特征；
    对离群特征进行 FP16 矩阵运算，非离群特征进行量化，做 INT8 矩阵运算；
    对非离群特征的计算结果反量化到 FP16，与离群特征的结果相加。
    虽然 LLM.in8() 带来的性能下降微乎其微，但是这种分离计算的方式拖慢了推理速度。
    结论：模型越大，int8() 加速越明显，可能是由于非 outlier 数量变多了，更多的参数进行了 int8 计算，抵消了额外的量化转化时间开销。

### exllamaV2
    exllama-V2是一个用于在现代消费级GPU上本地运行大型语言模型的快速推理库，能够利用最新GPU技术，以非常快的速度对巨大神经网络进行推理，
    在不牺牲太多质量的情况下，实现在普通台式机上运行大模型。

### GGML
    GGML是Llama.cpp的一个核心组件。使用 C 编写，支持 Integer quantization（4-bit, 5-bit, 8-bit） 以及 16-bit float。
    同时也对部分硬件架构进行了加速优化。LLaMa.cpp 有很多周边产品，如 llama-cpp-python 等，我们以 GGML 称呼这类模型量化方案。
    LLaMa.cpp 比 AutoGPTQ 有更快的推理速度，但是还是比 exllama 慢很多。

### SmoothQuant量化
    针对激活中的离群值，SmoothQuant 给出了与 LLM.int8() 不同的解题思路。
    既然激活的量化比权重的量化难得多，那么可以通过一个平滑系数，把二者的难度“中和”一下;
    然后我们对平滑之后的激活X、权重W做量化即可。 
    需要注意的是，对于精度要求高的算子，如 LayerNorm 和 Softmax，计算前反量化为 FP16；其余算子，如矩阵乘法、激活函数，都用 INT8 数据类型。
    结果比较：在大规模模型上，效果与 LLM.int8() 旗鼓相当，但是速度更快。

### GPT-Q
    LLM.int8() 和 SmoothQuant 都属于 round-to-nearest (RTN) 量化：舍入到最近的定点数。GPT-Q 则是把量化问题视作优化问题，逐层寻找最优的量化权重
    GPTQ 的核心思想是逐一量化模型的各个层。
    即对于每个需要被量化的层(对应参数权重)，希望量化前后该层输出变化尽量小。
    GPTQ 主要参考了 Optimal Brain Quanization (OBQ)，对OBQ 方法进行了提速改进。
    我们常用的量化则是把数值近似到一个接近的值， 而剪枝实际上可以看做把数值直接近似成0（某种意义上或许可以称作1bit或0bit量化），可以理解为一种特殊的量化。

### QLoRA
    不同于以上几种的训练后量化方案，QLoRA 是一种参数高效微调（PEFT）方法，它是 LoRA 的量化版本。
    LoRA
    LoRA (Low rank adaptation) 是现在最常用的 PEFT 方法。它也是一种“即插即用” 的方案。
    思路很简单：既然微调就是更新参数，那么不如直接模拟参数的更新量权重 —— 用两个 MLP 去近似 权重 。
    由于 LoRA 微调时不需要更新原本的模型参数，可以对它们进行 8bit 甚至 4bit 量化存储，节省显存、加速训练。
    现阶段，QLoRA 是重要的节省显存的 PEFT 方法。论文声称，微调 65B 的模型只需要 48 GB 显存，并且效果不输 16bit 全参数微调。
    具体来说，QLoRA 的两个主要技术：
    1、提出了一种新的 4bit 数据类型—— 4-bit NormalFloat (NF4) —— 更适合对正态分布的权重做量化；
    2、使用双量化技术（Double Quantization），进一步节省量化常数的空间占用；
    QLoRA 中，模型的权重有两种格式：用 NF4 存储；用 BF16 计算。需要用相应权重计算前向传播时，对 NF4 的权重反量化为 BF16；计算结束后，再量化为 NF4
    先把高精度的模型下载到磁盘，在加载的时候，转化成混合 4bit 的量化模型。注：也有一些模型提供了量化后的版本，可以直接下载。
    Remark 1：这里的量化模型只用于模型推理，不要和 bitsandbytes 提供的 optimizer 量化搞混了。后者可以对优化器状态用 8bit 量化，可以用在训练中。如 bitsandbytes.optim.Adam8bit(....)
    Remark 2：这个时候大多数层的参数变成 uint8，但也有一部分层保持 FP16；bias 的精度也保持 FP16
    
### 参数高效微调 (PEFT) 
    微调是 AI 模型绕不开的一个话题，它是迁移学习的一种（最常用的）实现方法。在“小模型”时代，参数量不过几百万，全参数微调（full fine tuning）毫无压力。
    GPT 系列的 NLP 模型引领潮流后，全参数微调既占显存，速度又慢，相比之下，PEFT (Parameter-Efficient Fine-Tuning) 就显得很重要了。
    目前用得最多的 PEFT 方法就是 LoRA（低秩自适应方法） 了，但其他诸如 Prefix-tuning, Prompt-tuning, P-tuning 也是比较常见的方法。

### In-context learning（上下文学习）
    上下文学习（In-Context Learning）简单来说就是，预训练好的模型在迁移到新任务上的时候并不需要重新训练，而只需要提供任务描述（这个任务描述是可选项）接着提供几个示例（任务查询和对应答案，以一对对的形式组织），
    最后加上要模型回答的查询。将以上内容打包一起作为模型的输入，则模型就能正确输出最后一个查询对应的答案。
    对于一个预训练好的大语言模型，迁移到新任务上的时候，只需要给模型输入几个示例（示例输入和示例输出对），模型就能为新输入生成正确输出而不需要对模型做 fine-tuning。
    In-context learning（上下文学习）其实不算是微调，它没有变动模型结构，也没有更新任何参数。
    In-context learning 也叫 few-shot learning（少样本学习）。在推理阶段，给出下游任务的几个示例，就可以“激发”模型在这类任务上的能力。
    它的本质就是给予模型一些先验知识（prompt），模型输出 token 的概率分布就会改变（条件概率），进而得到合适的输出。
    In-context learning 中的 prompt 是一些自然语言，并且是人们手动设计的，目的是改变条件概率，“激发”模型潜能。
    prompt 的设计对输出有很大影响；并且针对不同的下游任务，需要精心设计不同的 prompt

### Prefix-tuning
    Prefix-tuning是一种自动确定 prompt，而且不一定是自然语言”。它把下游任务的示例替换成一些虚拟 token（下图中的 prefix）。
    这些虚拟 token 并不对应任何自然语言中的 subword，它们仅仅是一些连续的向量
    原先人工设计的 prompt 中那些对应自然语言的真实 token 也要经过嵌入层，被向量化。由于 token 是离散的，得到的结果大概率是次优的。
    相较而言，连续化的 prefix prompt 搜索更具优势。
    Prefix-tuning 时，原本模型的参数被冻结，这些虚拟 token 对应的向量就是可学习参数。
    有一点需要注意：为了增强表达力，不只是嵌入层后的第一个 Transformer block，Prefix-tuning 对模型的每个 Transformer block 的输入都加入了 Prefix vector。
    这些 Prefix vector 对应着自注意力中的 key-value 对. 
    总的来说，Prefix-tuning 通过在更大的向量空间内搜索、优化 Prefix vector，有点像自动化的 prompt engineering. 
    Prefix-tuning 是一种“即插即用”的方法：针对每个下游任务，只需要保存一份 prefix vector 就好了。

### Prompt-Tuning
    Prompt-Tuning 这是 Prefix-tuning 的一个简化版本。
    Prompt-Tuning 也有一些 prefix vector，但它们只拼接在嵌入层的输出（第一个 Transformer block 的输入）；
    由于 Prompt-Tuning 只在第一个 block 拼接，同等 prefix 长度下可训练参数量比 Prefix-tuning 少很多。

### P-Tuning
    和前面两个方法一样，P-Tuning 的思路也是基于 Continuous prefix (soft) prompt。和 Prompt-Tuning 类似，P-Tuning 也是只对模型的输入 embedding 做了拼接。

### Adapter
    Adapter 的思路非常简单，作用在每个transformer层，在特定位置插入了adapter层。

### 动态量化
    推理过程中，实时计算激活的量化系数，对激活进行量化。

## 静态量化
    与动态量化相反，静态量化在推理前就计算好激活的量化系数，在推理过程中应用即可。
    这就要求我们做一些准备工作：准备一些有代表性的数据（称为“校准数据”），送入模型，计算每一层激活的统计量；利用这些统计量（主要是最大值）计算量化系数。
    相比之下，静态量化更快，但对校准数据的敏感度较高。

### Post-training float16 quantization
    比较直观，从FP32压到FP16。模型小一半。一些GPU支持FP16计算，因此性能通常会有可观地提高，而且大部分情况下准确率损失不大，可以说是一种性价比比较高的做法。
    注意，fp16计算也称Half，CPU上不支持，显卡不支持half-float数据类型。

### FP16 
    FP16也叫做 float16，两种叫法是完全一样的，全称是Half-precision floating-point(半精度浮点数)，在IEEE 754标准中是叫做binary16，简单来说是用16位二进制来表示的浮点数
    Sign(符号位): 1 位，0表示整数；1表示负数。
    Exponent(指数位)：5位，简单地来说就是表示整数部分
    Fraction(尾数位)：10位，简单地来说就是表示小数部分

### BF16
    BF16也叫做bfloat16(这是最常叫法)，或叫“BF16”，全称brain floating point，也是用16位二进制来表示的，和上述FP16不一样的地方就是指数位和尾数位不一样
    Sign(符号位): 1 位，0表示整数；1表示负数
    Exponent(指数位)：8位，表示整数部分，偏置值是 127
    Fraction(尾数位)：7位，表示小数部分
    这里要注意一下，并不是所有的硬件都支持bfloat16，因为它是一个比较新的数据类型，在 NVIDIA GPU 上，只有 Ampere 架构以及之后的GPU 才支持，如何判断呢？很简单：
    import transformers
    transformers.utils.import_utils.is_torch_bf16_gpu_available()
    结果为True就是支持
    在自动混合精度策略（AMP）场景下判断设备是否支持 bfloat16、float16。
    import paddle
    paddle.amp.is_bfloat16_supported() # True or False
    paddle.amp.is_float16_supported() # True or False
    可以看到，虽然精度bfloat16比 float16 要差很多，但 bfloat16 数据类型拥有和 float32 相同的表示范围。
    很多情况下，更大的范围比精度重要很多（能够有效防止上下溢出）。
    transformer是层数多且有lipschitz常量大的组件的模型，在训练时梯度很容易超过fp16的表示范围，导致训练loss爆掉，而BF16表示范围跟fp32一致，训练模型非常稳定；

### FP32
    FP32也叫做 float32，两种叫法是完全一样的，全称是Single-precision floating-point(单精度浮点数)，在IEEE 754标准中是叫做binary32，简单来说是用32位二进制来表示的浮点数
    Sign(符号位): 1 位，0表示整数；1表示负数
    Exponent(指数位)：8位，表示整数部分，偏置值是 127
    Fraction(尾数位)：23位，表示小数部分

### FP8 
    浮点数会分为符号位（sign）, 指数位 （exponent）, 和小数位 （Mantissa）
    FP8根据指数位和小数位的不同，支持 E5M2(指数位5、小数位2) 和 E4M3(指数位4、小数位3)

### PTQ（Post-training quantization, 训练后量化）
    PTQ, 顾名思义，就是在模型训练后做的量化。又可细分为两种：
    需要calibration数据，这些数据主要用来统计得到量化参数，因此是不需要打标的，一般百来张即可。
    完全不需要数据集。这适用于那些完全拿不到训练环境和数据的场景。
    在LLM训练完成后对其参数进行量化，只需要少量校准数据，适用于追求高易用性和缺乏训练资源的场景。主要目标是减少LLM的存储和计算复杂性，
    而无需对LLM架构进行修改或进行重新训练。PTQ的主要优势在于其简单性和高效性。但PTQ可能会在量化过程中引入一定程度的精度损失。

### QAT（Quantization aware training, 量化感知训练）
    QAT，模型训练中开启量化。又可分是要从头训练还是fine-tuning。基本上到4位及以下量化由于信息丢失较多，因此很多方法中（也不绝对）需要训练介入。
    一般来说，QAT可以得到更高的准确率，但同时也会有更强的假设，就是有训练数据，训练环境和所需的成本。

### 量化感知微调（Quantization-Aware Fine-tuning，QAF）
    在微调过程中对LLM进行量化。主要目标是确保经过微调的LLM在量化为较低位宽后仍保持性能。
    通过将量化感知整合到微调中，以在模型压缩和保持性能之间取得平衡。

### GPTQ(Generative Pre-trained Transformers Quantization)
    GPTQ，一种基于近似二阶信息的新型一次性权重量化方法，既准确又高效。具体来说，GPTQ 可以在大约 4 个 GPU 小时内量化具有 1750 亿个参数的 
    GPT 模型，将位宽降低到每权重 3 或 4 位，相对于未压缩的基线，精度下降可以忽略不计。
    流行的一个 GPTQ 量化工具是AutoGPTQ，它可以量化任何 Transformer模型而不仅仅是Llama，
    现在 Huggingface 已经将 AutoGPTQ 集成到了 Transformers 中。

### GGML
    GGML与llama-cpp这个项目相关，它是基于 Llama 模型手撸的纯 C/C++ 版本，它最大的优势是可以在 CPU上快速地进行推理而不需要 GPU。
    然后作者将该项目中模型量化的部分提取出来做成了一个模型量化工具：GGML。
    一般模型名称中包含GGML字样，且扩展名为.bin，且整个模型只有一个文件。
    GGML大意就是说把所有向量分成每32个一组。然后找最大值除以 (2^7-1)，把这个数作为scale，令offset=0。
    这样做能尽可能整数化，并保留精度。
    GGUF是 GGML 团队增加的一个新功能，与 GGML 相比，GGUF 可以在模型中添加额外的信息，而原来的 GGML 模型是不可以的，
    同时 GGUF 被设计成可扩展，这样以后有新功能就可以添加到模型中，而不会破坏与旧模型的兼容性。
    除了llama.cpp能做cpu量化外，fastllm、chatglm.cpp也能做cpu量化。

### 监督微调(Supervised fine-tuning, SFT)
    Supervised fine-tuning (SFT)：使用标注人员标注的数据，有监督的微调预训练模型。
    有时候，SFT模型在验证集上1个epoch后就overfit了，但是继续更多的epoch(如：InstructGPT训了16个epoch)有利于RM score和人的偏好。
    在进行Supervised Fine-Tuning（SFT）之后，有时可能会观察到基座模型（如语言模型）的性能下降或产生一些“傻”的行为。这可能是由于以下原因：
    1、数据偏移：SFT过程中使用的微调数据集可能与基座模型在预训练阶段接触到的数据分布有所不同。如果微调数据集与预训练数据集之间存在显著的差异，模型可能会在新任务上表现较差。
    这种数据偏移可能导致模型在新任务上出现错误的预测或不准确的输出。
    2、非典型标注：微调数据集的标注可能存在错误或不准确的标签。这些错误的标签可能会对模型的性能产生负面影响，导致模型产生“傻”的行为。
    3、过拟合：如果微调数据集相对较小，或者模型的容量（参数数量）较大，模型可能会过拟合微调数据，导致在新的输入上表现不佳。
    过拟合可能导致模型过于依赖微调数据的特定样本，而无法泛化到更广泛的输入。
    4、缺乏多样性：微调数据集可能缺乏多样性，未能涵盖模型在新任务上可能遇到的各种输入情况。这可能导致模型在面对新的、与微调数据集不同的输入时出现困惑或错误的预测。
    为了解决这些问题，可以尝试以下方法：
    1、收集更多的训练数据，以增加数据的多样性和覆盖范围。
    2、仔细检查微调数据集的标注，确保标签的准确性和一致性。
    3、使用正则化技术（如权重衰减、dropout）来减少过拟合的风险。
    4、进行数据增强，通过对微调数据进行一些变换或扩充来增加多样性。
    使用更复杂的模型架构或调整模型的超参数，以提高模型的性能和泛化能力。
    通过这些方法，可以尽量减少Supervised Fine-Tuning之后模型出现“傻”的情况，并提高模型在新任务上的表现。

### 奖励模型(Reward modeling,RM) 
    把监督微调(SFT)模型最后的解嵌(unembedding)层去掉,即最后一层不用softmax,改成一个线性层,这样RM模型就可以做到输入问题+答案,输出一个标量的分数。
    首先通过在数据集中随机抽取问题，使用第一阶段生成的SFT模型，对每个问题生成多个不同的回答，然后再让标注人员对这些回答进行排序。
    对于标注人员来说，对输出进行排序比从头开始打标要容易得多，因此这一过程可以扩展数据量，大约产生3万3千个训练用的数据。
    接下来，再使用这个排序结果来训练奖励模型。对于多个排序结果，两两组合，形成多个训练数据对。RM 模型接受输入后，给出评价回答质量的分数。
    对于一对训练数据，通过调节参数使得高质量回答的打分比低质量的打分要高。奖励模型学会了为评分高的响应计算更高的奖励，为评分低的回答计算更低的奖励。

### PPO （Proximal Policy Optimization，近端策略优化）
    PPO是一种用于在强化学习中训练 agent 的策略，这里被用来微调 SFT 模型。
    这一阶段利用第二阶段训练好的奖励模型，靠奖励打分来更新预训练模型参数。
    在数据集中随机抽取问题后，使用 PPO 模型生成回答，并用上一阶段训练好的 RM 模型计算奖励，给出质量分数，然后用这个奖励来继续更新 PPO 模型。
    奖励依次传递，由此产生策略梯度，通过强化学习的方式更新 PPO 模型参数。
    不断重复第二和第三阶段，通过迭代，会训练出更高质量的 InstructGPT 模型。
    我们将来自于人类反馈的强化学习简称为 RLHF（reinforcement learning from human feedback）：使用人类的偏好作为奖励信号来微调模型。
    这也是 ChatGPT 在实际对话过程中的输出更符合人类偏好的原因。

### 强化学习（Reinforcement Learning, RL）
    强化学习（Reinforcement Learning, RL），又称再励学习、评价学习或增强学习，是机器学习的范式和方法论之一，用于描述和解决智能体（agent）在与环境的交互过程中通过学习策略以达成回报最大化或实现特定目标的问题。
    Agent：Agent 代表强化学习中的对环境产生反应的模块， 一般来说就是我们要训练的决策模型。
    Environment：对 Agent 的决策做出反馈的模块， 可以理解现实环境， 或者多现实环境的模拟。
    Action：Agent 做出的决策， 可以是离散的， 也可以是连续的。 比如，向左或者向右行使， 买入股票或者卖出股票等等。
    State：环境当前的状态全景， 每当 Agent 做出一个 Action 到 Environment ， 就可能会改变 State。 注意：State 可能获取到， 也可能获取不到。因为无法保证在每个任务中都能看到系统的全景状态。
    Observation：一个类似State 的概念， 但是是从 Agent 的角度来理解， 指的是 Agent “看到的” 当前系统的状态。
        不同于State， Observation并不要求包括系统的全景， 因此，Observation 的适用范围更广。 在强化学习中， 经常会把 Observation 当做 State 来用，因为有时候 State 不可得， 只能拉Observation 充数。
    Reward：Agent 从环境中获得的奖赏，可能是正数，也可能是负数。
    近年来,深度强化学习(Deep reinforcement learning, DRL)在诸多复杂序贯决策问题中取得巨大突破.
    由于融合了深度学习强大的表征能力和强化学习有效的策略搜索能力,深度强化学习已经成为实现人工智能颇有前景的学习范式.

### 提示学习(Prompt Learning)
    提示学习（Prompt Learning）, 能够通过在输入中添加一个提示词（Prompt），使得预训练模型的性能大幅提高。
    Prompt Learning 是指对输入文本信息按照特定模板进行处理，把任务重构成一个更能充分利用预训练语言模型处理的形式。
    随着预训练语言模型规模的增长，“预训练-微调”范式在下游自然语言处理任务上的表现越来越好，但与之相应地对训练数据量和计算存储资源的要求也越来越高。
    为了充分利用预训练语言模型学习到的知识，同时降低对数据和资源的依赖，提示学习（Prompt Learning）作为一种可能的新范式受到了越来越多的关注，在 FewCLUE、SuperGLUE 等榜单的小样本任务上取得了远优于传统微调范式的结果。
    提示学习的核心思想是将下游任务转化为预训练阶段的掩码预测（MLM）任务。
    实现思路包括通过模板（Template）定义的提示语句，将原有任务转化为预测掩码位置的词，以及通过标签词（Verbalizer）的定义，建立预测词与真实标签之间的映射关系。
    以情感分类任务为例，“预训练-微调”范式和“预训练-提示”范式（以 PET 为例）之间的区别:
    【微调学习】使用 [CLS] 来做分类，需要训练随机初始化的分类器，需要充分的训练数据来拟合。
    【提示学习】通过提示语句和标签词映射的定义，转化为 MLM 任务，无需训练新的参数，适用于小样本场景。

### Pattern-Exploiting Training(PET)
    Pattern-Exploiting Training(PET)的方法通过人工构建的模版与BERT的MLM模型结合,能够起到非常好的零样本、小样本乃至半监督学习效果。
    PET模型，一种半监督学习方法使用自然语言模式（natural language pattern）将输入的文本重构为完形填空模式。
    PET主要包括三个步骤：
    1、对于每个pattern，分别使用一个语言模型（PLM）在小规模的数据集上进行微调；
    2、集成所有的语言模型并为unlabeled数据集上进行标注，标注结果为soft-label（即每个label的概率分布）；
    3、使用带有soft-label的数据，使用标准的classifier进行分类。

### P-tuning
    设计一个自动的生成连续prompt的方法，来提升模型的效果，该方法称为P-tuning。P-tuning仅仅修改了模型的输入部分，采用了一种连续的表征来代替人工设计的离散prompt。
    P-tuning 引入可学习的连续型提示向量 prompt embeddings 参数, 让模型自己去学习最优的 prompt embedding, 而不再依赖人工去设置自然语言形式的提示（Prompt）信息。
    实验效果，和MP(MP代表手动设计的prompt, manual prompt)对比的话，P-tuning的效果有明显提升，和FT(fine-tuning)对比的话，效果也不错。
    P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks
    提出P-Tuning v2的原因：
    1. P-Tuning v1工作显示，Prompt tuning在normal-sized的预训练模型上效果一般；
    2. 现有的Prompt tuning(P-Tuning v1)方法在较难的文本序列问题上效果不好；
    P-Tuning v2具体介绍：
    在P-Tuning v1中，continuous prompt被插入到输入序列的embedding里，除了语言模型的第一层之外，其他层的prompt embddding都来自于上一层。这样的设计存在两个问题：
    1. 约束了要优化的参数量。由于模型的input text的长度是一定的，一般是512，那么prompt的长度就不能过于长。
    2. 当模型层数很深时，tuning时模型的稳定性难以保证；模型层数越深，在第一层输入的prompt对后面的影响是难以预估的，这会影响模型的稳定性。
    P-Tuning v2的改进在于，将只在第一层插入continuous prompt修改为在许多层都插入continuous prompt，层与层之间的continuous prompt是相互独立的。
    这样一来，在模型tuning时，可训练的参数就增多了，从0.01%增加到了0.1%-3%。
    在复杂任务（例如RTE, BoolQ，CB）上，可以看到PT(P-Tuning v1)的效果相比于FT要差的多。而P-Tuning v2的差别则没那么大，甚至更好。

### 思维链(Chain-of-Thought，CoT）)
    在写prompt的时候，不仅给出结果，还要一步一步地写结果是怎么推算出来的；

### BERT（Bidirectional Encoder Representation from Transformers)
    BERT 是“Bidirectional Encoder Representations from Transformers”的首字母缩写，整体是一个自编码语言模型（Autoencoder LM），并且其设计了两个任务来预训练该模型。
    第一个任务是采用 MaskLM 的方式来训练语言模型，通俗地说就是在输入一句话的时候，随机地选一些要预测的词，然后用一个特殊的符号[MASK]来代替它们，之后让模型根据所给的标签去学习这些地方该填的词。
    第二个任务在双向语言模型的基础上额外增加了一个句子级别的连续性预测任务，即预测输入 BERT 的两段文本是否为连续的文本，引入这个任务可以更好地让模型学到连续的文本片段之间的关系。
    BERT 相较于原来的 RNN、LSTM 可以做到并发执行，同时提取词在句子中的关系特征，并且能在多个不同层次提取关系特征，进而更全面反映句子语义。
    相较于 word2vec，其又能根据句子上下文获取词义，从而避免歧义出现。同时缺点也是显而易见的，模型参数太多，而且模型太大，少量数据训练时，容易过拟合。
    BERT 只使用了 Transformer 的 Encoder 模块，原论文中，作者分别用 12 层和 24 层 Transformer Encoder 组装了两套 BERT 模型，分别是：
    BERT-base: L=12,H=768,A=12, 参数量=110M; BERT-large: L=24,H=1024,A=16, 参数量=340M; 
    其中层的数量(即，Transformer Encoder 块的数量)为L，隐藏层的维度为H，自注意头的个数为A。
    「需要注意的是，与 Transformer 本身的 Encoder 端相比，BERT 的 Transformer Encoder 端输入的向量表示，多了 Segment Embeddings(句子向量，是第一个句子，还是第二个句子)。」
    BERT随机mask语料中15%的token，然后将masked token 位置输出的最终隐层向量送入softmax，来预测masked token。
    BERT在pretrain的时候 会对训练集进行MASK 操作, 其中mask的方法是:
    15%的原始数据被mask, 85% 没有被mask。对于被mask的15% 分3种处理方式: 其中80%真正替换mask，10%随机替换，10%不动。
    mask的时候15% 的有被替换的概率，其中80% 被真正替换。在这80%真正替换的里面有80%单个token被替换，20%的二元(bigram)或者三元tokens(trigram)被替换。
    80%真正替换、10%随机、10%保留的mask策略的好处，Transformer encoder就不知道会让其预测哪个单词，或者说不知道哪个单词会被随机单词给替换掉，
    那么它就不得不保持每个输入token的一个上下文的表征分布(a distributional contextual representation)。
    也就是说如果模型学习到了要预测的单词是什么，那么就会丢失对上下文信息的学习，而如果模型训练过程中无法学习到哪个单词会被预测，
    那么就必须通过学习上下文的信息来判断出需要预测的单词，这样的模型才具有对句子的特征表示能力。
    另外，由于随机替换相对句子中所有tokens的发生概率只有1.5%(即15%的10%)，所以并不会影响到模型的语言理解能力。
    在pre-train和fine-tuning的过程中出现不match，因为[mask]在微调的过程中不会出现。
    将部分词mask，这个主要作用是用被mask词前后的词来去猜测mask掉的词是什么，因为是人为mask掉的，所以计算机是知道mask词的正确值，所以也可以判断模型猜的词是否准确。
    与ELMo相比BERT，前者训练出的词级别(word-level)向量变成后者句子级别(sentence-level)的向量
    Bert是直接在输入端显示地通过引入Mask标记，在输入侧隐藏掉一部分单词，让这些单词在预测的时候不发挥作用，要求利用上下文中其它单词去预测某个被Mask掉的单词；
    引入Masked Language Model(MLM)预训练目标，能够获取上下文相关的双向特征表示；
    引入Next Sentence Prediction(NSP)预训练目标，擅长处理句子或段落的匹配任务；
    引入强大的特征抽取机制Transformer(多种机制并存)：
    Multi-Head self attention：多头机制类似于“多通道”特征抽取，self attention通过attention mask动态编码变长序列，解决长距离依赖（无位置偏差）、可并行计算；
    Feed-forward ：在位置维度计算非线性层级特征；
    Layer Norm & Residuals：加速训练，使“深度”网络更加健壮；
    引入大规模、高质量的文本数据；
    Bert 的预训练阶段使用的是不带标签的数据，貌似是无监督学习，其实可以看到通过 [MASK]，模型已经使用标签在做分类任务训练了，这有点类似词向量模型 CBOW，
    只不过它使用了更强的 Transformer 做语义特征提取器，从而考虑到更长的上下文信息，而不仅仅是只截取窗口长度的 token 使用浅层全连接神经网络做训练。
    另外 CBOW 经过训练后学习到的是每个 token 的静态词向量，是一个结果；而 Bert 学到的是“一种学习能力”，
    就是能根据 token 的上下文信息来学习出 token 的词向量以供下游任务使用，这种词向量是动态的，
    很好的解决了一词多义的问题（也就是同一个 token 在不同的上下文环境里具有不同的词向量 embedding）。

### BART
    BART模型——用来预训练seq-to-seq模型的降噪自动编码器（autoencoder）。
    BART的训练包含两步：
    1） 利用任意一种噪声函数分解文本
    2） 学习一个模型来重构回原来的文本

    GPT是一种Auto-Regressive(自回归)的语言模型。
    它也可以看作是Transformer model的Decoder部分，它的优化目标就是标准的语言模型目标：序列中所有token的联合概率。
    GPT采用的是自然序列中的从左到右（或者从右到左）的因式分解。

    BERT是一种Auto-Encoding(自编码)的语言模型。
    它也可以看作是Transformer model的Encoder部分，在输入端随机使用一种特殊的[MASK]token来替换序列中的token，这也可以看作是一种noise(噪声)，所以BERT也叫Masked Language Model。

    BART吸收了BERT的bidirectional encoder和GPT的left-to-right decoder各自的特点，建立在标准的seq2seq Transformer model的基础之上，这使得它比BERT更适合文本生成的场景；
    相比GPT，也多了双向上下文语境信息。BART就是一个BERT+GPT的结构，但是相对于BERT中单一的noise类型(只有简单地用[MASK] token进行替换这一种noise)，BART在encoder端尝试了多种noise。
    其原因和目的也很简单:BERT的这种简单替换导致的是encoder端的输入携带了有关序列结构的一些信息（比如序列的长度等信息），而这些信息在文本生成任务中一般是不会提供给模型的。
    BART采用更加多样的noise，意图是破坏掉这些有关序列结构的信息，防止模型去“依赖”这样的信息。

### ALBERT
    ALBERT也是采用和BERT一样的Transformer的encoder结果，激活函数使用的也是GELU。在ALBERT中主要有三个改进方向。
    为了说明这三个改进，先定义几个变量：词的embedding我们设置为E，encoder的层数我们设置为L，hidden size即encoder的输出值的维度我们设置为H，词典的大小为V。
    1、对Embedding因式分解（Factorized embedding parameterization）
    矩阵分解:在两个大维度之间加入一个小维度，从O(V*H)变为O(V*E+E*H)，其中H 远远大于 E，以达到降维的作用
    在BERT中，词embedding与encoder输出的embedding维度是一样的。但是ALBERT认为，词级别的embedding是没有上下文依赖的表述，而隐藏层的输出值不仅包括了词本生的意思还包括一些上下文信息，
    理论上来说隐藏层的表述包含的信息应该更多一些，因此应该让H>>E，所以ALBERT的词向量的维度是小于encoder输出值维度的。
    在NLP任务中，通常词典都会很大，embedding matrix的大小是E×V，如果和BERT一样让H=E，那么embedding matrix的参数量会很大，并且反向传播的过程中，更新的内容也比较稀疏。
    结合上述说的两个点，ALBERT采用了一种因式分解的方法来降低参数量。首先把one-hot向量映射到一个低维度的空间，大小为E，然后再映射到一个高维度的空间，
    说白了就是先经过一个维度很低的embedding matrix，然后再经过一个高维度matrix把维度变到隐藏层的空间内，从而把参数量从O(V×H)降低到了O(V×E+E×H)，当E<<H时参数量减少的很明显。
    在这里做了一个矩阵分解，将矩阵V x H(E)分解为两个小的矩阵V x E，E x H，E << H。在这里不再将E=H，而是将E设置为一个远小于H的值，然后再经过一个矩阵E x H将词向量维度映射到H。
    BERT的情况是，E=H；ALBERT的方案是，将E降低，在词嵌入和隐藏层之间加入一个project层，连接两个层。假设V=30000，H=1024，BERT参数量= E*V = H*V=30000*1024；
    ALBERT中，E=128；H=1024，其参数量 = (V +H)*E=30000*128+128*1024。  
    2、跨层的参数共享（Cross-layer parameter sharing）
    在ALBERT还提出了一种参数共享的方法，全连接层与attention层都进行参数共享，也就是说共享encoder内的所有参数，也就是ALBERT依然有多层的深度连接，但是各层之间的参数是一样的。
    很明显的，通过这种方式，ALBERT中隐藏层的参数量变为原来的1/12或者1/24。同样量级下的Transformer采用该方案后实际上效果是有下降的，但是参数量减少了很多，训练速度也提升了很多。
    ALBERT通过参数共享的方式降低了内存，但是因为层数没有变，在推断时的计算量并没有下降，预测阶段还是需要和BERT一样的时间。
    ALBERT解决的是训练时候的速度提升。隐藏层间参数共享能够极大的减少模型参数，对模型训练速度的提升也有一定的帮助。但是对推理预测速度却不会有任何帮助，因为前向传播时的计算量一点也没有减少。
    3、句间连贯（Inter-sentence coherence loss）
    BERT的NSP任务实际上是一个二分类，训练数据的正样本是通过采样同一个文档中的两个连续的句子，而负样本是通过采用两个不同的文档的句子。但是后续的研究发现该任务效果并不好，主要原因是因为其任务过于简单。
    NSP其实包含了两个子任务，主题预测与关系一致性预测，但是主题预测相比于关系一致性预测简单太多了。
    在ALBERT中，为了只保留一致性任务去除主题识别的影响，提出了一个新的任务 sentence-order prediction（SOP），SOP的正样本和NSP的获取方式是一样的，负样本把正样本的顺序反转即可。
    SOP因为实在同一个文档中选的，其只关注句子的顺序并没有主题方面的影响。并且SOP能解决NSP的任务，但是NSP并不能解决SOP的任务。
    在BERT中，句子间关系的任务是next sentence predict(NSP)，即向模型输入两个句子，预测第二个句子是不是第一个句子的下一句。
    在ALBERT中，句子间关系的任务是sentence-order prediction(SOP)，即句子间顺序预测，也就是给模型两个句子，让模型去预测两个句子的前后顺序。
    此外，ALBERT还有一个albert_tiny模型，其隐藏层仅有4层，模型参数量约为1.8M，非常的轻便。相对于BERT，其训练和推理预测速度提升约10倍，但精度基本保留。

### RoBERTa
    RoBERTa 主要是在 Bert 基础上做了几点调整：
    训练时间更长（100k->300k->500k steps），batch size 更大（由bert的256变为2k或8k），训练数据更多（在BERT16G语料变为了160G语料）。
    移除了 NSP 任务。
    训练序列更长。
    动态调整 Mask 机制，一开始把预训练的数据复制 10 份，每一份都随机选择 15% 的 Tokens 进行 Mask，也就是说，同样的一句话有 10 种不同的 Mask 方式。
    然后每份数据都训练 N/10 个 epoch。这就相当于在这 N 个 epoch 的训练中，每个序列的被 Mask 的 tokens 是会变化的。

### ERINIE
    融合了更多知识，mask的时候采用短语和实体级别的mask，添加了更多优质语料

### SimBERT
    SimBERT属于有监督训练，训练语料是自行收集到的相似句对，通过一句来预测另一句的相似句生成任务来构建Seq2Seq部分，
    [CLS]的向量事实上就代表着输入的句向量，所以可以同时用它来训练一个检索任务。
    假设SENT_a和SENT_b是一组相似句，那么在同一个batch中，把[CLS] SENT_a [SEP] SENT_b [SEP]和[CLS] SENT_b [SEP] SENT_a [SEP]都加入训练，做一个相似句的生成任务，这是Seq2Seq部分。
    
### SimCSE
    SimCSE去掉了SimBERT的生成部分，仅保留检索模型；
    由于SimCSE没有标签数据，所以把每个句子自身视为相似句传入。    
    本质上来说就是(自己,自己)作为正例、(自己,别人)作为负例来训练对比学习模型。
    当然这里会使用一些数据扩增手段，让正例的两个样本有所差异。
    SimCSE则提出了一个极为简单的方案：直接把Dropout当作数据扩增！

### Sentence-BERT
    有监督的方式主要是 Sentence-Bert (SBERT)，SBERT 通过 Bert 的孪生网络获得两个句子的向量，进行有监督学习
    训练阶段：A和B两个sentence分别进入同一个BERT模型得到各自的池化(poolout)向量u,v，通过计算 |u-v| 对u,v向量提取交叉特征，并和u,v向量concat到一起, 
    即(u,v, |u-v|) 进入fusion layers得到预测结果。其中|u−v|是指u−v的每个元素都取绝对值后构成的向量）拼接起来做为特征。
    这种在u,v后面加入fusion layers的方式会让预测结果更准，但也会让u,v向量与预测结果的关联性变弱。
    推理阶段：对于两个sentence A和B，分别进入训练好的BERT模块得到各自的sentence-embedding表示u和v，计算u和v向量的余弦相似度(cosine-sim)评分作为sentence A和B的预测概率。
    在部分有监督的实验结果SBERT表现一般，对于“有监督”方式的评估，SBERT参与训练的fusion layers并不参与推理，这会造成训练目标和评估目标之间存在一个很大差距，即训练目标是sentence-embedding向量，而评估目标是相似性，因此具体任务上SBERT表现不如官方BERT还是比较正常的。

##### 对于Bert输出的向量直接用于相似度计算的问题上，往往表现是较差的。
    语言模型的目标就是最大化token与上下文的共现概率，在该目标下， 上下文c 与 建模单词x 的表示会不断的拉近，如果同一个 建模单词x 存在于另一个上下文 c' 中，那么在训练中 上下文c 与 上下文c' 的表示也会不断拉近。
    通过上述分析我们可以得出Bert的预训练过程和语义相似度的计算目标是十分接近的，训练得到的句向量包含了文本间的语义相似度的信息，原则上是可以通过点积(cosine)来进行相似度计算的。
    实际操作时候效果往往不理想的原因呢？
    Bert模型的向量表示存在的问题
    1.Bert的词向量在空间中的分布并不均匀
      Bert的词向量在空间中的分布呈现锥形，高频的词都靠近原点。 (所有词的均值)，而低频词远离原点，这会导致即使一个高频词与一个低频词的语义等价，
      但是词频的差异也会带来巨大的差异，从而词向量的距离不能很好的表达词间的语义相关性。
    2.高频词分布紧凑，低频词的分布稀疏
      分布稀疏会导致区间的语义不完整(poorly defined)，低频词表示训练的不充分，而句向量仅仅是词向量的平均池化，所以计算出来的相似度存在问题。
      故而需要校正BERT出来的句向量的分布，从而使得计算出来的cos相似度更为合理一些。
    还有个原因：单词级相似度比较不适用于BERT embeddings，因为这些嵌入是上下文相关的，这意味着单词vector会根据它出现在的句子而变化。
    而通过SimCSE 或 Sentence-BERT 得到的句向量用于计算相似度，效果好于原始BERT输出的向量；

### WoBERT
    WoBERT 同样也是采用 BERT 的结构，但区别是在训练中加入的词，这是一个专门为生成式下游任务训练的模型。
    以字为粒度的生成方式有两个主要的弊端，第一是对于相同长度的句子，字粒度式的生成比词粒度式的生成需要更长的序列，这导致生成的时间过长，并且过长的生成序列还容易造成在解码阶段带来的累积误差。
    第二个问题是中文的词很多时候是固定搭配，拆为字来进行生成增加了模型生成的难度。
    基于以上的原因，在保留原本 BERT 词表中的字的基础上，新增了一定数量的词，并进行再训练。

### exhaustive search（穷举搜索）
    穷举所有可能的输出序列，3个时间步长，每个步长3种选择，共计 3^3=27 种排列组合。
    从所有的排列组合中找到输出条件概率最大的序列。穷举搜索能保证全局最优，但计算复杂度太高，当输出词典稍微大一点根本无法使用。
    
### greedy search（贪心搜索）
    贪心算法在翻译每个字的时候，直接选择条件概率最大的候选值作为当前最优。    
    贪心算法每一步选择中都采取在当前状态下最好或最优的选择，通过这种局部最优策略期望产生全局最优解。
    贪心算法本质上没有从整体最优上加以考虑，并不能保证最终的结果一定是全局最优的。但是相对穷举搜索，搜索效率大大提升。
    
### beam search（集束搜索）
    beam search是对greedy search的一个改进算法。相对greedy search扩大了搜索空间，但远远不及穷举搜索指数级的搜索空间，是二者的一个折中方案。
    beam search有一个超参数beam size（束宽），设为 k 。第一个时间步长，选取当前条件概率最大的 k 个词，当做候选输出序列的第一个词。
    之后的每个时间步长，基于上个步长的输出序列，挑选出所有组合中条件概率最大的 k 个，作为该时间步长下的候选输出序列。始终保持 k 个候选。最后从 k 个候选中挑出最优的。
    beam search不保证全局最优，但是比greedy search搜索空间更大，一般结果比greedy search要好。
    greedy search 可以看做是 beam size = 1时的 beam search。
    
### T5 模型
    Transfer Text-to-Text Transformer 的简写。
    使用了标准的Encoder-Decoder模型，并且构建了无监督/有监督的文本生成预训练任务。
    T5的预训练包含无监督和有监督两部分。无监督部分使用的是Google构建的近800G的语料（论文称之为C4），而训练目标则跟BERT类似，只不过改成了Seq2Seq版本。
    有监督部分，则是收集了常见的NLP监督任务数据，并也统一转化为SeqSeq任务来训练。
    希望用文字把我们要做的任务表达出来，然后都转化为文字的预测。

### 模型蒸馏
    越复杂的深度学习网络，其拟合效果越好，但伴随出现推理（预测）速度越慢的问题。
    模型蒸馏，其目的就是尽量不损失模型精度的前提下，大大的提升模型的推理速度。
    实现方法：
    第一步，训练好原本的复杂网络模型，如BERT，我们称为Teacher模型；
    第二步，用一个较为简单的模型去拟合Teacher模型（Student模型是去拟合Teacher模型推理的soft targets，因为soft targets包含的信息更多），称为Student模型；
    Student模型的Loss为：Loss=Crossentropy(s,t)，其中s表示Student模型推理的soft targets，t表示Teacher模型推理的soft targets
    但也有把hard targets也加进去Student的Loss中，即： Loss=Crossentropy(s,t)+Crossentropy(s,y)，其中y表示样本的真实标签（hard targets）。
    最后，利用训练好的Student模型进行推理预测。
    蒸馏训练
    第一种方法：
    在离线的情况下，将Teacher模型对所有样本的推理结果存入磁盘中，然后Student模型从磁盘中读取样本及Teacher模型推理的软标签，进行模型训练。
    第二种方法：
    将Teacher模型和Student模型同时加载到网络中，但将Teacher模型冻结，只进行前向传播，不进行反向传播更新参数；然后将前向传播的结果传递给Student模型的Loss中，训练Student模型。
    第三种方法：
    方法2存在这样的缺点：每一个batch，Student模型都需要等待Teacher模型推理结束才能进行反向传播，影响训练速度。
    那我们就可以将Teacher模型和Student模型分开部署，进行异步计算。Teacher模型只需要前向传播，而Student模型需要前向和反向传播，在处理时间上可能处于一个持平的状态。

### 蒸馏温度
    用于知识蒸馏的softmax公式，和常见的softmax公式比， 多了一个参数T，称为温度；
    随着T的增加，Softmax的输出分布越来越平缓，信息熵会越来越大；
    有时候学生模型loss分两部分，一部分学习soft target，另一部分学习hard target，二者通过一个权重参数λ控制比例，
    通常情况下soft target的loss比重大一些，即更偏向于学习soft target。
    如，alpha = 0.3, temp=7
    student_loss = student_loss_fn(student_preds, targets)
    ditillation_loss = divergence_loss_fn(
        F.softmax(student_preds / temp, dim=1),
        F.softmax(teacher_preds / temp, dim=1)
    )
    loss = alpha * student_loss + (1 - alpha) * ditillation_loss
    示例，没有蒸馏温度和蒸馏温度为8时的效果对比：
    F.softmax(torch.tensor([1.0, 0.8, 0.4, 1.2, 10., 0.]),dim=0)
    Out[185]: tensor([1.2335e-04, 1.0099e-04, 6.7696e-05, 1.5066e-04, 9.9951e-01, 4.5378e-05])
    F.softmax(torch.tensor([1.0, 0.8, 0.4, 1.2, 10., 0.])/8,dim=0)
    Out[186]: tensor([0.1267, 0.1236, 0.1176, 0.1299, 0.3903, 0.1118])

### soft targets（软标签）
    soft targets：属于不同标签的概率，一般是softmax的计算结果。例如，我们的标签是“男”和“女”两种，那么软标签的形式应该就是（0.4，0.6）。

### hard targets（硬标签）
    hard targets：属于哪一种标签。例如，该样本的标签是“男”，那么硬标签就是（1，0）。
        
### 优势特征蒸馏(Privileged Features Distillation)
    将区分度高、但只能离线获取的特征定义为优势特征(Privileged Features)。
    优势特征蒸馏(Privileged Features Distillation, 简称PFD)。在离线环境下，会同时训练两个模型：一个学生模型以及一个教师模型。
    其中学生模型和原始模型完全相同，而教师模型额外利用了优势特征， 其准确率也因此更高。通过将教师模型蒸馏出的知识(Knowledge, 如教师模型中最后一层的输出)传递给学生模型，
    可以辅助其训练并进一步提升准确率。在线上服务时，只抽取学生模型进行部署，因为输入不依赖于优势特征，离线、在线的一致性得以保证。
    在PFD中，所有的优势特征都被统一到教师模型作为输入，加入更多的优势特征往往能带来模型更高的准确度。

### BPE（Byte Pair Encoding，双字节编码）
    用于解决 集外词（OOV, out-of-vocabulary）和罕见词（Rare word）问题。
    BPE将单词作为单词片段处理（word pieces），以便于处理未出现单词。
    先将训练集单词划分成片段（利用BPE），然后将片段随机赋值后放到RNNs或CNNs中训练出片段的embedding，再将片段组合得出word的embedding后，进行工作。
    这样如果在训练集或者其他情况中，遇到生僻词或者未登录词时，直接利用片段进行组合来进行任务。

### 梯度弥散(梯度消失)（vanishing gradient problem）
    靠近输出层的hidden layer 梯度大，参数更新快，所以很快就会收敛；
    而靠近输入层的hidden layer 梯度小，参数更新慢，几乎就和初始状态一样，随机分布。
    这种现象就是梯度弥散（vanishing gradient problem）。
    梯度消失:随着深度增加,梯度急剧减小。梯度消失是指在反向传播过程中梯度逐渐降低到0导致参数不可学习的情况。

### 梯度爆炸(exploding gradient problem)
    前面靠近输入的层的梯度通过训练变大，而后面靠近输出层的层的梯度指数级增大，这种现象又叫做梯度爆炸(exploding gradient problem)。
    深度神经网络训练的时候，采用反向传导的方式，其背后的本质是链式求导，计算每层梯度的时候会涉及到一些连乘操作。
    每一层的残差都由后一层的残差乘以两层之间的权重矩阵，再乘以当前层的激活函数的导数得到。因此，神经网络权重的初始化至关重要，不当的初始化可能会带来梯度消失或者梯度爆炸。
    当网络过深，如果连乘的因子大部分小于1，最后乘积可能趋于0；另一方面，如果连乘的因子大部分大于1，最后乘积可能趋于无穷。这就是所谓的梯度消失与梯度爆炸。
    影响梯度更新的有，初始权重、激活函数、梯度流动方式、损失值过大等。
    （1）初始权重带来的影响：神经网络权重初始化不当；
    （2）激活函数带来的影响：激活函数选择不当；
    （3）梯度流动方式带来的影响：网络结构本身的问题，如RNN；
    （4）损失值过大带来的影响：数据集的问题，如标注不准等。

### 神经元失活
    失去活性,即权重变为0(不与其他神经元产生连接)。 随机失活的好处:避免网络过度地依赖某一个神经元从而实现减轻过拟合现象。 
    神经元输出恒为0，导致失活，由于此时梯度为0，无法恢复。

### 激活函数（Activation Function）
    激活函数（Activation Function），就是在人工神经网络的神经元上运行的函数，负责将神经元的输入映射到输出端。
    在神经元中，输入的 inputs 通过加权，求和后，还被作用了一个函数，这个函数就是激活函数。
    引入激活函数是为了增加神经网络模型的非线性。没有激活函数的每层都相当于矩阵相乘。就算你叠加了若干层之后，无非还是个矩阵相乘罢了。 
    如果不用激活函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合，这种情况就是最原始的感知机（Perceptron）。
    如果使用的话，激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中。
    
### Sigmoid函数
    Sigmoid函数是一个在生物学中常见的S型函数，也称为S型生长曲线。
    在信息科学中，由于其单增以及反函数单增等性质，Sigmoid函数常被用作神经网络的阈值函数，将变量映射到0,1之间。
    sigmoid 函数是一个 logistic 函数，意思就是说：不管输入是什么，得到的输出都在 0 到 1 之间。
    也就是说，你输入的每个神经元、节点或激活都会被缩放为一个介于 0 到 1 之间的值。
    sigmoid 函数输入一个很大的 x 值（正或负）时，我们得到几乎为 0 的 y 值
    在反向传播时，当 x 是一个很大的值（正或负）时，我们本质上就是用一个几乎为 0 的值来乘这个偏导数的其余部分。
    如果有太多的权重都有这样很大的值，那么我们根本就没法得到可以调整权重的网络，这可是个大问题。
    如果我们不调整这些权重，那么网络就只有细微的更新，这样算法就不能随时间给网络带来多少改善。
    对于针对一个权重的偏导数的每个计算，我们都将其放入一个梯度向量中，而且我们将使用这个梯度向量来更新神经网络。
    可以想象，如果该梯度向量的所有值都接近 0，那么我们根本就无法真正更新任何东西。
    总结起来说就是梯度消失问题使得 sigmoid 函数在神经网络中并不实用
    公式: f(x) = 1/(1+e^-x)

### Tanh函数
    Tanh是双曲函数中的一个，Tanh()为双曲正切。在数学中，双曲正切“Tanh”是由基本双曲函数双曲正弦和双曲余弦推导而来。
    f(x) = (e^x-e^-x)/(e^x+e^-x)

### Relu激活函数（The Rectified Linear Unit，整流线性单元）
    用于隐层神经元输出。x 值小于零的一切都映射为 0 的 y 值，但 x 值大于零的一切都映射为它本身。
    当我们将 ReLU 函数引入神经网络时，我们也引入了很大的稀疏性。
    稀疏：数量少，通常分散在很大的区域。在神经网络中，这意味着激活的矩阵含有许多 0。
    神经网络是稀疏的，能提升时间和空间复杂度方面的效率——常数值（通常）所需空间更少，计算成本也更低。
    如果在计算梯度时有太多值都低于 0 ，会得到相当多不会更新的权重和偏置，因为其更新的量为 0，即死亡 ReLU 问题。
    优点：
    相比于 sigmoid，由于稀疏性，时间和空间复杂度更低；不涉及成本更高的指数运算；
    能避免梯度消失问题。
    缺点：
    引入了死亡 ReLU 问题，即网络的大部分分量都永远不会更新。但这有时候也是一个优势；
    ReLU 不能避免梯度爆炸问题。
    公式如下
    f(x) = max(0, x)

### 指数线性单元（ELU）
    如果你输入的 x 值大于 0，则结果与 ReLU 一样——即 y 值等于 x 值；但如果输入的 x 值小于 0，则我们会得到一个稍微小于 0 的值: α(e^x-1)。
    所得到的 y 值取决于输入的 x 值，但还要兼顾参数 α——你可以根据需要来调整这个参数（常见的α值是在 0.1 到 0.3 之间）。
    优点：
    能避免死亡 ReLU 问题；
    能得到负值输出，这能帮助网络向正确的方向推动权重和偏置变化；
    在计算梯度时能得到激活，而不是让它们等于 0。
    缺点：
    由于包含指数运算，所以计算时间更长；
    无法避免梯度爆炸问题；
    神经网络不学习 α 值。
    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.elu, name="hidden1")

### 渗漏型整流线性单元激活函数（Leaky ReLU）
    渗漏型整流线性单元激活函数也有一个 α 值，通常取值在 0.1 到 0.3 之间。
    如果输入 x 大于 0，则输出为 x；如果输入 x 小于或等于 0，则输出为 α 乘以输入。
    优点：
    类似 ELU，Leaky ReLU 也能避免死亡 ReLU 问题，因为其在计算导数时允许较小的梯度；
    由于不包含指数运算，所以计算速度比 ELU 快。
    缺点：
    无法避免梯度爆炸问题；
    神经网络不学习 α 值；
    在微分时，两部分都是线性的；而 ELU 的一部分是线性的，一部分是非线性的。
    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.leaky_relu, name="hidden1")

### 扩展型指数线性单元激活函数（SELU）
    当实际应用这个激活函数时，必须使用 lecun_normal 进行权重初始化。如果希望应用 dropout，则应当使用 AlphaDropout。
    公式的两个值：α 和 λ 是预先确定的.
    如果输入值 x 大于 0，则输出值为 x 乘以 λ；如果输入值 x 小于 0，则会得到一个奇异函数——它随 x 增大而增大并趋近于 x 为 0 时的值 0.0848。
    本质上看，当 x 小于 0 时，先用 α 乘以 x 值的指数，再减去 α，然后乘以 λ 值。
    SELU 激活能够对神经网络进行自归一化（self-normalizing）
    在初始化函数为 lecun_normal 的假设下，网络参数会被初始化一个正态分布（或高斯分布），然后在 SELU 的情况下，网络会在论文中描述的范围内完全地归一化。
    本质上看，当乘或加这样的网络分量时，网络仍被视为符合高斯分布。我们就称之为归一化。
    反过来，这又意味着整个网络及其最后一层的输出也是归一化的。
    SELU 的输出是归一化的，这可称为内部归一化（internal normalization），因此事实上其所有输出都是均值为 0 且标准差为 1。
    当输入小于 0 时，方差减小；当输入大于 0 时，方差增大——而标准差是方差的平方根，这样我们就使得标准差为 1。
    优点：
    内部归一化的速度比外部归一化快，这意味着网络能更快收敛；
    不可能出现梯度消失或爆炸问题，见 SELU 论文附录的定理 2 和 3。

### gelu激活函数（Gaussian error linear units，高斯误差线性单元）
    高斯误差线性单元激活函数在最近的 Transformer 模型（谷歌的 BERT 和 OpenAI 的 GPT-2）中得到了应用。
    看得出来，这就是某些函数（比如双曲正切函数 tanh）与近似数值的组合。
    当 x 大于 0 时，输出为 x；但 x=0 到 x=1 的区间除外，这时曲线更偏向于 y 轴。
    优点：
    似乎是 NLP 领域的当前最佳；尤其在 Transformer 模型中表现最好；
    能避免梯度消失问题。

### 归一化（normalization）
    归一化首先是减去均值，然后除以标准差。
    因此，经过归一化之后，网络的组件（权重、偏置和激活）的均值为 0，标准差为 1。

### Layer Normalization(层归一化)
    Layer Normalization是一个通用的技术，其本质是规范优化空间，加速收敛。
    为了保证数据特征分布的稳定性，加入Layer Normalization，这样可以加速模型的优化速度。

### 过拟合(overfit)与欠拟合(underfit)
    训练开始时,优化和泛化是相关的:训练数据上的损失越小,测试数据上的损失也越小。
    这时的模型是欠拟合(underfit)的,即仍有改进的空间,网络还没有对训练数据中所有相关模式建模。
    但在训练数据上迭代一定次数之后,泛化不再提高,验证指标先是不变,然后开始变差,即模型开始过拟合。
    这时模型开始学习仅和训练数据有关的模式,但这种模式对新数据来说是错误的或无关紧要的。
    过拟合的问题表现：高方差(high variance);
    欠拟合的问题表现：高偏差(high bias);
    导致过拟合的原因：
    1、训练数据太少，样本单一。如果训练样本只有负样本，然后拿生成的模型去预测正样本，这肯定预测不准。所以训练样本要尽可能的全面，覆盖所有的数据类型；
    2、存在噪声。噪声指训练数据中的干扰数据。过多的干扰会导致记录了很多噪声特征，忽略了真实输入和输出之间的关系；
    3、模型过于复杂。模型太复杂，已经能够死记硬背记录下了训练数据的信息，但是遇到没有见过的数据的时候不能够变通，泛化能力太差。我们希望模型对不同的数据都有稳定的输出。模型太复杂是过拟合的重要因素。
    导致欠拟合的原因：
    1、模型过于简单，模型没有很好的捕捉数据特征，不能很好地拟合数据；
    2、特征项不够，没有足够的信息支持模型做判断。
    防止神经网络过拟合的常用方法包括:
    1、获取更多的训练数据
    2、减小网络容量,降低模型复杂度。
    3、添加权重正则化
    4、添加 dropout
    5、集成学习方法bagging(如随机森林）能有效防止过拟合；
    6、交叉检验，通过交叉检验得到较优的模型参数；早停策略本质上是交叉验证策略，选择合适的训练次数，避免训练的网络过度拟合训练数据；
    欠拟合的解决方法：
    1、做特征工程，添加更多的特征项，比如特征组合、高次特征，来增大假设空间；
    2、集成学习方法boosting（如GBDT）能有效解决high bias；
    3、增加模型复杂度。如果模型太简单，不能够应对复杂的任务。可以使用更复杂的模型；
    4、减小正则化系数。

# 高方差(high variance)与高偏差(high bias)
    当训练集和测试集的误差之间有大的差距时，为高方差;
    当方差很高，训练集和验证集的准确率相差太多，应该是过拟合;
    表现为训练效果好,但是测试效果差，即模型的泛化能力差。可以通过观察模型在训练集和测试集上的损失函数值随着epoch的变化，如果是过拟合，模型在测试集上的损失函数值一般是先下降后上升。
    当训练集和测试集的误差收敛但却很高时，为高偏差;
    当偏差很高，训练集和验证集的准确率都很低，很可能是欠拟合；
    表现为不能很好的拟合数据，训练集和测试集效果都不佳。

### 权重正则化(weight regularization)
    为了防止模型从训练数据中学到错误或无关紧要的模式,最优解决方法是获取更多的训练数据。
    模型的训练数据越多,泛化能力自然也越好。如果无法获取更多数据,次优解决方法是调节模型允许存储的信息量,或对模型允许存储的信息加以约束。
    如果一个网络只能记住几个模式,那么优化过程会迫使模型集中学习最重要的模式,这样更可能得到良好的泛化。这种降低过拟合的方法叫作正则化(regularization)。
    奥卡姆剃刀(Occam’s razor)原理:如果一件事情有两种解释,那么最可能正确的解释就是最简单的那个,即假设更少的那个。
    这个原理也适用于神经网络学到的模型:给定一些训练数据和一种网络架构,很多组权重值(即很多模型)都可以解释这些数据。
    简单模型比复杂模型更不容易过拟合。    
    这里的简单模型(simple model)是指参数值分布的熵更小的模型(或参数更少的模型)。
    因此,一种常见的降低过拟合的方法就是强制让模型权重只能取较小的值,从而限制模型的复杂度,这使得权重值的分布更加规则(regular)。
    这种方法叫作权重正则化(weight regularization),其实现方法是向网络损失函数中添加与较大权重值相关的成本(cost)。
    这个成本有两种形式。
    * L1 正则化(L1 regularization):
    添加的成本与权重系数的绝对值[权重的 L1 范数(norm)]成正比。
    * L2 正则化(L2 regularization):
    添加的成本与权重系数的平方(权重的 L2 范数)成正比。
    神经网络的 L2 正则化也叫权重衰减(weight decay)。
    不要被不同的名称搞混,权重衰减与 L2 正则化在数学上是完全相同的。

### dropout 正则化
    dropout 是神经网络最有效也最常用的正则化方法之一。对某一层使用 dropout,就是在训练过程中随机将该层的一些输出特征舍弃(设置为 0)。
    假设在训练过程中,某一层对给定输入样本的返回值应该是向量 [0.2, 0.5,1.3, 0.8, 1.1] 。
    使用 dropout 后,这个向量会有几个随机的元素变成 0,比如 [0, 0.5,1.3, 0, 1.1] 。
    dropout 比率(dropout rate)是被设为 0 的特征所占的比例,通常在 0.2~0.5范围内。
    测试时没有单元被舍弃,而该层的输出值需要按 dropout 比率缩小,因为这时比训练时有更多的单元被激活,需要加以平衡。
    Dropout是一个共享隐藏单元的Bagging集成模型；但Bagging的情况下，所有模型是独立的。
    在Dropout的情况下，模型参数共享的，其中每个模型集成的父神经网络参数的不同子集(网络中的每个单元乘以相应的掩码，然后正常地继续通过网络的其余部分向前传)。
    参数共享使得在有限可用的内存下，训练和评估指数级数量的神经网络成为可能。
    1.tf.nn.dropout(x=x,rate=0.2)是底层API，功能很简单，没有其他额外的附加功能。因此，dropout就真的是在dropout，一个单一的函数。
    2.tf.keras.layers.Dropout(0.2)(x)是个高层API，功能完善，他会区分当前状态是fit还是predict，如果是fit，他会把is_training = True，dropout生效；
    当遇到是predict、evalue时，is_training = False，dropout不生效。
    tf.keras.layers.Dropout会在训练fit时才会起作用，随机设定输入的值x的某一维=0，这个概率为输入的百分数 ，训练fit才有效。在模型预测(predict)时，不生效，所有神经元均保留也就是不进行dropout。

### 随机深度正则化(Stochastic depth for regularization)
    通过增加卷积网络的深度能显著降低预测误差，提高网络的表达能力，然而随着网络层的加深，也会带来一些负面影响，比如梯度消失，前向传播耗时增加、训练缓慢、模型过拟合训练数据等等。
    为了解决这些问题，我们提出了随机深度的方法，一个看似矛盾的设置，在训练过程降低网络的深度，在测试阶段保持网络的深度。
    通过该方法，大大减少了训练时间，并在评估的一些数据集上显著改善了测试误差。

### zoneout
    zoneout是rnn 时间维度上的“dropout”，要么维持前一个时刻的hidden vector，要么按照一般的样子更新。
    不是指单独的cell，而是指训练时的一种操作。
    Dropout就是通用的一种深度学习技巧，训练时随机失活一些神经元，可以增强模型泛化抑制过拟合作用。
    zoneout是指随机失活一个rnncell，跳过一步。

### 优化(optimization)
    优化(optimization)是指调节模型以在训练数据上得到最佳性能(即机器学习中的学习)
    
### 泛化(generalization)
    泛化(generalization)是指训练好的模型在前所未见的数据上的性能好坏。机器学习的目的当然是得到良好的泛化,但你无法控制泛化,只能基于训练数据调节模型。
    指的是你的模型能够适当地适应新的、以前未见过的数据，这些数据来自于用于创建模型的相同分布。这与过拟合的概念密切相关。如果你的模型过拟合，那么它就不能很好地泛化。
    一个好的模型捕获了训练数据的基本部分，而忽略了不重要的部分。这创建了泛化的性能，但是任何模型都有一定程度的局限性。
    泛化(Generalisation)可以理解为一种迁移学习的能力,大致可以理解为把从过去的经验中学习到的表示、知识和策略应用到新的领域,是大模型最被需要的能力。
    在上下文学习 (in-context learning, 简写为 ICL) 中，泛化分两种情况。一个是 in-domain（域内） 的，即泛化的时候测试数据的分布和训练数据一样，注意这个情况里面测试任务不必和训练任务一样，即这里已经考虑了对未见任务 (unseen task) 的泛化。
    另一个是 out-of-domain（域外） 的，即测试、训练数据分布不一样。

### 数据漂移(Data Drift)
    数据漂移(Data Drift)，指的是将进入数据仓库的数据，偏离标准、正常或预期的情况。它三种表现形式:
    结构漂移(Structural Drift) ，指的是数据源的数据结构发生了改变，比如字段增减、字段数据类型变更等。
    语义漂移(Semantic Drift) ，源于(历史)数据的改变，比如过了一段时间后，才发现已入仓库的历史数据有错误等。
    基础设施漂移(Infrastructure Drift) ，源于系统依赖的软件或平台发生变化后，与变化前不兼容。

### 概念漂移(Concept Drift)
    模型要预测的目标变量 随着时间的推移发生改变的现象。
    模型试图预测的目标变量的统计特性随着时间以不可预见的方式发生变化。这导致了预测的准确性会降低，
    在Online Learning的训练过程中，会存在全新的数据（特征空间不同或特征分布不同）进入模型，此时预测结果就会存在较大的偏差。这种现象称为概念漂移（Concept Drifting）。
    真实数据如果在特征空间上出现了较大的变化，那么原先模型的预测效果就会大打折扣。当模型学到的模式不再成立时，就会发生概念漂移。
    按照概念漂移速度，可以大致分为以下情况：
    Sudden： 即相当于突然发生了疫情，那么原先对于市场的预测模型会被瞬间击穿，不具有预测价值。也是我们在思考数据或其他现象时经常会提疫情后时代这个概念。sudden指的是迅速同时又不可逆的改变，强调的是发生的迅速。
    incremental： 和gradual都是强调改变发生的缓慢，incremental强调值的随时间改变，gradual则是数据分布的改变。也有些研究者将这两种变化划分为同一类，用incremental gradual这个术语来代替。
    recurring: 是一种temporary（临时性）的改变，在一段短时间内会恢复之前的状态。所以也有些研究者将其称为local drift，它不具有周期性，是在不规则的时间间隔内反复转换。
    blip: 是代表一种很稀少的事件，它可以被视为一种anomaly或者outlier（异常）。
    noise: 是一种随机的改变，通常这种数据会从样本数据中filter out。

### 模型退化
    随着时间的推移，模型预测的准确性会降低，称之为模型退化。模型退化的时间是不确定的。
    假设机器学习解决方案一旦投入生产，无需维护就能完美运行，这是一个错误的假设。当你把一个模型投入生产，它就开始退化。
    大多数模型只能捕获反映它们所看到的训练数据的模式。一个好的模型捕获了这些数据的基本部分，而忽略了不重要的部分。这创建了泛化的性能，但是任何模型都有一定程度的局限性。
    我们对数据的解释随时间而变化(概念漂移)，而数据的一般分布则没有变化。这导致最终用户将模型预测解释为随着时间的推移，对相同/相似数据的预测已经恶化。
    数据和概念都可能同时漂移，致使模型退化更严重。

### XLNet    
    通过Attention Mask机制，在Transformer内部随机Mask掉一部分单词（这个被Mask掉的单词比例跟当前单词在句子中的位置有关系，位置越靠前，被Mask掉的比例越高，位置越靠后，被Mask掉的比例越低），
    让这些被Mask掉的单词在预测某个单词的时候不发生作用。
    使用自回归语言模型，为解决双向上下文的问题，引入了排列语言模型；
    使用transformer-xl代替了transformer，能获取更长距离的依赖信息，即，融合Transformer-XL的优点处理过长文本；

### 词向量(word embedding)
    最简单的word embedding是把词进行基于词袋（BOW）的One-Hot表示。这种表示方法学习不到单词之间的关系（位置、语义），并且如果文档中有很多词，词向量可能会很长。
    另外一种方法就是通过word2vec训练词汇，将词汇用向量表示。该模型涉及两种算法：CBOW和Skip-Gram。这种方法可以将我们语义上相似的词用相似的向量表示，但是有个缺点，同一个词只有一种语义。
    使用语言模型预训练（如ELMo，GPT和 BERT其实可以看成是一个句子级别的上下文的词表示，它可以充分利用大规模的单语语料，并且可以对一词多义进行建模。

### 位置向量(Position Embedding)
    将每个位置编号，然后每个编号对应一个向量，通过结合位置向量和词向量，就给每个词都引入了一定的位置信息，这样就可以分辨出不同位置的词了。
    在RNN、CNN模型中其实都出现过Position Embedding，但在那些模型中，Position Embedding是锦上添花的辅助手段，也就是“有它会更好、没它也就差一点点”的情况，
    因为RNN、CNN本身就能捕捉到位置信息。但是在这个纯Attention模型中，Position Embedding是位置信息的唯一来源，因此它是模型的核心成分之一，并非仅仅是简单的辅助手段。
    Positional Embedding的成分直接叠加于Embedding之上，使得每个token的位置信息和它的语义信息(embedding)充分融合，并被传递到后续所有经过复杂变换的序列表达中去。
    论文中使用的Positional Encoding(PE)是正余弦函数，位置(pos)越小，波长越长，每一个位置对应的PE都是唯一的。
    之所以选用正余弦函数作为PE，是因为这可以使得模型学习到token之间的相对位置关系

### 注意力机制(attention机制)
    AttentionModel(注意力模型，AM)
    给定一组向量集合values，以及一个向量query，attention机制是一种根据该query计算values的加权求和的机制。
    attention的重点就是这个集合values中的每个value的“权值”的计算方法。
    有时候也把这种attention的机制叫做query的输出关注了原文的不同部分。
    具体过程：目标字及其上下文的字都有各自的原始Value，Attention机制将目标字作为Query、其上下文的各个字作为Key，
        并将Query与各个Key的相似性作为权重，把上下文各个字的Value融入目标字的原始Value中。
        Attention机制将目标字和上下文各个字的语义向量表示作为输入，首先通过线性变换获得目标字的Query向量表示、
        上下文各个字的Key向量表示以及目标字与上下文各个字的原始Value表示，然后计算Query向量与各个Key向量的相似度作为权重，
        加权融合目标字的Value向量和各个上下文字的Value向量，作为Attention的输出，即：目标字的增强语义向量表示。
    一个序列每个字符对其上下文字符的影响作用都不同，每个字对序列的语义信息贡献也不同，
    可以通过一种机制将原输入序列中字符向量通过加权融合序列中所有字符的语义向量信息来产生新的向量，即增强了原语义信息。
    注意力机制都可以一个三元组去描述，这个三元组是（Query，Key，Value）。
    假设我们有Q，K，V三个向量，每个输入单位（比如一个词）都具有对应的三元组的值，具体做法是将embedding后的词表示分别与Q，K，V相乘，得到最终的Q，K，V表示。
    query 和 key的维度必须保持一致；value 和query/key的维度可以不一致；key 和 value的长度必须保持一致；key 和value 本质上同一个句子在不同空间的表达；
    attention得到的输出，维度和value的维度一致，长度和 query的长度一致；Output(M*Dv) = Query(M*Dqk)* Key(Dqk*N)* Value(N*Dv)
    Output每个位置t是由Value的所有位置的vector加权平均之后的向量；而其权值是由位置为t的Query和Key的所有位置经过Attentiton计算得到的，权值的个数等于Key/Value的长度。
    而每一层线性映射参数矩阵都是独立的，所以经过映射后的Q, K, V各不相同，模型参数优化的目标在于将q, k, v被映射到新的高维空间，
    使得每层的Q, K, V在不同抽象层面上捕获到q, k, v之间的关系。
    一般来说，底层layer捕获到的更多是lexical-level(词汇层面)的关系，而高层layer捕获到的更多是semantic-level(语义层面)的关系。

### 自注意力机制(self attention机制)
    self attention其实是对attention进行了一个更广泛的定义罢了，
    比如很多时候我们是把k和v都当成一样的算来，做self的时候还可能是quey=key=value。
    Self-Attention:
        对于输入文本，我们需要对其中的每个字分别增强语义向量表示，因此，我们分别将每个字作为Query，加权融合文本中所有字的语义信息，
        得到各个字的增强语义向量。
        在这种情况下，Query、Key和Value的向量表示均来自于同一输入文本，即 Q = K = V(后面会经过变化变的不一样)， 
        同时对attention权重做了缩放，除去了维度值。
        因此，该Attention机制也叫Self-Attention。
    引入 Self-Attention 后会更容易捕获句子中长距离相互依赖特征，因为 Self-Attention 在计算过程中直接将句子任意两个单词的联系起来，
    此外，由于不依赖时间序列这一特性，Self-Attention 增加了计算的并行性。

### 多头注意力机制(Multi-head Attention)
    为了增强Attention的多样性，进一步利用不同的Self-Attention模块获得文本中每个字在不同语义空间下的增强语义向量，
    并将每个字的多个增强语义向量进行线性组合，从而获得一个最终的与原始字向量长度相同的增强语义向量
    “多头”实际上是指在初始化Q，K，V时，使用多组初始化值。
    Transformer使用了8组，每组都是随机初始化的，在经过训练后，我们就将得到8个获取了不同权重的结果表示。
    Multi-Head Attention的结果会被输入到前馈网络层中。为了避免同时输入8个结果，我们只需要再初始化一个矩阵W，
    和8个连接起来的attention结果做乘法，最终变换成前馈网络层可以接收的大小。
    在不改变参数量的情况下增强每一层attention的表现力。
    Multi-head Attention的本质是，在 「参数总量保持不变」 的情况下，
    将同样的query, key, value映射到原来的高维空间的「不同子空间」中进行attention的计算，在最后一步再合并不同子空间中的attention信息。
    这样降低了计算每个head的attention时每个向量的维度，在某种意义上防止了过拟合；由于Attention在不同子空间中有不同的分布，
    Multi- head Attention实际上是寻找了序列之间不同角度的关联关系，并在最后通过与权值矩阵相乘得以合并，将不同子空间中捕获到的关联关系综合起来。
    qi 和 kj 之间的attention score从1个变成了h个，这就对应了h个子空间中它们的关联度。
    Transformer 或 Bert 的特定层是有独特功能的，底层更偏向于关注语法，顶层更偏向于关注语义。
    同一层 Transformer 关注的方面是相同的，那么对于该方面而言，不同头的关注点应该也是一样的。
    但是在Multi-head Attention的同一层中，总有那么一两个头独一无二，和其它头关注的 Token 不同。
    利用多头机制，明显学会了不同的任务下采取不一样的注意力。
    Multi-head Attention的本质是，在 「参数总量保持不变」 的情况下，将同样的query, key, value映射到原来的高维空间的「不同子空间」中进行attention的计算，在最后一步再合并不同子空间中的attention信息。
    这样降低了计算每个head的attention时每个向量的维度，在某种意义上防止了过拟合；
    由于Attention在不同子空间中有不同的分布，Multi- head Attention实际上是寻找了序列之间不同角度的关联关系，并在最后concat这一步骤中，将不同子空间中捕获到的关联关系再综合起来。

### SE注意力机制（Squeeze-and-Excitation Networks, SENet）
    SE注意力机制（Squeeze-and-Excitation Networks）在通道维度增加注意力机制，关键操作是squeeze和excitation。
    通过自动学习的方式，即使用另外一个新的神经网络，获取到特征图的每个通道的重要程度，然后用这个重要程度去给每个特征赋予一个权重值，从而让神经网络重点关注某些特征通道。
    提升对当前任务有用的特征图的通道，并抑制对当前任务用处不大的特征通道。
    一个SEblock的过程分为 Squeeze(压缩) 和 Excitation(激发) 两个步骤：
    Squeeze(压缩) 通过在Feature Map层上执行Global Average Pooling，得到当前Feature Map的全局压缩特征量；
    Excitation(激发) 通过两层全连接的bottleneck结构得到Feature Map中每个通道的权值，并将加权后的Feature Map作为下一层网络的输入。
    实现过程：
    （1）Squeeze（Fsq）：通过全局平均池化，将每个通道的二维特征（H*W）压缩为1个实数，将特征图从 [h, w, c] ==> [1,1,c]
    （2）excitation（Fex）：给每个特征通道生成一个权重值，论文中通过两个全连接层构建通道间的相关性，输出的权重值数目和输入特征图的通道数相同。[1,1,c] ==> [1,1,c]
    （3）Scale（Fscale）：将前面得到的归一化权重加权到每个通道的特征上。论文中使用的是乘法，逐通道乘以权重系数。[h,w,c]*[1,1,c] ==> [h,w,c]
    squeeze操作：特征图经过全局平均池化，将特征图压缩成特征向量[1,1,c]
    excitation操作：FC1层+Swish激活+FC2层+Sigmoid激活。通过全连接层（FC1），将特征图向量的通道维度降低为原来的1/r，即[1,1,c*1/r]；
    然后经过Swish激活函数；再通过一个全连接层（FC2），将特征图向量的特征图上升回原来[1,1,c]；然后经过sigmoid函数转化为一个0-1之间的归一化权重向量。
    scale操作：将归一化权重和原输入特征图逐通道相乘，生成加权后的特征图。
    def se_block(input_feature, ratio=8):
        channel_axis = 1 if K.image_data_format() == "channels_first" else -1
        channel = input_feature._keras_shape[channel_axis]
        se_feature = GlobalAveragePooling2D()(input_feature)
        se_feature = Reshape((1, 1, channel))(se_feature)  # 第一步：压缩(Squeeze), reshape成1✖️1✖️C
        # assert se_feature._keras_shape[1:] == (1,1,channel)
        # 第二步：激励(Excitation),
        # 由两个全连接层组成，其中SERatio是一个缩放参数，这个参数的目的是为了减少通道个数从而降低计算量。
        # 第一个全连接层有(C/radio)个神经元，输入为1×1×C，输出1×1×(C/radio)。
        # 第二个全连接层有C个神经元，输入为1×1×(C/radio)，输出为1×1×C。
        se_feature = Dense(channel // ratio,
                           activation='relu',
                           kernel_initializer='he_normal',
                           use_bias=True,
                           bias_initializer='zeros')(se_feature)
        #assert se_feature._keras_shape[1:] == (1, 1, channel // ratio)
        se_feature = Dense(channel,
                           activation='sigmoid',
                           kernel_initializer='he_normal',
                           use_bias=True,
                           bias_initializer='zeros')(se_feature)
        if K.image_data_format() == 'channels_first':
            se_feature = Permute((3, 1, 2))(se_feature)
        se_feature = multiply([input_feature, se_feature])
        return se_feature

### 通道注意力,channel attention
    通道注意力机制模块流程：
    先将输入特征图(featuremap)分别经过基于width和height进行全局最大池化(global max pooling)和全局平均池化(global average pooling)，
    对特征映射基于两个维度压缩，获得两张不同维度的特征描述。池化后的特征图共用一个多层感知器网络，先通过1*1卷积降维再1*1卷积升维。
    将两张特征图叠加layers.add()，经过sigmoid激活函数归一化特征图的每个通道的权重。将归一化后的权重和输入特征图相乘。
    def channel_attenstion(inputs, ratio=0.25):
        '''ratio代表第一个全连接层下降通道数的倍数'''
        channel = inputs.shape[-1]  # 获取输入特征图的通道数
        # 分别对输出特征图进行全局最大池化和全局平均池化
        # [h,w,c]==>[None,c]
        x_max = layers.GlobalMaxPooling2D()(inputs)
        x_avg = layers.GlobalAveragePooling2D()(inputs)
        # [None,c]==>[1,1,c]
        x_max = layers.Reshape([1,1,-1])(x_max)  # -1代表自动寻找通道维度的大小
        x_avg = layers.Reshape([1,1,-1])(x_avg)  # 也可以用变量channel代替-1
        # 第一个全连接层通道数下降1/4, [1,1,c]==>[1,1,c//4]
        x_max = layers.Dense(channel*ratio)(x_max)
        x_avg = layers.Dense(channel*ratio)(x_avg)
        # relu激活函数
        x_max = layers.Activation('relu')(x_max)
        x_avg = layers.Activation('relu')(x_avg)
        # 第二个全连接层上升通道数, [1,1,c//4]==>[1,1,c]
        x_max = layers.Dense(channel)(x_max)
        x_avg = layers.Dense(channel)(x_avg)
        # 结果在相叠加 [1,1,c]+[1,1,c]==>[1,1,c]
        x = layers.Add()([x_max, x_avg])
        # 经过sigmoid归一化权重
        x = tf.nn.sigmoid(x)
        # 输入特征图和权重向量相乘，给每个通道赋予权重
        x = layers.Multiply()([inputs, x])  # [h,w,c]*[1,1,c]==>[h,w,c]
        return x

### 空间注意力机制， spatial attention
    首先，特征图分别经过基于通道维度的最大池化和平均池化，将输出的两张特征图在通道维度堆叠 layers.concatenate()。
    然后使用 7*7（或3*3、1*1）大小的卷积核融合通道信息，调整通道数，最后经过sigmoid函数归一化权重。将归一化权重和输入特征度相乘。
    def spatial_attention(inputs):
        # 在通道维度上做最大池化和平均池化[b,h,w,c]==>[b,h,w,1]
        # keepdims=Fale那么[b,h,w,c]==>[b,h,w]
        x_max = tf.reduce_max(inputs, axis=3, keepdims=True)  # 在通道维度求最大值
        x_avg = tf.reduce_mean(inputs, axis=3, keepdims=True)  # axis也可以为-1
        # 在通道维度上堆叠[b,h,w,2]
        x = layers.concatenate([x_max, x_avg])
        # 1*1卷积调整通道[b,h,w,1]
        x = layers.Conv2D(filters=1, kernel_size=(1,1), strides=1, padding='same')(x)
        # sigmoid函数权重归一化
        x = tf.nn.sigmoid(x)
        # 输入特征图和权重相乘
        x = layers.Multiply()([inputs, x])
        return x

### CBAM模块（Convolutional Block Attention Module）
    输入特征图想经过通道注意力机制，将权重和输入特征图相乘后再送入空间注意力机制，将归一化权重和空间注意力机制的输入特征图相乘，得到最终的特征图。
    该注意力模块( CBAM )，可以在通道和空间维度上进行 Attention 。其包含两个子模块 Channel Attention Module(CAM) 和 Spartial Attention Module(SAM)。
    CBAM相比SE，只是多了一个并行的Max Pooling层。那为什么加个并行的呢？结果导向，作者通过实验说明这样的效果好一些，也许其好一些的原因是因为多一种信息编码方式.
    def CBAM_attention(inputs):
        # 先经过通道注意力再经过空间注意力
        x = channel_attenstion(inputs)
        x = spatial_attention(x)
        return x

### ECA(Efficient Channel Attention) 高效通道注意力
    ECA(Efficient Channel Attention) 注意力模块通过快速的一维卷积产生通道注意；
    提出了一种不降维的局部跨通道交互策略，有效避免了降维对于通道注意力学习效果的影响；
    def eca_block(input_feature, dim_axis=-1, b=1, gamma=2, name_num=""):
        """对指定维度进行ECA注意力"""
        channel = input_feature.shape[dim_axis]
        kernel_size = int(abs((math.log(channel, 2) + b) / gamma))
        kernel_size = kernel_size if kernel_size % 2 else kernel_size + 1
        axis = []
        if dim_axis > 0:
            ax_list = range(1, len(input_feature.shape))
        elif dim_axis < 0:
            ax_list = range(-1, -len(input_feature.shape), -1)[::-1]
        else:
            raise ValueError('指定维度不应该为：{}'.format(dim_axis))
        for ax in ax_list:
            if dim_axis != ax:
                axis.append(ax)
        avg_pool = Lambda(lambda x: K.mean(x, axis=axis, keepdims=False), name='global_average_pool_{}'.format(name_num))(input_feature)
        x = Reshape((-1, 1))(avg_pool)
        x = Conv1D(1, kernel_size=kernel_size, padding="same", name="eca_layer_" + str(name_num), use_bias=False, )(x)
        x = Activation('sigmoid')(x)
        x = Reshape((1 if ax != dim_axis else channel for ax in ax_list))(x)
        output = multiply([input_feature, x])
        return output

### 位置注意力, position attention
    位置注意力机制负责捕获特征图在任意两个位置的空间依赖关系，无论距离如何，类似的特征都会彼此相关。通道注意力机制负责整合所有通道映射之间的相关特征来选择性地强调存在相互依赖的通道映射。
    位置注意力机制的流程图如下:
    （1）输入特征图A(C×H×W)首先分别通过3个卷积层得到3个特征图B,C,D,然后将B,C,D reshape为C×N，其中N=H×W
    （2）然后将reshape后的特征图B的转置(NxC)与reshape后的特征图C(CxN)矩阵相乘tf.multul()，再通过softmax得到归一化后的权重 S(N×N)
    （3）接着在reshape后的特征图D(CxN)和权重S的转置(NxN)之间执行矩阵乘法tf.multul()，再乘以尺度系数α，再reshape为原来形状，其中α初始化为0，并逐渐的学习得到更大的权重
    （4）最后与输入特征图A相叠加layers.add()得到最后的输出E
    def position_attention(inputs):
        # 定义可训练变量，反向传播可更新
        gama = tf.Variable(tf.ones(1))  # 初始化1
        # 获取输入特征图的shape
        b, h, w, c = inputs.shape
        # 深度可分离卷积[b,h,w,c]==>[b,h,w,c//8]
        x1 = layers.SeparableConv2D(filters=c//8, kernel_size=(1,1), strides=1, padding='same')(inputs)
        # 调整维度排序[b,h,w,c//8]==>[b,c//8,h,w]
        x1_trans = tf.transpose(x1, perm=[0,3,1,2])
        # 重塑特征图尺寸[b,c//8,h,w]==>[b,c//8,h*w]
        x1_trans_reshape = tf.reshape(x1_trans, shape=[-1,c//8,h*w])
        # 调整维度排序[b,c//8,h*w]==>[b,h*w,c//8]
        x1_trans_reshape_trans = tf.transpose(x1_trans_reshape, perm=[0,2,1])
        # 矩阵相乘
        x1_mutmul = x1_trans_reshape_trans @ x1_trans_reshape
        # 经过softmax归一化权重
        x1_mutmul = tf.nn.softmax(x1_mutmul)
        # 深度可分离卷积[b,h,w,c]==>[b,h,w,c]
        x2 = layers.SeparableConv2D(filters=c, kernel_size=(1,1), strides=1, padding='same')(inputs)
        # 调整维度排序[b,h,w,c]==>[b,c,h,w]
        x2_trans = tf.transpose(x2, perm=[0,3,1,2])
        # 重塑尺寸[b,c,h,w]==>[b,c,h*w]
        x2_trans_reshape = tf.reshape(x2_trans, shape=[-1,c,h*w])
        # 调整x1_mutmul的轴，和x2矩阵相乘
        x1_mutmul_trans = tf.transpose(x1_mutmul, perm=[0,2,1])
        x2_mutmul = x2_trans_reshape @ x1_mutmul_trans
        # 重塑尺寸[b,c,h*w]==>[b,c,h,w]
        x2_mutmul = tf.reshape(x2_mutmul, shape=[-1,c,h,w])
        # 轴变换[b,c,h,w]==>[b,h,w,c]
        x2_mutmul = tf.transpose(x2_mutmul, perm=[0,2,3,1])
        # 结果乘以可训练变量
        x2_mutmul = x2_mutmul * gama
        # 输入和输出叠加
        x = layers.add([x2_mutmul, inputs])
        return x

### DANet 注意力机制
    DANet 注意力机制由位置注意力机制（position）和通道注意力机制（channel）组合而成。
    DANet的总体流程: 输入图像分别经过位置注意力机制和通道注意力机制，将输出的特征图叠加layers.add()，得到输出特征图。

### Seq2Seq
    Seq2Seq ( Sequence-to-sequence 的缩写)，就如字面意思，输入一个序列，输出另一个序列。这种结构最重要的地方在于输入序列和输出序列的长度是可变的。
    Seq2Seq 强调目的，不特指具体方法，满足输入序列，输出序列的目的，都可以统称为 Seq2Seq 模型。Seq2Seq 使用的具体方法基本都是属于 Encoder-Decoder 模型的范畴。
    在机器翻译里面，如将英语 「it is a cat.」翻译成汉语 「这是一只猫。」，输入 4 个单词，输出 5 个汉字。
    在训练数据集中，我们可以在每个句子后附特殊字符 <EOS> (end of sequence) 以表示序列终止，每个句子前用到了特殊字符 <BOS> (begin of seqence) 表示序列开始。
    Encoder 在最终时间步的隐状态作为输入句子表征和编码信息。Decoder 在各个时间步中使用输入句子的编码信息和上一个时间步的输出以及隐藏状态作为输入。
    案例：英文 it is a cat. 翻译成中文的过程。
        先将整个源句子进行符号化处理，以一个固定的特殊标记作为翻译的开始符号和结束符号。此时句子变成 it is a cat .
        对序列进行建模，得到概率最大的译词，如第一个词为 “这”。将生成的词加入译文序列，重复上述步骤，不断迭代。
        直到终止符号被模型选择出来，停止迭代过程，并进行反符号化处理，得到译文。

### encoder-decoder模型(编码-解码模型)
    Encoder-Decoder模型中的编码，就是将输入序列转化成一个固定长度的向量；解码，就是将之前生成的固定向量再转化成输出序列。
    准确的说，Encoder-Decoder并不是一个具体的模型，而是一类框架。
    RNN 的局限，在机器翻译中，输入某一序列，通过 RNN 将其转化为一个固定向量，再将固定序列转化为输出序列，如将英文翻译成中文。
    不管输入序列和输出序列长度是什么，中间的「向量 c」长度都是固定的。所以，RNN 结构的 Encoder-Decoder 模型存在长程梯度消失问题，
    对于较长的句子，我们很难寄希望于将输入的序列转化为定长的向量而保存所有有效信息，即便 LSTM 加了门控机制可以选择性遗忘和记忆，随着所需翻译的句子难度增加，这个结构的效果仍然不理想。

### Transformer
    Transformer是一个完全依靠Self-attention而不使用序列对齐的RNN或卷积的方式来计算输入输出表示的转换模型。
    Transformer整体架构就是一个Seq2Seq结构，包括编码器(Encoder)和解码器(decoder)，即是由编码组件、解码组件和它们之间的连接组成。
    编码组件部分由一堆编码器（encoder）构成（将n个编码器叠在一起）。
    解码组件部分也是由相同数量（与编码器对应）的解码器（decoder）组成的。
    
    Encoder的输出和decoder的结合方式是将最后一个encoder的输出将和每一层的decoder进行结合。
    所有的编码器(Encoder)在结构上都是相同的，但它们没有共享参数。每个编码器(Encoder)都可以分解成两个子层，
    即Encoder的每一层有两个操作，分别是注意力（self-attention）和前馈（feed-forward）；
    而Decoder的每一层有三个操作，分别是Self-Attention、Encoder-Decoder Attention以及Feed Forward操作。
    这里的Self-Attention和Encoder-Decoder Attention都是用的是Multi-Head Attention机制。
    
    从编码器输入的句子首先会经过一个自注意力（self-attention）层，这层帮助编码器在对每个单词编码时关注输入句子的其他单词。
    自注意力层的输出会传递到前馈（feed-forward）神经网络中。每个位置的单词对应的前馈神经网络都完全一样。
    解码器(decoder)中也有编码器的自注意力（self-attention）层和前馈（feed-forward）层。
    除此之外，这两个层之间还有一个编码-解码注意力层(Encoder-Decoder Attention)，用来关注输入句子的相关部分。
    
    一个编码器接收向量列表作为输入，接着将向量列表中的向量传递到自注意力层进行处理，然后传递到前馈神经网络层中，
    将输出结果传递到下一个编码器中。
    输入序列的每个单词都经过自编码过程。然后，他们各自通过前向传播神经网络——完全相同的网络，而每个向量都分别通过它。               

    模型框架如下
    Encoder 部分
        由 6 个相同的层(或者称之为块)组成，每个层包含 2 个部分：Multi-Head Self-Attention 和 Position-Wise Feed-Forward Network (全连接层)
        Multi-head self attention - 多组 self attention 的连接。
        Position-wise feed forward network，其实就是一个 MLP 网络,引入非线性变换，从而增加了模型的表现能力。
    Decoder 部分
        Decoder 也是由 6 个相同的层组成，每个层包含 3 个部分：Multi-Head Self-Attention、Multi-Head Context-Attention、Position-Wise Feed-Forward Network。
        Multi-head self attention (with mask) 与 encoder 部分相同，只是采用 0-1 mask 消除右侧单词对当前单词 attention 的影响。
        Multi-head attention(with encoder)引入encoder 部分的输出在此处作为multi-head 的其中几个head。
        Position-wise feed forward network与encoder 部分相同。
    上面每个部分（Encoder2个部分之间，Decode3个部分之间）都有残差连接 (redidual connection)，然后接一个 Layer Normalization。

    Transformer中的注意力机制
    谷歌将注意力机制一般化了，一个注意力函数描述为将Query与一组健值对(Key-Value)映射到输出，其中这三者均为向量形式。
    对于翻译任务，Query 可以认为是源词语向量序列，而 Key 和 Value 可以认为为目标词向量序列。
    即注意力通过计算 Query 和 Key 之间的相似性，并通过相似性来确定 Query 和 Value 之间的注意力关系。

### Transformers库
    "Transformers"库和"Transformer"模型是两个不同的概念，但它们都与自然语言处理（NLP）紧密相关。
    "Transformers"是一个由Hugging Face团队开发的开源Python库，它提供了一系列预训练模型和相关工具，用于各种NLP任务。
    这个库的目标是使先进的NLP技术易于访问和使用，无论用户的经验水平如何。"Transformers"库包括了多种流行的预训练模型，如BERT、GPT-2、RoBERTa、T5等，这些模型在各种NLP任务上都取得了卓越的性能。
    使用"Transformers"库，开发者可以轻松地在自己的应用程序中加载预训练模型，并进行微调（fine-tuning）以适应特定的任务。此外，库还提供了丰富的API，用于处理数据、生成文本、分类文本、命名实体识别等多种任务。
    与Transformer模型区别
    用途: "Transformers"库是一个提供多种预训练模型和工具的软件库，用于简化和加速NLP任务的开发。而"Transformer"模型是一种特定的深度学习架构，用于处理序列数据，尤其是在NLP领域。
    范围: "Transformers"库包含多种基于"Transformer"架构的模型以及其他类型的模型，而"Transformer"通常指的是最初提出的那个具有自注意力机制的模型。
    实现: "Transformers"库提供了一个高级接口，使得用户可以方便地加载、使用和微调各种预训练模型。"Transformer"模型则是这些预训练模型之一的底层架构。
    简而言之，"Transformers"库是一个工具集，它包含了基于"Transformer"架构的多种模型和其他模型，而"Transformer"是一种特定的深度学习模型，是"Transformers"库中的一个组成部分。

### Feed forward network(FNN层)
    每一层经过attention之后，还会有一个FFN，这个FFN的作用就是空间变换。FFN包含了2层linear transformation(线性变换)层，中间的激活函数是ReLu。
    FFN的加入引入了非线性(ReLu激活函数)，变换了attention output的空间, 从而增加了模型的表现能力。把FFN去掉模型也是可以用的，但是效果差了很多。

### Transformer-XL
    XL实际上是“extra-long”的意思，这意味着Transformer-XL在模型设计上做了长度方面的延申工作。
    句段层级的循环复用; Transformer每次处理的是定长片段。那么在XL版本中，上一次处理的片段信息将会被存储起来，在当前片段的处理中会把这部分信息添加进来，这便是“延长”的含义。这样做便完成了上下文之间的迁移。
    使用相对位置编码;在Transformer中，原本的位置embedding是一种绝对位置编码，因此，当我们采用上面提到的迁移方法时，绝对编码会让网络“产生困惑”，例如片段大小为4，那么每个片段的绝对位置编码都为（0，1，2，3），
    这变失去了位置顺序的信息。因此在XL中，使用相对位置编码。

### 人工神经网络（Artificial Neural Network，ANN）
    简称神经网络（Neural Network，NN）或类神经网络，在机器学习和认知科学领域，是一种数学模型或计算模型，用于对函数进行估计或近似。神经网络由大量的人工神经元联结进行计算。
    神经网络作为一类非线性的机器学习模型，可以更好的实现输入和输出之间的映射。

### 前馈神经网络（feedforward neural network，FNN）
    前馈神经网络简称前馈网络，是人工神经网络的一种。前馈神经网络采用一种单向多层结构。
    在它内部，参数从输入层向输出层单向传播。有异于递归神经网络，它的内部不会构成有向环。
    其中每一层包含若干个神经元。在此种神经网络中，各神经元可以接收前一层神经元的信号，并产生输出到下一层。
    第0层叫输入层，最后一层叫输出层，其他中间层叫做隐含层（或隐藏层、隐层）。隐层可以是一层。也可以是多层。
    “前馈”是指整个网络中无反馈，信号从输入层向输出层单向传播，可用一个有向无环图表示
    其实我们常用的网络，都是前馈神经网络，从输入到输出是一个有向图，中间不会有环或者反向传播。
    当然，我们在训练前馈神经网络的时候，会用到反向传播进行参数调整。但仍不影响整个网络的有向和前馈性质。
    前馈神经网络各神经元分层排列。每个神经元只与前一层的神经元相连。
    接收前一层的输出，并输出给下一层，数据正向流动，输出仅由当前的输入和网络权值决定，各层间没有反馈。
    包括：单层感知器，线性神经网络，BP神经网络、RBF神经网络等。

### 反馈神经网络
    反馈神经网络中，结构图的有向图是有回路的。
    在这种网络中，每个神经元同时将自身的输出信号作为输入信号反馈给其他神经元。
    反馈网络，也称记忆网络。网络中的神经元不仅可以接受其他神经元的信息也可以接受自己的历史信息，因此神经元具有记忆功能。
    记忆网络包括循环神经网络、Hopfield网络、玻尔兹曼机、受限玻尔兹曼机等。这种网络具有更强的计算与记忆能力。
    
### 反向传播（Backpropagation，缩写为BP）
    是“误差反向传播”的简称，是一种与最优化方法（如梯度下降法）结合使用的，用来训练人工神经网络的常见方法。
    该方法对网络中所有权重计算损失函数的梯度。这个梯度会反馈给最优化方法，用来更新权值以最小化损失函数。
    反向传播要求有对每个输入值想得到的已知输出，来计算损失函数梯度。因此，它通常被认为是一种监督式学习方法，虽然它也用在一些无监督网络（如自动编码器）中。
    它是多层前馈网络的Delta规则的推广，可以用链式法则对每层迭代计算梯度。反向传播要求人工神经元（或“节点”）的激励函数可微。

### 神经网络（Neural Network，NN）
    神经网络即人工神经网络，由具有权重和偏置的神经元组成，简单来说就是模拟生物神经元进行信息处理的模型。
    在训练过程中，神经网络通过调整神经元的权重和偏置，最终得到一个能将输入信息处理成为接近或符合我们预期输出的模型。
    标准神经网络（NN），深度学习本质上是神经网络，各种神经网络的基础就是NN, 一个经典的神经网络图，包含三个层次的神经网络：输入层、输出层、中间层（也叫隐藏层）。

### 卷积神经网络（Convolutional Neural Networks，CNN）
    针对普通神经网络无法处理图像的问题，后来引入了卷积神经网络。
    CNN在处理数据时充分考虑图像的结构，其中的神经元按三维排列——宽度（W）、高度（H）和深度（D）。
    当前层中的每个神经元都连接到前一层输出的小块，全连接层的每个神经元均与前一层的所有神经元相连，因而能够有效提取图像的特征。
    CNN通常使用以下类型的层：输入层、卷积层、激励层、池化层、全连接层。

### 递归神经网络（RNN）
    RNN主要处理有时序关系的变长序列问题。
    每个神经元在每一时刻都一个特殊的隐藏(hidden)状态h(t)，由当前节点的输入I(t)和上一时刻t-1隐藏状态h(t-1)加权求和后经过一个非线性激活函数(tanh)得到。
    单纯的RNN因为无法处理随着递归，权重指数级爆炸或梯度消失问题，难以捕捉长期时间关联；而结合不同的LSTM可以很好解决这个问题。
    时间递归神经网络可以描述动态时间行为，因为和前馈神经网络（feedforward neural network）接受较特定结构的输入不同，RNN将状态在自身网络中循环传递，
    因此可以接受更广泛的时间序列结构输入。
    RNN存在梯度爆炸和梯度消失问题，如果时间序列较长，W_{hh}奇异值如果>1，t个σ连乘后会非常大，反之则会非常小。
    递归神经网络（RNN）是两种人工神经网络的总称。一种是时间递归神经网络（recurrent neural network），又名循环神经网络，包括RNN、LSTM、GRU等；另一种是结构递归神经网络（recursive neural network）。

### 深度神经网络（Deep Neural Network，DNN）：
    当隐藏层多于2时的NN称为深度神经网络（DNN）；具有多层的神经网络我们一般就可以认为是深度神经网络。

### Simple RNN (without context)
    「Simple RNN」 ：这个encoder-decoder模型结构中，encoder将整个源端序列(不论长度)压缩成一个向量(encoder output)，
    源端信息和decoder之间唯一的联系只是: encoder output会作为decoder的initial states的输入。
    这样带来一个显而易见的问题就是，随着decoder长度的增加，encoder output的信息会衰减。
    这种模型有2个主要的问题:
    源端序列不论长短，都被统一压缩成一个固定维度的向量，并且显而易见的是这个向量中包含的信息中，
    关于源端序列末尾的token的信息更多，而如果序列很长，最终可能基本上“遗忘”了序列开头的token的信息。
    第二个问题同样由RNN的特性造成: 随着decoder timestep的信息的增加，initial hidden states中包含的encoder output相关信息也会衰减，
    decoder会逐渐“遗忘”源端序列的信息，而更多地关注目标序列中在该timestep之前的token的信息。

### Contextualized RNN(语境化RNN)
    「Contextualized RNN」 ：为了解决 encoder output随着decoder timestep增加而信息衰减的问题，
    有人提出了一种加了context的RNN sequence to sequence模型：
    decoder在每个timestep的input上都会加上一个context。
    为了方便理解，我们可以把这看作是encoded source sentence。
    这样就可以在decoder的每一步，都把源端的整个句子的信息和target端当前的token一起输入到RNN中，防止源端的context信息随着timestep的增长而衰减。
    但是这样依然有一个问题: context对于每个timestep都是静态的(encoder端的final hidden states，或者是所有timestep的output的平均值)。

### Contextualized RNN with Attention
    在每个timestep输入到decoder RNN结构中之前，会用当前的输入token的vector与encoder output中的每一个position(位置)的vector作一个"attention"操作，
    这个"attention"操作的目的就是计算当前token与每个position之间的"相关度"，从而决定每个position的vector在最终该timestep的context中占的比重有多少。
    最终的context即encoder output每个位置vector表达的 「加权平均」 。

### timestep
    文本处理中，一个单词代表一个timestep，在inference(推理)的时候，只能一个单词一个单词地输出；
    而在train的时候，我们有整个句子，因此可以一次feed若干个单词
    设我们输入数据的格式为(batch_size, time_steps, input_size)，其中time_steps表示序列本身的长度，
    如在Char RNN中，长度为10的句子对应的time_steps就等于10。最后的input_size就表示输入数据单个序列单个时间维度上固有的长度。
    这里一共五句话，把五句话当成一个批次，然后time_step为3
    sentences = ["i love you", "he loves me", "she likes baseball", "i hate you", "sorry for that"]
    >>>                       batch_size 为5
    time_step1:    i        he      she        i       sorry
    time_step2:    love     love    likes      hate    for
    time_step3:    you      me      baseball   you     that
    然后，rnn计算损失的时候,就会在每一个time_step计算batch_size大小个样本的平均损失(也有时候是最后一个time_step统计损失).

### LSTM (Long-Short-Term-Memory)
    一种特殊的RNN网络，该网络设计出来是为了解决长依赖问题。
    LSTM网络能通过一种被称为门的结构对细胞状态进行删除或者添加信息。
    LSTM相比RNN其实就是多了一个门(gate)机制和细胞记忆单元(cell-state)用来存储，用来记录信息。
    它的重复单元不同于标准RNN网络里的单元只有一个tanh激活网络层。
    一个LSTM里面包含三个门(输入门、输出门、遗忘门)来控制细胞状态。因为三者的计算方法都相同，区别只是使用了不同的权重矩阵以便反向传播时对三个门独立更新。
    LSTM的第一步就是决定细胞状态需要丢弃哪些信息(计算遗忘门)。
    这部分操作是通过一个称为忘记门的sigmoid单元来处理的。
    它通过查看前一隐藏状态h(t-1)和当前输入xt信息来输出一个0-1之间的向量，该向量里面的0-1值表示前一细胞状态C(t-1)中的哪些信息保留或丢弃多少。0表示不保留，1表示都保留。
    第二步是决定给细胞状态添加哪些新的信息。
    这一步又分为两个步骤，首先，利用h(t-1)和xt通过一个称为输入门的sigmoid操作来决定更新哪些信息，得到it。
    然后利用h(t-1)和xt通过一个tanh层得到新的候选细胞信息(临时细胞状态C't)，这些信息可能会被更新到细胞信息中。
    第三步：将更新旧的细胞信息C{t-1}，变为新的细胞信息C{t},即计算当前时刻的细胞状态。
    更新的规则就是通过忘记门选择忘记旧细胞信息的一部分(第一步的结果)，通过输入门选择添加候选细胞信息C‘{t}的一部分(第二步的2个结果)得到新的细胞信息C{t}。
    此步不涉及到激活函数，仅仅是信息的拼接，是一个线性变换。
    第四步：计算输出门和当前时刻隐藏层状态
    更新完细胞状态后(第三步的结果)需要根据输入的h(t-1)和xt来判断输出细胞的哪些状态特征，这里需要将输入经过一个称为输出门的sigmoid层得到判断条件，
    然后将细胞状态经过tanh层得到一个-1~1之间值的向量，该向量与输出门得到的判断条件相乘就得到了最终该RNN单元的输出。
    即是分两个步骤，首先，前一时刻隐藏层状态h(t-1)和当前时刻输入词xt经过一个sigmoid层激活得到输出门的值Ot；
    其次，细胞状态C{t}(第三步的结果)经过tanh层得到一个向量与输出门的值Ot进行点乘就得到当前时刻隐藏层的状态ht.
    各个控制单元的作用：
    输出门O(t-1)：用于保存C(t-1)中对h(t-1)有用的信息
    输入门it：用于判断当前输入xt是否对context有作用，当it=1时，使用xt作为输入
    遗忘门ft：用于判断当前细胞状态(cell_state)C(t)对上一个细胞状态C(t-1)的依赖程度,当前输入xt如果依赖上文信息，关闭遗忘门即可。
    细胞状态C(t)：它包含了当前输入xt和上一时刻细胞状态C(t-1)的信息，并且由于C(t)和C(t-1)之间是“短路连接”（由公式可以看出两者之间是线性关系），
    因此反向传播时，C(t)的梯度可以直接传播给C(t-1)，这也是LSTM能够有效缓解RNN中梯度消失和梯度爆炸的关键。

### BiLSTM
    BiLSTM是Bi-directional Long Short-Term Memory的缩写，是由前向LSTM-l与后向LSTM-r组合而成。
    利用LSTM对句子进行建模还存在一个问题：无法编码从后到前的信息。
    通过BiLSTM可以更好的捕捉双向的语义依赖。
    
### 门(gate)
    门能够有选择性的决定让哪些信息通过。其实门的结构很简单，就是一个sigmoid层和一个点乘操作的组合。
    因为sigmoid层的输出是0-1的值，这代表有多少信息能够流过sigmoid层。0表示都不能通过，1表示都能通过。
    
### 门循环单元（GRU）
    GRU是LSTM结构的变式；它将忘记门和输入门合并成一个新的门，称为更新门。GRU还有一个门称为重置门。
    重置门决定了如何将新的输入信息与前面的记忆相结合。更新门定义了前面记忆保存到当前时间步的量。
    
### 感知机(Perceptron)
    感知即意识对内外界信息的觉察、感觉、注意、知觉的一系列过程。模拟人类感知能力的机器，称之为‘感知机’，也称‘感知器’。
    感知机是二分类问题的线性分类模型，其输入为实例的特征向量，输出为实例的类别，分别是+1和-1，属于判别模型。
    感知器使用特征向量来表示的前馈神经网络，它是一种二元分类器，把矩阵上的输入 x（实数值向量）映射到输出值 f(x)上（一个二元的值）。

### 多层感知机(MLP,Multilayer Perceptron)
    多层感知器（Multilayer Perceptron,缩写MLP）是一种前向结构的人工神经网络，映射一组输入向量到一组输出向量。
    MLP可以被看作是一个有向图，由多个的节点层所组成，每一层都全连接到下一层。
    除了输入节点，每个节点都是一个带有非线性激活函数的神经元（或称处理单元）。
    一种被称为反向传播算法的监督学习方法常被用来训练MLP。 
    MLP是感知器的推广，克服了感知器不能对线性不可分数据进行识别的弱点。
    MLP主要由若干个全连接层(Fully Connected Layer)和激活层(ReLU)组成。
    通过keras实现MLP如下：
    def mlp(x, hidden_units, dropout_rate):
        for units in hidden_units:
            x = layers.Dense(units, activation=tf.nn.gelu)(x)
            x = layers.Dropout(dropout_rate)(x)
        return x

### utterance embedding 
    多轮对话的每条文本（utterance）的向量表示,即为 utterance embedding 

### context embedding
    多轮对话由每条文本（utterance）组合成utterance embedding sequence；
    utterance embedding sequence 再通过一层Gated RNN（GRU、LSTM等）把无用的utterances中的噪声滤掉，进而取最后一个时刻的隐状态得到整个多轮对话（context）的context embedding

### 随机加权平均（SWA）
    随机加权平均（SWA）方法来自于集成。集成是用于提高机器学习模型性能的流行的技术。
    最简单的方式为，集成可以对不同初始化的模型的若干副本进行训练，并将对副本的预测平均以得到整体的预测。但是这种方法的缺点是必须承担n个不同副本的成本。研究人员提出快照集成（Snapshot Ensembles）方法。
    改方法是对一个模型进行训练，并将模型收敛到几个局部最优点，保存每个最优点的权重。这样一个单一的训练就可以产生n个不同的模型，将这些预测平均就能预测出整体。

### 未登录词（Out-of-vocabulary， OOV）
    未登录词就是训练时未出现，测试时出现了的单词。
    在自然语言处理或者文本处理的时候，我们通常会有一个字词库（vocabulary）。
    这个vocabulary要么是提前加载的，或者是自己定义的，或者是从当前数据集提取的。
    假设之后你有了另一个的数据集，这个数据集中有一些词并不在你现有的vocabulary里，我们就说这些词汇是Out-of-vocabulary，简称OOV。

### 分布外数据（OOD数据, Out-of-distribution data）
    大多数现有机器学习模型都基于封闭世界假设（the closed-world assumption）来训练，即测试集和训练集独立同分布，或者说两者来源于同一分布（in-distribution）。
    然而，当模型被部署在开放世界场景（open-world scenario）中，测试样本的分布可以是取自不同于训练集分布的分布的（out of distribution），因而需要被谨慎处理。
    在很多现实场景中，训练数据的分布与测试数据分布通常表现出不一致性，即分布偏移（Distribution shifts），旨在提升模型在该类场景下性能的问题通常被称为分布外泛化（Out-of-Distribution Generalization OOD；或域泛化，Domain Generaliziation）问题。

### 查找表(lookup table)  
    查找表在不同的地方，其输入输出会有所不同；有时候，查找表就是一个全连接层。只不过输入比较稀疏的(二值)向量，所以矩阵乘法不需要那些输入为零值对应的系数，只要直接查表相加就可以了。 

### 表述性状态转移（Representational State Transfer，REST）
    REST是一种软件架构风格（约束条件和原则的集合，但并不是标准）。
    REST通过资源 的角度观察网络，以URI对网络资源进行唯一标识，响应端根据请求端的不同需求，通过无状态通信，对其请求的资源进行表述。
    满足REST约束条件和原则的架构或接口，就被称为是RESTful架构或RESTful接口。
    REST将资源的状态以最适合客户端或服务端的形式从服务器端转移到客户端。（或反过来）。

### 没有免费午餐定理(No Free Lunch，简称NFL)
    无免费午餐（No Free Lunch, NFL）定理证明了任何模型在所有问题上的性能都是相同的，其总误差和模型本身是没有关系的。
    NFL 定理的一个核心前提，也就是每种问题出现的概率是均等的，每个模型用于解决所有问题时，其平均意义上的性能是一样的。
    所有模型在等概率出现的问题上都有同样的性能，这件事可以从两个角度来理解：
    一是从模型的角度来看，如果单独拿出一个特定的模型来观察的话，这个模型必然会在解决某些问题时误差较小，而在解决另一些问题时误差较大；
    二是从问题的角度来看，如果单独拿出一个特定的问题来观察的话，必然有某些模型在解决这些问题时具有较高的精度，而另一些模型的精度就没那么理想了。
    脱离问题的实际情况谈论模型优劣是没有意义的，只有让模型的特点和问题的特征相匹配，模型才能发挥最大的作用。
    NFL定理表明没有一个学习算法可以在任何领域总是产生最准确的学习器。不管采用何种学习算法，至少存在一个目标函数，能够使得随机猜测算法是更好的算法
    NFL定理最重要意义是，在脱离实际意义情况下，空泛地谈论哪种算法好毫无意义，要谈论算法优劣必须针对具体学习问题

### 奥卡姆剃刀（Occam's Razor）
    奥卡姆剃刀（Occam's Razor）可以理解为如果有多种模型都能够同等程度地符合同一个问题的观测结果，那就应该选择其中使用假设最少的，也就是最简单的模型。
    尽管越复杂的模型通常能得到越精确的结果，但是在结果大致相同的情况下，模型就越简单越好。本质上说，奥卡姆剃刀的关注点是模型复杂度。

### 残差连接（skip connect）
    将输出表述为输入和输入的一个非线性变换的线性叠加。有时也叫跳过连接。
    在一定程度上，网络越深表达能力越强，性能越好。不过，随着网络深度的增加，带来了许多问题，梯度消散，梯度爆炸。
    深度学习依靠误差的链式反向传播来进行参数更新，一旦其中某一个导数很小，多次连乘后梯度可能越来越小，这就是常说的梯度消散，对于深层网络，传到浅层几乎就没了。
    但是如果使用了残差，每一个导数就加上了一个恒等项1。此时就算原来的导数df/dx很小，这时候误差仍然能够有效的反向传播。
    skip connect除了改善了反向传播过程中的梯度消散问题，另外还在一定程度上缓解了权重矩阵的退化问题。
    有时候虽然梯度范数大，但是如果网络的可用自由度对这些范数的贡献非常不均衡，也就是每个层中只有少量的隐藏单元对不同的输入改变它们的激活值，
    而大部分隐藏单元对不同的输入都是相同的反应，此时整个权重矩阵的秩不高。并且随着网络层数的增加，连乘后使得整个秩变的更低。
    这也是我们常说的网络退化问题，虽然是一个很高维的矩阵，但是大部分维度却没有信息，表达能力没有看起来那么强大。
    虽然权重矩阵是一个很高维的矩阵，但是大部分维度却没有信息，使得网络的表达能力没有看起来那么强大。
    这样的情况一定程度上来自于网络的对称性，而残差连接打破了网络的对称性。
    残差连接正是强制打破了网络的对称性，提升了网络的表征能力

### ReZero加权的残差连接
    在NLP领域，如Transformer的深度模型由于梯度消失/爆炸难以训练，往往需要花费很长的时间才能收敛。
    ReZero：在残差连接前增加一个权重，使模型能够更好接受到梯度信号，加快收敛速度。
    对每个残差连接加上一个可学习的系数——使得模型能在训练初期更加稳定，进而促进整个训练过程。
    这种方法能在上百层的Transformer上收敛，并在常见深层模型上大大缩短训练时间，同时取得相近的结果。
    正常的残差连接对一个L层的模型，第 i+1 层的输入（也即第i层的输出）可以表示为：x(i+1) = xi + Fi(xi)
    而ReZero做的很简单，它只是在残差连接前加了一个可学习的参数：x(i+1) = xi + αi * Fi(xi)
    并且所有的α都初始为0。显然，当α=1时就是正常的残差网络，当初始化α=0时就是ReZero

### ResNet(Residual Networks，残差网络)
    ResNet也称为残差网络。ResNet是由残差块(Residual Building Block)构建的，
    提出了两种映射：identity mapping(恒等映射)，指的是x的部分；residual mapping(残差映射)，残差指的是F(x)部分。最后的输出是F(x)+x。
    残差块由多个级联的卷积层和一个shortcut connections组成，将二者的输出值累加后，通过ReLU激活层得到残差块的输出。
    多个残差块可以串联起来，从而实现更深的网络。

### 几种连接方式    
    FC：单纯的全连接
        x(i+1) = Fi(xi)
    FC+Res：全连接和残差结合 
        x(i+1) = x(i) + Fi(xi)
    FC+Norm：全连接和normalization
        x(i+1) = Norm(Fi(xi))
    FC+Res+Pre-Norm: 全连接和残差和pre-normalization
        x(i+1) = x(i) + Fi(Norm(xi))
    FC+Res+Post-Norm：全连接和残差和post-normalization
        x(i+1) = Norm(x(i) + Fi(xi))
    FC+ReZero：
        x(i+1) = x(i) + αi * Fi(xi)

### 对称性(Symmetry)和打破对称性（symmetry breaking）    
    「对称」就是对象在变换之后，结果和变换前不变。如有一变换，前后系统的状态，等价或相同，则此变换为对称变换。
    当某一层的表示的权重相同的话，会导致下一层的单元计算结果全部相同，这意味着，特征在传递到下层时，全部压缩为一个特征了。
    我们把这种现象称为对称性(Symmetry)。
    一个全连接的神经网络，同一层中的任意神经元是同构的，对于相同的输入他们会有同样的输出，
    此时如果将参数全部初始化为相同的值，那么无论前向传播还是反向传播，参数的取值还是完全相同，学习将无法打破这种对称性，
    最终同一网络层中的各个参数仍然是相同的。
    必须随机的初始化神经网络参数的值，以打破这种对称性。

### 随机初始化
    将权重w全部初始化为零或相同的随机数，那么每一层所学到的参数都是一样的，因为它们的梯度一样，所以在反向传播的过程中，每一层的神经元也是相同的。
    因此会导致代价函数在开始的一段时间内，明显下降，但是一段时间以后，停止继续下降。都会导致同样的问题，即对称问题(Symmetry problem)
    初始化为较小的随机数,开始模型可以很好的运行一段时间，但是随着时间增加，前向传递时，方差开始减少，梯度也开始向零靠近，会导致梯度消失(Gradient Vanishing)。
    初始化为较大的随机数,反向传播时，倒数趋于零，梯度也会消失。此外，权重较大且当输入也很大时，如果使用sigmoid做激活函数，会使输出趋向于0和1，会导致更多问题。
    随机初始化很小的值，使得W很小是因为，可以参照激活函数sigmoid和tanh，当W很大，用W * X+b=a得到的a很大，再用对a用激活函数如sigmoid(a)，
    由于a很大了，sigmoid(a)中的a会趋向正无穷或负无穷，则函数值sigmoid(a)趋向于一个平缓的趋势，
    在梯度下降的时候计算的梯度很小，会导致学习的很慢，故使得W取一个很小的值

### 放缩点积attention（scaled dot-Product attention）
    attention可以有很多种计算方式: 加型注意力机制(additive attention)、点积型注意力机制(dot-product attention)，还有带参数的计算方式。
    Attention中(Q^T)*K矩阵计算，query和key的维度要保持一致；
    query和key的点积是 [seq_length * attention_head_size] * [attention_head_size * seq_length]=[seq_length * seq_length]
    其中，query可以看作M个维度为d的向量(长度为M的sequence的向量表达)拼接而成，key可以看作N个维度为d的向量(长度为N的sequence的向量表达)拼接而成。
    放缩点积attention（scaled dot-Product attention）就是使用点积进行相似度计算的attention，
    只是多除了一个（为K的维度）起到调节作用，使得内积不至于太大，防止梯度消失。
    当输入信息的维度 d 比较高，点积模型的值通常有比较大方差，从而导致 softmax 函数的梯度会比较小。因此，缩放点积模型可以较好地解决这一问题。
    缩放因子的作用是 「归一化」 。
    假设Q(m * d), K(N * d)里的元素的均值为0，方差为1，那么A{T}=Q{T} K中元素的均值为0，方差为d. 
    当d变得很大时， A 中的元素的方差也会变得很大，如果 A 中的元素方差很大，那么 softmax(A)的分布会趋于陡峭(分布的方差大，分布集中在绝对值大的区域)。
    总结一下就是softmax(A)的分布会和d有关。因此 A中每一个元素乘上 缩放因子 后，方差又变为1。
    这使得softmax(A) 的分布“陡峭”程度与d解耦，从而使得训练过程中梯度值保持稳定。

### 隐马尔科夫模型（Hidden Markov Model，HMM）
    隐马尔可夫模型（Hidden Markov Model，HMM）是统计模型，它用来描述一个含有隐含未知参数的马尔可夫过程。描述的是已知量和未知量的一个联合概率分布，p（x,y）。
    隐马尔可夫模型(Hidden Markov model, HMM)是一种结构最简单的动态贝叶斯网的生成模型，它是一种有向图模型。
    隐马尔科夫模型是关于时序的概率模型，描述由一个隐藏的马尔可夫链随机生成不可观测的状态随机序列，再由各个状态生成一个可观测的随机序列的过程。
    HMM只依赖于每一个状态和它对应的观察对象; 
    目标函数和预测目标函数不匹配：HMM学到的是状态和观察序列的联合分布P(Y,X)，而预测问题中，我们需要的是条件概率P(Y|X)。
    HMM模型对转移概率和表现概率直接建模，统计共同出现的概率，是一种生成式模型。

### 最大熵隐马尔科夫模型（MEMM）
    MEMM考虑到相邻状态之间依赖关系，且考虑整个观察序列，因此MEMM的表达能力更强；MEMM不考虑P(X)减轻了建模的负担，同时学到的是目标函数是和预测函数一致。
    HMM只限定在了观测与状态之间的依赖，而MEMM引入自定义特征函数，不仅可以表达观测之间的依赖，还可表示当前观测与前后多个状态之间的复杂依赖。

### 马尔可夫链(Markov chain)
    马尔可夫链（Markov chain），又称离散时间马尔可夫链（discrete-time Markov chain），为状态空间中经过从一个状态到另一个状态的转换的随机过程。
    该过程要求具备“无记忆”的性质：下一状态的概率分布只能由当前状态决定，在时间序列中它前面的事件均与之无关。
    这种特定类型的“无记忆性”称作马尔可夫性质。

### 随机场(random field, RF)
    随机场，可以看成是一组随机变量的集合（这组随机变量对应同一个样本空间）。当给每一个位置按照某种分布随机赋予一个值之后，其全体就叫做随机场。
    随机场包含两个要素：位置（site），相空间（phase space）。当给每一个位置中按照某种分布随机赋予相空间的一个值之后，其全体就叫做随机场。
    我们不妨拿种地来打个比方。“位置”好比是一亩亩农田； “相空间”好比是种的各种庄稼。
    我们可以给不同的地种上不同的庄稼，这就好比给随机场的每个“位置”，赋予相空间里不同的值。所以，俗气点说，随机场就是在哪块地里种什么庄稼的事情。
    
### 马尔科夫随机场(MRF)
    马尔科夫随机场(MRF)，描述了具有某种特性的集合。拿种地打比方，如果任何一块地里种的庄稼的种类仅仅与它邻近的地里种的庄稼的种类有关，与其它地方的庄稼的种类无关，那么这些地里种的庄稼的集合，就是一个马尔可夫随机场。
    如果给定的 MRF 中每个随机变量下面还有观察值，我们要确定的是给定观察集合下，这个 MRF 的分布，也就是条件分布，那么这个 MRF 就称为 Conditional random fields (CRF)。
    它的条件分布形式完全类似于 MRF 的分布形式，只不过多了一个观察集合 X。所以，CRF 本质上是给定了条件 (观察值 observations) 集合的 MRF.

### 条件随机场(conditional random field, CRF)
    HMM(隐马尔可夫模型)是CRF的一个特例。HMM等价于只有一个特征函数的CRF。
    可以用于构造在给定一组输入随机变量的条件下,另一组输出随机变量的条件概率分布模型.
    条件随机场(conditional random field)是给定随机变量X条件下,随机变量Y的马尔可夫随机场.
    Y是输出变量,表示标记序列,即状态序列,X是输入变量,也就是我们得到的需要标注的观测序列.
    我们利用训练数据集通过极大似然估计或正则化的极大似然估计得到条件概率模型,
    在预测时,我们根据给定的输入序列,求出条件概率最大的输出序列.
    CRF是一种判别式模型。
    NER（命名实体识别）这个任务用到的是线性链条件随机场。
    线性链条件随机场的形式是这样的，观测点是你要标注的这些词本身和他们对应的特征，例如说词性是不是专有名词、语义角色是不是主语之类的。
    隐节点，是这些词的标签，比如说是不是人名结尾，是不是地名的开头这样。
    CRF不仅解决了HMM输出独立性假设的问题，还解决了MEMM的标注偏置问题，MEMM容易陷入局部最优是因为只在局部做归一化，
    而CRF统计了全局概率，在做归一化时考虑了数据在全局的分布，而不是仅仅在局部归一化，这样就解决了MEMM中的标记偏置的问题。
    使得序列标注的解码变得最优解。
    【条件随机场】其实分为两个关键词【条件】和【随机场】，【条件】对应于【条件概率】。

### HMM和CRF的三个基本问题
    序列、参数、概率，这三因素，知道其中的两个，求解另外一个，就演化为了三个基本问题。
    求概率->概率问题；求参数->学习问题；求序列->预测问题。
    至于序列，HMM与CRF的是一样的，都有个观测序列和状态序列；至于参数，因两个不同的模型，具体参数多少是有些不同的；而至于概率，一个是联合分布概率，一个是条件概率。
| 模型 | 概率计算问题 | 预测问题 | 学习问题 |
| ------ | ------ | ------ | ------ |
| 定义 | 已知观测序列、模型参数，求观察序列Q的概率 | 已知模型参数、观测序列、观测序列条件概率，求解状态系列 | 已知观测序列，求解隐状态参数λ=(A,B,π)，使得在模型中观测序列发生的可能性P(Q|λ)最大 |
| HMM | 暴力计算法、前向算法、后向算法 | 近似算法、维特比算法 | 极大似然估计 |
| CRF | 前向算法、后向算法 | 维特比算法 | 梯度下降法、拟牛顿法 |

### 生成式模型(Generative Model)
    无穷样本 -> 概率密度模型 = 产生式模型 -> 预测
    通常，生成式模型计算联合概率分布
    基本思想是首先建立样本的概率密度模型，再利用模型进行推理预测。要求已知样本无穷或尽可能的大限制。
    训练阶段是只对 P(X,Y)建模，需要确定维护这个联合概率分布的所有的信息参数。在预测阶段再对新的样本计算P(X,Y)，导出Y
    生成模型是评估给定输出Y，如何从概率分布上生成输入序列 X 。
    估计的是联合概率分布，P(Y, X)=P(Y|X)*P(X)，由联合概率密度分布 P(X,Y)，然后求出条件概率分布 P(Y|X) 作为预测的模型，即生成模型公式为：P(Y|X)= P(X,Y)/ P(X)。
    基本思想是首先建立样本的联合概率密度模型 P(X,Y)，然后再得到后验概率 P(Y|X)，再利用它进行分类，其主要关心的是给定输入 X 产生输出 Y 的生成关系。

### 判别式模型(Discriminative Model)
    有限样本 -> 判别函数 = 判别式模型 -> 预测
    通常，判别式模型计算条件概率
    基本思想是有限样本条件下建立判别函数，不考虑样本的产生模型，直接研究预测模型。
    判别模型是直接对 P(Y|X)建模，就是说，直接根据X特征来对Y建模训练。
    判别模型是给定输入序列 X，直接评估对应的输出 Y。
    估计的是条件概率分布， P(Y|X)，是给定观测变量 X 和目标变量 Y 的条件模型。
    由数据直接学习决策函数 Y=f(X) 或者条件概率分布 P(Y|X) 作为预测的模型，其主要关心的是对于给定的输入 X，应该预测什么样的输出 Y。
    监督学习方法又分生成方法（Generative approach）和判别方法（Discriminative approach），所学到的模型分别称为生成模型（Generative Model）和判别模型（Discriminative Model）。简称GM和DM。

### LDA（Latent Dirichlet Allocation）
    LDA（Latent Dirichlet Allocation）是一种文档主题生成模型，也称为一个三层贝叶斯概率模型，包含词、主题和文档三层结构。
    所谓生成模型，就是说，我们认为一篇文章的每个词都是通过“以一定概率选择了某个主题，
    并从这个主题中以一定概率选择某个词语”这样一个过程得到。文档到主题服从多项式分布，主题到词服从多项式分布。
    LDA的目的就是要识别主题，即把文档—词汇矩阵变成文档—主题矩阵（分布）和主题—词汇矩阵（分布）


### 关系抽取的流水线模型(Pipline Method)
    Pipline Method，流水线方法：输入一个句子，首先进行命名实体识别，然后对识别出来的实体进行两两组合，再进行关系分类。
    流水线的方法存在蛮大的缺点，例如：
    1.错误传播，实体识别模块的错误会传播到后面的分类模块；
    2.忽略了两个子任务之间存在的关系。例如“中国的首都是北京”的例子，如果存在“首都”关系，那么前一个实体必然是国家类别，后一个实体比如是城市类别。流水线的方法，忽略了这些信息；
    3.产生了没必要的冗余信息，由于需要对识别出来的实体进行两两配对，然后再进行关系分类；那些没有关系的实体对就会产生多余的信息，提高错误率。

### 关系抽取的联合抽取模型(Joint Method)
    Joint Method，即联合抽取方法，则跟流水线的方法不同，基于流水线方法的诸多缺陷，Joint Method能够通过一个实体识别和关系分类的联合模型，直接得到有关系的实体三元组。
    Joint Method主要分为两个流派，基于参数共享(Parameter Sharing)和基于标注策略（Tagging Policy）两类。
    基于参数共享(Parameter Sharing)的联合抽取方法，每个词都会被映射到一个实体标记（BILOS:Begin Inside Last Outside Single)，它包含了该字在实体中的位置信息。
    NER模块没有用CRF，而是额外用了一层LSTM来解码双向LSTM编码出来的Hidden state，并建模它和实体标记之间的关系。关系分类模块采用CNN模型，处理BiLSTM的Hidden state并输出关系类别。
    基于标注策略（Tagging Policy）的联合抽取方法，将实体识别和关系分类两个问题，转化为一个序列标注的问题，然后通过一个端对端的神经网络模型直接得到关系实体三元组。
    这种标注策略主要由下图中三部分组成：
    1）实体中词的位置信息{B（实体开始），I（实体内部），E（实体结尾），S（单个实体）}；
    2）关系类型信息{根据预先定义的关系类型进行编码}；
    3）实体角色信息{1（实体1），2（实体2）}。注意，这里只要不是实体关系三元组内的词全部标签都为"O"。
    如`北京是中国首都。`,其系列标注如下：
    北:B-首都-1
    京:E-首都-1
    是:O
    中:B-首都-2
    国:E-首都-1
    首:O
    都:O
    。:O
    "B-首都-1"表示这个词是一个实体的begin，同时这个实体属于关系`首都`的第一个实体。
    但这种方法也有些缺点，如"一个s,多个(p,o)","多个s,一个(p,o)","多个s,多个(p,o)","同一对(s,o)多种关系p","s,o出现重叠的情况"，估计得改成多标签多分类的预测问题了。

### cbow
    在cbow方法中，是用周围词预测中心词，利用中心词的预测标签，使用梯度下降(Gradient Descent)方法，不断的去调整周围词的向量。
    当训练完成之后，每个词都会作为中心词，把周围词的词向量进行了调整，这样也就获得了整个文本里面所有词的词向量。
    要注意的是， cbow的对周围词的调整是统一的：求出的梯度(gradient)会同样的作用到每个周围词的词向量当中去。
    cbow预测行为的次数跟整个文本的词数几乎是相等的,每次预测行为才会进行一次反向传播算法(BackPropagation)，复杂度大概是O(V);

### skip-gram
    skip-gram是用中心词来预测周围的词。在skip-gram中，会利用对周围词的预测结果情况，使用梯度下降(Gradient Descent)来不断的调整中心词的词向量，最终所有的文本遍历完毕之后，也就得到了文本所有词的词向量。
    可以看出，skip-gram进行预测的次数是要多于cbow的：因为每个词在作为中心词时，都要使用周围词进行预测一次。这样相当于比cbow的方法多进行了K-1次（假设K为窗口大小），因此时间的复杂度为O(KV)，训练时间要比cbow要长。
    但是在skip-gram当中，每个词都要收到周围的词的影响，每个词在作为中心词的时候，都要进行K次的预测、调整。
    因此， 当数据量较少，或者词为生僻词出现次数较少时， 这种多次的调整会使得词向量相对的更加准确。
    因为尽管cbow从另外一个角度来说，某个词也是会受到多次周围词的影响，但是他的调整是跟周围的词一起调整的，梯度(gradient)会平均分到该词上， 相当于该生僻词没有收到专门的训练，它只是沾了周围词的光而已。

### True Positive(真正，TP)
    将正类预测为正类数
    
### True Negative(真负，TN)
    将负类预测为负类数
    
### False Positive(假正，FP)
    将负类预测为正类数→误报 (Type I error)
    
### False Negative(假负，FN)
    将正类预测为负类数→漏报 (Type II error)

### 混淆矩阵(Confusion Matrix)
                            预测为正                                  预测为负
    真实值为正               TP(True Positive)正类预测为正              FN(False Negative)正类预测为负，漏报
    真实值为负               FP(False Positive)将负类预测为正，误报      TN(True Negative)将负类预测为负

### 刺激反应矩阵,Stimuli response matrix
                                Response: Different (yes)       Response: Same (no)
    Stimuli: YES (different)    HIT(命中)                         MISS(漏报)
    Stimuli: NO (same)          FALSE ALARM(FA,误报)            CORRECT REJECTION(CR,正确否定)
    其中：hit rate(H) = HIT/(HIT+MISS)
    false alarm rate(F) = FALSE ALARM/(FALSE ALARM + CORRECT REJECTION)
    d-prime = z(H) - z(F)

### 准（正）确率（Accuracy）
    准确率反映分类器或者模型对整体样本判断正确的能力，即能将阳性（正）样本positive判定为positive和阴性（负）样本negative判定为negative的正确分类能力。
    值越大，性能越好。但需注意，在正负类极不均衡样本中，不能单纯追求准确率，因为将所有样本都判定为负样本，这种情况下准确率也是非常高的。
    准确率(accuracy)计算公式为：acc = (真正+真负)/(真正+真负+假真+假负) = (TP+TN)/(TP+TN+FP+FN)

### 精确率(precision)，查准率(P)
    精确率反映分类器或者模型正确预测正样本精度的能力，即预测的正样本中有多少是真实的正样本。值越大，性能越好
    precision=TP/(TP+FP)
    这里注意，单纯追求精确率，会造成分类器或者模型少预测为正样本，这时FP(误报)低，即精确率就会很高。

### 召回率recall，也称为真阳率（TPR）、命中率（hit rate） ，查全率(R)
    召回率反映分类器或者模型正确预测正样本全度的能力，增加将正样本预测为正样本，即正样本被预测为正样本占总的正样本的比例。
    值越大，性能越好；但同样注意，单纯追求召回率，会造成分类器或者模型基本都预测为正样本，这时FN(漏报)低，即召回率就会很高。
    TPR：在所有实际为阳性的样本中，被正确地判断为阳性之比率。TPR=TP/(TP+FN)

### 误报率(false alarm)，也称为假阳率、虚警率、误检率、伪阳性率（FPR）
    误报率反映分类器或者模型正确预测正样本纯度的能力，即负样本被预测为正样本占总的负样本的比例。
    值越小，性能越好；但同样要注意漏报率。
    FPR：在所有实际为阴性的样本中，被错误地判断为阳性之比率。
    FPR=负样本预测为正(误报)/(负样本预测为正+负样本预测为负) = FP/(FP+TN)
    
### 漏报率(miss rate)，也称为漏警率、漏检率（FNR）
    漏报率反映分类器或者模型正确预测负样本纯度的能力，即正样本被预测为负样本占总的正样本的比例。
    值越小，性能越好；但需同样注意误报率。
    FNR = 正样本预测为负(漏报)/(正样本预测为正+正样本预测为负) = FN/(TP+FN)
    TPR + FNR = 召回率+漏报率 = 1

### 特异度，特异性specificity(TNR)
    反映分类器或者模型正确预测负样本全度的能力，即负样本被预测为负样本占总的负样本的比例。
    值越大，性能越好；但同样需要注意漏报率；
    TNR = 负样本预测为负/(负样本预测为正+负样本预测为负) = TN/(FP+TN)
    FPR + TNR = 误报率 + 特异度 = 1

### F值，F-score
    F值是精确率precision和召回率recall的加权调和平均。
    值越大，性能越好。F值可以平衡precision少预测为正样本和recall基本都预测为正样本的单维度指标缺陷。
    计算公式如下：
    F(α)-Score = ((α*α+1)*precision*recall)/(α*α*precision+recall))
    常用的是F1-Score，即a=1，所以上述公式转化为：
    F1-Score = (2*precision*recall)/(precision+recall)
    当α>1时查全率重要，α<1时查准率重要。
    此外还有F2分数和F0.5分数。F1分数认为召回率和精确率同等重要，F2分数认为召回率的重要程度是精确率的2倍即α取2，而F0.5分数认为召回率的重要程度是精确率的一半即α取0.5。

### AP&mAP
    P-R曲线：横坐标为查全率，纵坐标为查准率，而绘制曲线；
    AP表示precision-recall曲线（P-R曲线）下的面积，mAP是mean average precision的简称，是各类别AP的平均值。

### ROC曲线
    ROC的全称是Receiver Operating Characteristic Curve,也称“受试者工作特征曲线”,或者感受性曲线,
    ROC来说，横坐标就是误报率(FPR)，而纵坐标就是召回率(TPR)，当 TPR越大，而FPR越小时，说明分类结果是较好的。
    ROC本质上就是在设定某一阈值之后，计算出该阈值对应的TPR & FPR，便可以绘制出ROC上对应的一个点，当设定若干个阈值之后，便可以连成ROC曲线，因此可以想见，当所采样的阈值越多，ROC Curve越平滑。
       
### AUC
    AUC：Aera Under Curve，即ROC曲线下的面积
    AUC值表示用不同阈值（threshold）下召回率(TPR)为纵坐标轴与误报率(FPR)为横坐标轴连成的ROC曲线下方的面积。
    AUC值越高,模型对于正负样本的区分能力越强,效果越好。
    从左下角到右上角移动过程中，随着阀值的逐渐变化，误报率和召回率都增加，越来越多的正样本会被判定为正样本，但也伴随着负样本被判定为正样本。所以TPR和FPR都会增大。
    这个面积显然不会大于1，又因为ROC曲线一般都在y=x这条直线上方，所以AUC的值域为(0, 1)
    使用AUC作为评价指标是因为很多时候我们并不能够从ROC曲线上清晰准确地判断哪个分类器的性能更好，而作为一个数值，AUC越大，对应的分类器的性能越好。
    其物理意义可以表示为：随机给定一正一负两个样本，将正样本排在负样本之前的概率，因此AUC越大，说明正样本越有可能被排在负样本之前，即分类额结果越好。

### GAUC
    GAUC（group auc）实际是计算每个用户的auc，然后加权平均，最后得到group auc

### AUPRC
    AUPRC 是指精确率-召回率曲线下方面积。该指标计算不同概率阈值的精度率-召回率对。
    精确率-召回率曲线的下方面积，通过为分类阈值的不同值绘制（召回率、精确率）点获得。根据计算方式，PR AUC 可能相当于模型的平均精确率。
    def plot_prc(name, labels, predictions, **kwargs):
        precision, recall, _ = sklearn.metrics.precision_recall_curve(labels, predictions)
        plt.plot(precision, recall, label=name, linewidth=2, **kwargs)
        plt.xlabel('Precision')
        plt.ylabel('Recall')
        plt.grid(True)
        ax = plt.gca()
        ax.set_aspect('equal')
    colors = plt.rcParams['axes.prop_cycle'].by_key()['color']
    plot_prc("Train Baseline", train_labels, train_predictions_baseline, color=colors[0])
    plot_prc("Test Baseline", test_labels, test_predictions_baseline, color=colors[0], linestyle='--')
    plt.legend(loc='lower right');

### DET曲线即Detection error tradeoff (DET) curve，检测误差权衡曲线
    DET曲线即Detection error tradeoff (DET) curve，检测误差权衡曲线。功能类似于ROC曲线，但有时DET曲线更容易判断分类器的性能。
    绘制DET曲线通常是在正态偏差尺度下绘制的，因此绘制之前需要进行数据尺度变换。
    检测错误权衡图（DET）是刻画二分类系统的分类错误率的一种图表，表示错误拒绝率（false  reject  rate）和错误接受率（false  accept  rate）之间的关系，x和y轴一般用log坐标轴表示。
    ```python
    from sklearn.metrics import det_curve, DetCurveDisplay
    import scipy as sp
    import matplotlib.pyplot as plt
    fpr_det, fnr_det, thresholds_det = det_curve(y_true, y_score, pos_label=1)
    display = DetCurveDisplay(fpr=fpr_det, fnr=fnr_det)
    display.plot()
    plt.show()
    ```

### d-prime 指标
    事实上，在性能比较好的二分类器中，ROC 曲线往往很接近角落，AUC 指标也会很接近 1，难以显示出不同系统之间的差距。
    出于跟 DET 曲线一样的动机，实际中也常常用标准正态分布累积密度函数的反函数对 AUC 进行非线性伸缩，得出的指标称为 d-prime。

### KS曲线
    KS曲线是两条线，其横轴是阈值，纵轴是TPR与FPR。两条曲线之间之间相距最远的地方对应的阈值，就是最能划分模型的阈值。
    KS曲线横轴的指标，是阈值（Threshold）。KS曲线中有两条线，这两条线有共同的横轴，但是纵轴分别有两个指标：FPR与TPR。

### KS值
    KS值是MAX(TPR - FPR），即两曲线相距最远的距离。
    
### WOE
    WOE的全称是“Weight of Evidence”，即证据权重。WOE是对原始自变量的一种编码形式。
    要对一个变量进行WOE编码，需要首先把这个变量进行分组处理（也叫离散化、分箱等等，说的都是一个意思）。
    响应客户（风险模型中，对应的是违约客户，总之，指的是模型中预测变量取值为“是”或者说1的个体）;
    WOE表示的实际上是“当前分组中响应客户占所有响应客户的比例”和“当前分组中没有响应的客户占所有没有响应的客户的比例”的差异。
    WOE也可以这么理解，他表示的是当前这个组中响应的客户和未响应客户的比值，和所有样本中这个比值的差异。这个差异是用这两个比值的比值，再取对数来表示的。WOE越大，这种差异越大，这个分组里的样本响应的可能性就越大，WOE越小，差异越小，这个分组里的样本响应的可能性就越小。
    当前分组中，响应的比例越大，WOE值越大；
    当前分组WOE的正负，由当前分组响应和未响应的比例，与样本整体响应和未响应的比例的大小关系决定，当前分组的比例小于样本整体比例时，WOE为负，当前分组的比例大于整体比例时，WOE为正，当前分组的比例和整体比例相等时，WOE为0。
    WOE的取值范围是全体实数。

### IV值
    IV的全称是Information Value，中文意思是信息价值，或者信息量。
    我们需要一些具体的量化指标来衡量每自变量的预测能力，并根据这些量化指标的大小，来确定哪些变量进入模型。IV就是这样一种指标，他可以用来衡量自变量的预测能力。类似的指标还有信息增益、基尼系数等等。
    对于变量的一个分组，这个分组的响应和未响应的比例与样本整体响应和未响应的比例相差越大，IV值越大，否则，IV值越小；
    极端情况下，当前分组的响应和未响应的比例和样本整体的响应和未响应的比例相等时，IV值为0；
    IV值的取值范围是[0,+∞)，且，当当前分组中只包含响应客户或者未响应客户时，IV = +∞。

### 集成学习（Ensemble Learning）
    集成学习（Ensemble Learning），就是使用一系列学习器进行学习，并使用某种规则将各个学习器的结果进行整合，从而获得比单个学习器效果更好的学习效果的一种方法。
    通过集成学习提高分类器的整体泛化能力有以下两个条件：
    1、基分类器之间具有差异性。如果使用的是同一个分类器集成，集成分类器的性能是不会有提升的。
    2、每个基分类器的分类精度必须大于0.5。当基分类器精度小于0.5时，随着集成规模的增加，分类集成分类器的分类精度会下降；但是如果基分类器的精度大于0.5时，集成分类器的最终分类精度是趋近于1的。
    集成学习常见的三种元算法是Bagging, Boosting和Stacking。Bagging用于提升机器学习算法的稳定性和准确性。
    Boosting主要用于减少bias（偏差）和variance（方差），是将一个弱分类器转化为强分类器的算法。Stacking是一种组合多个模型的方法。

### 弱分类器
    弱分类器，weak classifier, 是指分类器仅能对少量样本进行正确分类，其分类效果仅略优于随机猜测。

### 强分类器
    强分类器，strong classifier, 是指对样本分类的正确率很高的分类器。

### 有放回采样
    有放回采样，sampling with replacement。对于n个样本的训练集T进行采样，每次采用得到的样本被放回原训练集T中，下次对训练集T进行采样时该样本仍有可能被采集到。

### 无放回采样
    无放回采样，sampling without replacement。对于n个样本的训练集T进行采样，每次采样得到的样本不再被放回原训练集T中，以后对训练集T进行采样时，这个样本以后都不会被采样到。

### 分层采样(Stratification, stratified sampling) 
    保留类别比例的采样方式。
    先将总体的单位按某种特征分为若干次级总体（层），然后再从每一层内进行单纯随机抽样，组成一个样本。分层可以提高总体指标估计值的精确度，它可以将一个内部变异很大的总体分成一些内部变异较小的层（次总体）。
    当数据集很大时(尤其是和属性数相比)，纯随机的取样方法通常可行;但如果数据集不大，就会有采样偏差的风险。
    比如调查公司想要对 1000 个人进行调查。调查公司要保证这 1000 个人对人群整体有代表性。例如，美国人口的 51.3% 是女性，48.7% 是男性。
    所以在美国， 严谨的调查需要保证样本也是这个比例:513 名女性，487 名男性。这称作分层采样 (stratified sampling)
    将人群分成均匀的子分组，称为分层，从每个分层去取合适数量的实例，以保证测试集对总人数有代表性。
    可以使用scikit-learn库中的StratifiedShuffleSplit方法对数据集进行分层采样。
    StratifiedShuffleSplit方法可以根据数据集中各类别的比例，将数据集按照指定的比例随机划分为训练集和测试集，从而保证训练集和测试集中各类别的比例与原始数据集中相同。

### Bagging集成
    Bagging是通过结合几个模型降低泛化误差的技术。
    主要想法是分别训练几个不同的模型，然后让所有模型表决测试样例的输出。 
    采用这种策略的技术被称为集成方法。模型平均奏效的原因是不同的模型通常不会在测试集上产生完全相同的误差。
    如果单模型间的误差是独立的，集成将显著地比单个模型表现得更好。
    Bagging (Bootstrap Aggregating, 引导聚合)
    bagging，对训练集采用有放回采样。通过对原数据集进行有放回的采样，构建出大小和原数据集T一样的新数据集T1，T2，T3…，然后用这些新的数据集训练多个分类器f1，f2，f3…。最终分类结果根据这些分类器各自结果的投票来决定。
    bagging的性能依赖于基分类器的稳定性，如果基分类器是不稳定的，bagging有助于减低训练数据的随机扰动导致的误差，但是如果基分类器是稳定的，即对数据变化不敏感，那么bagging方法就得不到性能的提升，甚至会减低。
    基于Bagging算法的一个典型例子：随机森林，采用的基分类器是决策树。

### Boosting集成
    Boosting，是一个迭代的过程，每次在新分类器中强调上一个分类器中被错误分类的样本（增加错误分类样本的权重），最后将这些模型组合起来的方法。
    每次对正确分类的样本降权，对错误分类的样本加权，最后分类器是多个弱分类器的加权组合。Boosting没有对样本进行重采样，而是对样本的分布进行了调整。
    boosting算法中，基分类器之间存在强依赖关系，基分类器需要串行生成。
    Boosting算法伪代码
    1、在训练集T上，训练一个弱分类器；
    2、根据上一步的结果对训练集T进行权值调整，训练集T中数据被赋予新的权值：对错分的样本数据增加权重，对正确分类的样本数据进行降低权重；得到权值调整后，更新好的训练集T’；
    3、在权值调整后的训练集T’上，进行弱分类器的学习训练；
    4、迭代步骤2，直到弱学习器数达到事先指定的数目X
    5、最终将这X个弱学习器通过结合策略进行整合，得到最终的强学习器。

### Stacking (Stacked generalization)集成
    Stacking的基本思想是训练一个基本分类器池，然后使用另一个分类器来组合它们的预测，目的是减少泛化误差。
    Stacking算法伪代码
    1、将训练集分成两个不想交的部分；
    2、在第一部分的训练集上训练若干个基本学习器；
    3、在第二部分的训练集上测试得到的基本学习器；
    4、使用步骤3中的预测结果作为输入，将正确的响应（responses）作为输出，训练更高级别的学习器。
    步骤1到3，类似与交叉验证（cross-validation），但是不同于winner-takes-all的策略，stacking通过组合基本分类器来得到更高级的学习结果。
    组合算法（combiner algorithm）使用所有其他算法的预测作为附加输入（additional inputs）来训练得到最终的预测结果。
    理论上可以表示任何一种组合学习方法（ensemble techniques）；实际中，单层的逻辑回归模型（single-layer logistic regression model）通常被用作组合器（combiner）。
    使用Stacking后，效果不佳？
    a、本身数据集就不大，stacking不适合极小的数据集。——没有必要坚持用stacking。
    b、第一层的模型不够准确，或有交差的模型拖后腿——观察每个子模型，移除那些明显逊色的模型。
    c、基分类器数量少——丰富基分类器的种类。如果想不出新的模型，可以换不同参数的类似模型。
    d、第一层的信息捕捉有遗漏——尝试下在第二层加入原始数据

    

