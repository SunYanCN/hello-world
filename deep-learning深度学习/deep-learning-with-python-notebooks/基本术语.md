
### 类(class)
    在机器学习中,分类问题中的某个类别叫作类(class)。

### 样本(sample)
    数据点叫作样本(sample)。

### 标签(label)
    某个样本对应的类叫作标签(label)。

### 张量(tensor)
    是一个数据容器。它包含的数据几乎总是数值数据,因此它是数字的容器。你可能对矩阵很熟悉,它是二维张量。
    张量是矩阵向任意维度的推广[注意,张量的维度(dimension)通常叫作轴(axis)]。

### 维度(dimension)
    可以表示沿着某个轴上的元素个数(比如 5D 向量),也可以表示张量中轴的个数(比如 5D 张量),这有时会令人感到混乱。
    对于后一种情况,技术上更准确的说法是 5 阶张量(张量的阶数即轴的个数),但 5D 张量这种模糊的写法更常见。

### 轴(axis)
    轴用来为超过一维的数组定义的属性，二维数据拥有两个轴：第0轴沿着行的垂直往下，第1轴沿着列的方向水平延伸。
    axis取值说明：一维数组时axis=0，二维数组时axis=0，1，维数越高，则axis可取的值越大，数组n维时，axis=0，1，…，n-1。
    在numpy中数组都有着[]标记，则axis=0对应着最外层的[]，axis=1对应第二外层的[]，以此类推，axis=n对应第n+1外层的[]。
    a = np.array([[1,2,3],[4,5,6]]), axis = 0表示对最外层[]里的最大单位块做块与块之间的运算, 即对[1,2,3]与[4,5,6]，做运算；若是做drop，则表示对[1,2,3]，或[4,5,6]，做drop;
    axis = 1, 表示对次外层[]的最大单位块与块之间做运算，或块drop操作。比如，第一行则是 1,2,3之间做运算，drop,则是针对1或2或3做drop，因为其他行也是类似操作，表现出来的效果就是对列进行操作。
    p = pd.DataFrame([[1,2,3], [4,5,6]])
    p.drop(0, axis=1)
    Out[13]: 
       1  2
    0  2  3
    1  5  6
    p.drop(0, axis=0)
    Out[14]: 
       0  1  2
    1  4  5  6
    d = np.array([[1,2,3], [4,5,6]])
    d.sum(axis=0)
    Out[16]: array([5, 7, 9])
    d.sum(axis=0)
    Out[17]: array([5, 7, 9])

### 样本轴
    通常来说,深度学习中所有数据张量的第一个轴(0 轴,因为索引从 0 开始)都是样本轴(samples axis,有时也叫样本维度)

### 批量
    深度学习模型不会同时处理整个数据集,而是将数据拆分成小批量。
    具体来看,下面是 MNIST 数据集的一个批量,批量大小为 128。
    batch = train_images[:128]
    然后是下一个批量。
    batch = train_images[128:256]
    然后是第 n 个批量。
    batch = train_images[128 * n:128 * (n + 1)]
    对于这种批量张量,第一个轴(0 轴)叫作批量轴(batch axis)或批量维度(batch dimension)。

### 向量数据
    2D 张量,形状为 (samples, features) 。
    对于这种数据集,每个数据点都被编码为一个向量,因此一个数据批量就被编码为 2D 张量(即向量组成的数组),其中第一个轴是样本轴,第二个轴是特征轴。
    
### 时间序列数据或序列数据
    3D 张量,形状为 (samples, timesteps, features) 。
    当时间(或序列顺序)对于数据很重要时,应该将数据存储在带有时间轴的 3D 张量中。
    每个样本可以被编码为一个向量序列(即 2D 张量),因此一个数据批量就被编码为一个 3D 张量。
    根据惯例,时间轴始终是第 2 个轴(索引为 1 的轴)。

### 图像
    4D 张量,形状为 (samples, height, width, channels) 或 (samples, channels,height, width) 。
    图像通常具有三个维度:高度、宽度和颜色深度。
    虽然灰度图像(比如 MNIST 数字图像)只有一个颜色通道,因此可以保存在 2D 张量中,但按照惯例,图像张量始终都是 3D 张量,灰度图像的彩色通道只有一维。
    因此,如果图像大小为 256×256,那么 128 张灰度图像组成的批量可以保存在一个形状为 (128, 256, 256, 1) 的张量中,
    而 128 张彩色图像组成的批量则可以保存在一个形状为 (128, 256, 256, 3) 的张量中。
    图像张量的形状有两种约定:通道在后(channels-last)的约定(在 TensorFlow 中使用)和通道在前(channels-first)的约定(在 Theano 中使用)。
    Google 的 TensorFlow 机器学习框架将颜色深度轴放在最后: (samples, height, width, color_depth) 。
    与此相反,Theano将图像深度轴放在批量轴之后: (samples, color_depth, height, width) 。
    如果采用 Theano 约定,前面的两个例子将变成 (128, 1, 256, 256) 和 (128, 3, 256, 256) 。
    Keras 框架同时支持这两种格式。

### 视频
    5D 张量,形状为 (samples, frames, height, width, channels) 或 (samples,frames, channels, height, width) 。
    视频数据是现实生活中需要用到 5D 张量的少数数据类型之一。视频可以看作一系列帧,每一帧都是一张彩色图像。
    由于每一帧都可以保存在一个形状为 (height, width, color_depth) 的 3D 张量中,
    因此一系列帧可以保存在一个形状为 (frames, height, width, color_depth) 的 4D 张量中,
    而不同视频组成的批量则可以保存在一个 5D 张量中,其形状为(samples, frames, height, width, color_depth) 。

### 广播(broadcast)
    将一个 2D 张量与一个向量相加。如果将两个形状不同的张量相加,如果没有歧义的话,较小的张量会被广播(broadcast),以匹配较大张量的形状。
    广播包含以下两步。
    (1) 向较小的张量添加轴(叫作广播轴),使其 ndim 与较大的张量相同。
    (2) 将较小的张量沿着新轴重复,使其形状与较大的张量相同。

### 张量点积
    点积运算,也叫张量积(tensor product,不要与逐元素的乘积弄混),是最常见也最有用的张量运算。
    与逐元素的运算不同,它将输入张量的元素合并在一起。
    注意,两个向量之间的点积是一个标量,而且只有元素个数相同的向量之间才能做点积。
    对一个矩阵 x 和一个向量 y 做点积,返回值是一个向量,其中每个元素是 y 和 x的每一行之间的点积。
    a·b=|a|×|b|cosθ；（θ为a,b之间的夹角）

### 张量变形(tensor reshaping)
    张量变形是指改变张量的行和列,以得到想要的形状。变形后的张量的元素总个数与初始张量相同。
    简单的例子可以帮助我们理解张量变形。
    >>> x = np.array([[0., 1.],
                                    [2., 3.],
                                    [4., 5.]])
    >>> print(x.shape)
    (3, 2)
    >>> x = x.reshape((6, 1))
    >>> x
    array([[ 0.],
    [ 1.],
    [ 2.],
    [ 3.],
    [ 4.],
    [ 5.]])
    >>> x = x.reshape((2, 3))
    >>> x
    array([[ 0., 1., 2.],
                [ 3., 4., 5.]])

### 阶(rank)
    张量轴的个数也叫作阶(rank)。
    例如,3D 张量有 3 个轴,矩阵有 2 个轴。这在 Numpy 等 Python 库中也叫张量的 ndim 。
    >>> x = np.random.random((3, 2))
    >>> x
    array([[0.8957804 , 0.96712402],
           [0.73855227, 0.34115149],
           [0.36667734, 0.16249242]])
    >>> x.ndim
    2

### 形状
    这是一个整数元组,表示张量沿每个轴的维度大小(元素个数)。
    例如,前面矩阵示例的形状为 (3, 5) ,3D 张量示例的形状为 (3, 3, 5) 。
    向量的形状只包含一个元素,比如 (5,) ,而标量的形状为空,即 () 。
    >>> x = np.array(3)
    >>> x.shape
    ()
    >>> x = np.array([2,3])
    >>> x.shape
    (2,)

### 数据类型(在 Python 库中通常叫作 dtype )
    这是张量中所包含数据的类型,例如,张量的类型可以是 float32 、 uint8 、 float64 等。
    在极少数情况下,你可能会遇到字符( char )张量。
    注意,Numpy(以及大多数其他库)中不存在字符串张量,因为张量存储在预先分配的连续内存段中,而字符串的长度是可变的,无法用这种方式存储。
    >>> x = np.array([2.0,3])
    >>> x.dtype
    dtype('float64')

### 标量(scalar, 0D 张量)
    仅包含一个数字的张量叫作标量(scalar,也叫标量张量、零维张量、0D 张量)。
    在 Numpy中,一个 float32 或 float64 的数字就是一个标量张量(或标量数组)。
    你可以用 ndim 属性来查看一个 Numpy 张量的轴的个数。标量张量有 0 个轴( ndim == 0 )。
    张量轴的个数也叫作阶(rank)。下面是一个 Numpy 标量。
    >>> import numpy as np
    >>> x = np.array(12)
    >>> x
    array(12)
    >>> x.ndim
    0

### 向量(vector, 1D 张量)
    数字组成的数组叫作向量(vector)或一维张量(1D 张量)。一维张量只有一个轴。
    下面是一个 Numpy 向量。
    >>> x = np.array([12, 3, 6, 14, 7])
    >>> x
    array([12, 3, 6, 14, 7])
    >>> x.ndim
    1
    
    这个向量有 5 个元素,所以被称为 5D 向量。不要把 5D 向量和 5D 张量弄混!
    5D 向量只有一个轴,沿着轴有 5 个维度,而 5D 张量有 5 个轴(沿着每个轴可能有任意个维度)。

### 矩阵(matrix, 2D 张量)
    向量组成的数组叫作矩阵(matrix)或二维张量(2D 张量)。
    矩阵有 2 个轴(通常叫作行和列)。你可以将矩阵直观地理解为数字组成的矩形网格。
    下面是一个 Numpy 矩阵。
    >>> x = np.array([[5, 78, 2, 34, 0],
                                    [6, 79, 3, 35, 1],
                                    [7, 80, 4, 36, 2]])
    >>> x.ndim
    2
    第一个轴上的元素叫作行(row),第二个轴上的元素叫作列(column)。
    在上面的例子中,[5, 78, 2, 34, 0] 是 x 的第一行, [5, 6, 7] 是第一列。

### 3D 张量
    将多个矩阵组合成一个新的数组,可以得到一个 3D 张量,你可以将其直观地理解为数字组成的立方体。
    下面是一个 Numpy 的 3D 张量。
    >>> x = np.random.random((3, 2, 4))
    >>> x
    array([[[0.12988825, 0.77277393, 0.46243809, 0.12377541],
            [0.8033865 , 0.81821495, 0.22307274, 0.52566756]],
           [[0.80553673, 0.79450534, 0.14504729, 0.60237352],
            [0.03084524, 0.12027406, 0.45299607, 0.05332909]],
           [[0.68057101, 0.81951978, 0.65863182, 0.85608584],
            [0.68971186, 0.01891051, 0.17315731, 0.02773519]]])
    >>> x.ndim
    3

### 高维张量
    将多个 3D 张量组合成一个数组,可以创建一个 4D 张量,以此类推。
    深度学习处理的一般是 0D 到 4D 的张量,但处理视频数据时可能会遇到 5D 张量。

### 训练集(training set)

### 测试集(test set）

### 权重(weight)或可训练参数(trainable parameter)
    权重是利用随机梯度下降学到的一个或多个张量,其中包含网络的知识。

### 层(layer)
    神经网络的核心组件是层(layer),它是一种数据处理模块,你可以将它看成数据过滤器。
    进去一些数据,出来的数据变得更加有用。具体来说,层从输入数据中提取表示——我们期望这种表示有助于解决手头的问题。
    大多数深度学习都是将简单的层链接起来,从而实现渐进式的数据蒸馏(data distillation)。
    深度学习模型就像是数据处理的筛子,包含一系列越来越精细的数据过滤器(即层)。
    层是一个数据处理模块,将一个或多个输入张量转换为一个或多个输出张量。有些层是无状态的,但大多数的层是有状态的,即层的权重。

### 密集层(dense layer, Dense 层)
    密集连接(也叫全连接)的神经层。
    密集连接层[densely connected layer,也叫全连接层(fully connected layer)或密集层(dense layer),对应于 Keras 的 Dense 类]来处理。

###　全连接层(fully connected layers,FC)
    全连接（Full Connect）的核心操作就是矩阵向量乘积 y = Wx；
    本质就是由一个特征空间线性变换到另一个特征空间。
    目标空间的任一维——也就是隐层的一个 cell——都认为会受到源空间的每一维的影响。
    全连接层中的每个神经元与其前一层的所有神经元进行全连接．
    全连接层（fully connected layers，FC）在整个卷积神经网络中起到“分类器”的作用。
    如果说卷积层、池化层和激活函数层等操作是将原始数据映射到隐层特征空间的话，全连接层则起到将学到的“分布式特征表示”映射到样本标记空间的作用。
    全连接层参数冗余

### 循环层(recurrent layer,比如 Keras 的 LSTM 层)

### 二维卷积层(Keras 的 Conv2D )

### softmax 层
    softmax函数，又称归一化指数函数。它是二分类函数sigmoid在多分类上的推广，目的是将多分类的结果以概率的形式展现出来。
    softmax把一些输入映射为0-1之间的实数，并且归一化保证和为1，因此多分类的概率之和也刚好为1。
    顾名思义，softmax由两个单词组成，其中一个是max。
    另外一个单词为soft(柔和)。如果将max看成一个分类问题，就是非黑即白，最后的输出是一个确定的变量。
    更多的时候，我们希望输出的是取到某个分类的概率，或者说，我们希望分值大的那一项被经常取到，而分值较小的那一项也有一定的概率偶尔被取到，所以我们就应用到了soft的概念，即最后的输出是每个分类被取到的概率。
    假设有一个数组V， Vi表示V中的第i个元素，那么这个元素的softmax值为:Si = e^i/∑j e^j ;
    该元素的softmax值，就是该元素的指数与所有元素指数和的比值。
    softmax层只是对神经网络的输出结果进行了一次换算，将输出结果用概率的形式表现出来。将网络输出值的每一维映射成（0，1）之间的概率值，且所有维的概率值之和等于1。

### 哈夫曼树（Huffman Tree）
    给定N个权值作为N个叶子结点，构造一棵二叉树，若该树的带权路径长度达到最小，称这样的二叉树为最优二叉树，也称为哈夫曼树。
    
### 分层softmax(Hierachical Softmax)
    Hierachical Softmax的基本思想就是首先将词典中的每个词按照词频大小构建出一棵Huffman树，保证词频较大的词处于相对比较浅的层，
    词频较低的词相应的处于Huffman树较深层的叶子节点，每一个词都处于这棵Huffman树上的某个叶子节点；
    第二，将原本的一个|V|分类问题变成了log(V)次的二分类问题，做法简单说来就是，使用的是普通的softmax，
    势必要求词典中的每一个词的概率大小，为了减少这一步的计算量，在Hierachical Softmax中，
    将计算当前词在其上下文中的概率大小，变成在Huffman树中的路径预测问题就可以了，
    因为当前词在Huffman树中对应到一条路径，
    这条路径由这棵二叉树中从根节点开始，经过一系列中间的父节点，最终到达当前这个词的叶子节点而组成，
    那么在每一个父节点上，都对应的是一个二分类问题（本质上就是一个LR分类器），而Huffman树的构造过程保证了树的深度为log(V)，
    所以也就只需要做log(V)次二分类便可以求得概率的大小，这相比原来|V|次的计算量，已经大大减小了。
    
### 负采样
    负采样每遍历到一个目标词，为了使得目标词的概率最大，让其他非目标词的概率最小，
    普通softmax的计算量太大就是因为它把词典中所有其他非目标词都当做负例了，
    而负采样的思想，就是每次按照一定概率随机采样一些词当做负例，从而就只需要计算这些负采样出来的负例了，
    将原来的|V|分类问题变成了K分类问题，这便把词典大小对时间复杂度的影响变成了一个常数项。

### 编译(compile)

### 拟合(fit)

### 过拟合(overfit)
    过拟合是指机器学习模型在新数据上的性能往往比在训练数据上要差

### 标签平滑（Label Smoothing）
    标签平滑是一种损失函数的修正。在训练样本中，我们并不能保证所有的样本标签都标注正确，如果某个样本的标注是错误的，那么在训练时，该样本就有可能对训练结果产生负面影响。
    在每次迭代时，并不直接将(xi,yi)（其中yi是样本xi的标签，为0或1）放入训练集，而是设置一个错误率ε，以1-ε的概率将(xi,yi)代入训练，以ε的概率将(xi,1-yi)代入训练。
    这样，模型在训练时，既有正确标签输入，又有错误标签输入，可以想象，如此训练出来的模型不会“全力匹配”每一个标签，而只是在一定程度上匹配。
    这样，如果真的出现错误标签，模型受到的影响就会更小。
    也就是说，当标签为0时，我们并不把0直接放入训练，而是将其替换为一个比较小的数ε。
    同样的，如果标签为1，我们也将其替换为较接近的数1-ε。
    这样，在交叉熵模型中，模型输出永远不可能达到0和1，因此模型会不断增大w,使得预测输出尽可能逼近0或1，
    而这个过程与正则化是矛盾的，或者说，有可能过拟合。
    如果我们把标签0和1分别替换成ε和1-ε，模型的输出在达到这个值之后，就不会继续优化。
    因此，所谓平滑，指的就是把两个极端值0和1变成两个不那么极端的值。
    但在知识蒸馏教师模型中，不要使用标签平滑。尽管使用标签平滑化训练提高了教师的最终准确性，但与使用“硬”目标训练的教师(没有标签平滑化)相比，它未能向学生网络传递足够多的知识。
    标签平滑“擦除”了在hard目标训练中保留的一些细节。这样的泛化有利于教师网络的性能，但是它传递给学生网络的信息更少。
    丢失的信息最终会对它教授新学生模型的能力产生负面影响。因此，准确性更高的老师并不能更好地向学生提炼信息。

### 损失( loss )

### 精度( acc )

### 损失函数(loss function)
    网络如何衡量在训练数据上的性能,即网络如何朝着正确的方向前进。
    该函数也叫目标函数(objective function)。
    损失函数的输入是网络预测值与真实目标值(即你希望网络输出的结果),然后计算一个距离值,衡量该网络在这个示例上的效果好坏。
    损失函数(目标函数)——在训练过程中需要将其最小化。它能够衡量当前任务是否已成功完成。

### 优化器(optimizer)
    基于训练数据和损失函数来更新网络的机制。
    利用网络预测值与真实目标值的距离值作为反馈信号来对权重值进行微调,以降低当前示例对应的损失值。
    优化器——决定如何基于损失函数对网络进行更新。它执行的是随机梯度下降(SGD)的某个变体。

### 指标(metric)

### 轮次(epoch)
    在所有训练数据上迭代一次叫作一个轮次(epoch)

### 模型的深度(depth)
    数据模型中包含多少层,这被称为模型的深度(depth)。

### logistic 回归(logistic regression,简称 logreg)
    logreg 是一种分类算法,而不是回归算法。
    Logistic回归分析也用于研究影响关系，即X对于Y的影响情况。Y为定量数据，X可以是定量数据或定类数据。
    Logistic回归和线性回归最大的区别在于，Y的数据类型。线性回归分析的因变量Y属于定量数据，而Logistic回归分析的因变量Y属于分类数据。
    Logistic回归在进一步细分，又可分为二元Logit（Logistic）回归(分类数据有且仅有两类)、
    多元无序Logit（Logistic）回归(分类数据超过两类，类别之间没有对比意义)，
    多元有序Logit（Logistic）回归(分类数据超过两类，类别之间具有对比意义，如：非常满意，满意，不满意，非常不满意)。

### 监督学习(supervised learning)
    监督学习是目前最常见的机器学习类型。给定一组样本(通常由人工标注),它可以学会将输入数据映射到已知目标[也叫标注(annotation)]。
    其目标是学习训练输入与训练目标之间的关系。

### 序列生成(sequence generation)
    给定一张图像,预测描述图像的文字。序列生成有时可以被重新表示为一系列分类问题,比如反复预测序列中的单词或标记。

### 语法树预测(syntax tree prediction)
    给定一个句子,预测其分解生成的语法树。

### 目标检测(object detection)
    给定一张图像,在图中特定目标的周围画一个边界框。这个问题也可以表示为分类问题(给定多个候选边界框,对每个框内的目标进行分类)或分类与回归联合问题(用向量回归来预测边界框的坐标)。

### 图像分割(image segmentation)
    给定一张图像,在特定物体上画一个像素级的掩模(mask)

### 降维(dimensionality reduction)

### 聚类(clustering)

### 样本(sample)或输入(input)
    进入模型的数据点。

### 预测(prediction)或输出(output)
    从模型出来的结果。

### 目标(target)
    真实值。对于外部数据源,理想情况下,模型应该能够预测出目标。

### 预测误差(prediction error)或损失值(loss value)
    模型预测与目标之间的距离。

### 误差(error)
    样本误差是指样本对母本(无法观察到的)均值及真实值的均值的偏离. 
    误差:即观测值与真实值的偏离;
    误差与测量有关，误差大小可以衡量测量的准确性，误差越大则表示测量越不准确。
    误差分为两类：系统误差与随机误差。其中，系统误差与测量方案有关，通过改进测量方案可以避免系统误差。
    随机误差与观测者，测量工具，被观测物体的性质有关，只能尽量减小，却不能避免。
    误差: 所有不同样本集的均值,与真实总体均值的偏离.由于真实总体均值通常无法获取或观测到,因此通常是假设总体为某一分布类型,则有N个估算的均值; 表征的是观测/测量的精确度;
    误差大,由异常值引起.表明数据可能有严重的测量错误;或者所选模型不合适,;

### 残差(Residuals)
    残差则是指样本和观察值(样本总体)或回归值(拟合)的差额. 
    残差:观测值与拟合值的偏离.
    残差――与预测有关，残差大小可以衡量预测的准确性。残差越大表示预测越不准确。残差与数据本身的分布特性，回归方程的选择有关。
    残差: 某样本的均值与所有样本集均值的均值的偏离; 表征取样的合理性,即该样本是否具代表意义;
    残差大,表明样本不具代表性,也有可能由特征值引起.
    反正要看一个模型是否合适,看误差;要看所取样本是否合适,看残差;

### 类别(class)
    分类问题中供选择的一组标签。例如,对猫狗图像进行分类时,“狗”和“猫”就是两个类别。

### 标签(label)
    分类问题中类别标注的具体例子。比如,如果 1234 号图像被标注为

### 真值(ground-truth)或标注(annotation)
    数据集的所有目标,通常由人工收集。
### 二分类(binary classification)
    一种分类任务,每个输入样本都应被划分到两个互斥的类别中。

### 多分类(multiclass classification)
    一种分类任务,每个输入样本都应被划分到两个以上的类别中,比如手写数字分类。

### 多标签分类(multilabel classification)
    一种分类任务,每个输入样本都可以分配多个标签。
    举个例子,如果一幅图像里可能既有猫又有狗,那么应该同时标注“猫”标签和“狗”标签。每幅图像的标签个数通常是可变的。

### 标量回归(scalar regression)
    目标是连续标量值的任务。预测房价就是一个很好的例子,不同的目标价格形成一个连续的空间。

### 向量回归(vector regression)
    目标是一组连续值(比如一个连续向量)的任务。如果对多个值(比如图像边界框的坐标)进行回归,那就是向量回归。

### 小批量(mini-batch)或批量(batch)
    模型同时处理的一小部分样本(样本数通常为 8~128)。
    样本数通常取 2 的幂,这样便于 GPU 上的内存分配。
    训练时,小批量用来为模型权重计算一次梯度下降更新。

### 特征图(feature map)
    对于包含两个空间轴(高度和宽度)和一个深度轴(也叫通道轴)的 3D 张量,其卷积也叫特征图(feature map)。
    应该是特征图的意思，是指每个卷积核和输入卷积后形成的特征图，特征图的个数和卷积核的个数相同

### 填充(padding)
    填充(padding)是指在输入高和宽的两侧填充元素(通常是 0 元素)。

### 步幅(stride)
    在卷积神经网络中，卷积窗口从输入数组的最左上方开始,按从左往右、从上往下的顺序,依次在输入数组上滑动。我们将每次滑动的行数和列数称为步幅(stride)。

### 范数(norm)
    距离的定义是一个宽泛的概念，只要满足非负、自反、三角不等式就可以称之为距离。
    范数是一种强化了的距离概念，它在定义上比距离多了一条数乘的运算法则。
    有时候为了便于理解，我们可以把范数当作距离来理解。
    范数是把一个事物映射到非负实数，且满足非负性、齐次性、三角不等式，符合以上定义的都可以称之为范数
    范数包括向量范数和矩阵范数
    不同的范数表示不同的度量方法，就好比米和光年都可以来度量远近一样；

### 向量范数
    向量范数表征向量空间中向量的大小.
    一种非严密的解释就是，对应向量范数，向量空间中的向量都是有大小的，这个大小如何度量，就是用范数来度量的，
    不同的范数都可以来度量这个大小，就好比米和光年都可以来度量远近一样；
    
    L0范数： L0范数表示向量中非零元素的个数。
    1-范数：，即向量元素绝对值之和; np.linalg.norm([1,2,3], ord=1)
    2-范数：，Euclid范数（欧几里得范数，常用计算向量长度），即向量元素绝对值的平方和再开方;
                np.linalg.norm([1,2,3], ord=2)
                Out[13]: 3.7416573867739413
                math.pow(1*1+2*2+3*3, 0.5)
                Out[9]: 3.7416573867739413
    ∞-范数：，即所有向量元素绝对值中的最大值
                np.linalg.norm([1,2,3], ord=np.inf)
                Out[16]: 3.0
    -∞-范数：，即所有向量元素绝对值中的最小值。
                np.linalg.norm([1,2,3], ord=-np.inf)
                Out[17]: 1.0
    p-范数：，即向量元素绝对值的p次方和的1/p次幂。

### 矩阵特征值；特征向量
    设A是n阶方阵，如果数λ和n维非零列向量x使关系式Ax=λx成立，那么这样的数λ称为矩阵A特征值，非零向量x称为A的对应于特征值λ的特征向量。
    式Ax=λx也可写成( A-λE)X=0。这是n个未知数n个方程的齐次线性方程组，它有非零解的充分必要条件是系数行列式| A-λE|=0。
    np.linalg.eig(np.array([[1, 0, 0],
       [0, 5, 0],
       [0, 0, 9]]))
    Out[43]: 
    (array([1., 5., 9.]), array([[1., 0., 0.],
            [0., 1., 0.],
            [0., 0., 1.]]))

### 矩阵范数
    矩阵范数表征矩阵引起变化的大小。
    对于矩阵范数，通过运算AX=BAX=B，可以将向量X变化为B，矩阵范数就是来度量这个变化大小的。
    不同的范数都可以来度量这个大小，就好比米和尺都可以来度量远近一样；
    一个集合（向量），通过一种映射关系（矩阵），得到另外一个集合（另外一个向量）。
    矩阵的范数，就是表示这个变化过程的大小的一个度量。矩阵范数反映了线性映射把一个向量映射为另一个向量，向量的“长度”缩放的比例。
    1-范数：， 列和范数，即所有矩阵列向量绝对值之和的最大值，
                np.linalg.norm([[1,2,3], [4, 5, 6]], ord=1)
                Out[19]: 9.0
    2-范数：，谱范数，即AT·A矩阵的最大特征值的开平方。
                np.linalg.norm([[1,2,3], [4, 5, 6]], ord=2)
                Out[31]: 9.508032000695724
                x = np.array([[1,2,3], [4, 5, 6]])
                x.T
                Out[36]: 
                array([[1, 4],
                       [2, 5],
                       [3, 6]])
                np.dot(x.T, x)
                Out[37]: 
                array([[17, 22, 27],
                       [22, 29, 36],
                       [27, 36, 45]])
                ### 计算特征值及特征向量       
                np.linalg.eig(np.dot(x.T, x))
                Out[38]: 
                (array([9.04026725e+01, 5.97327474e-01, 7.23299057e-16]),
                 array([[-0.42866713, -0.80596391,  0.40824829],
                        [-0.56630692, -0.11238241, -0.81649658],
                        [-0.7039467 ,  0.58119908,  0.40824829]]))
                math.pow(max(np.linalg.eig(np.dot(x.T, x))[0]), 0.5)
                Out[39]: 9.508032000695724
    
    ∞-范数：，行和范数，即所有矩阵行向量绝对值之和的最大值。
    F-范数：，Frobenius范数，即矩阵元素绝对值的平方和再开平方。

### 向量模长
    向量模长； 即向量元素绝对值的平方和再开方;
    向量的第二范数为传统意义上的向量长度
    向量的模，sum(vector**2)**0.5
    math.sqrt(sum(vec**2 for vec in vector))

### 矩阵乘积
    矩阵乘积; 矩阵相乘最重要的方法是一般矩阵乘积。
    它只有在第一个矩阵的列数（column）和第二个矩阵的行数（row）相同时才有意义  。
    np.dot([[1,2,3], [4, 5,6]], [[1,2], [3,4], [5,6]])
    Out[63]: 
    array([[22, 28],
           [49, 64]])

### 点乘运算
    就是对这两个向量对应位一一相乘之后求和的操作; 即：若a=(x1,y1),b=(x2,y2)，则a·b=x1·x2+y1·y2
    np.dot([1,2,3], [4,5,6])
    Out[4]: 32
    1*4 + 2*5 + 3*6
    Out[5]: 32

### np.dot
    1.对于一维数组，其作用同inner
    np.dot([1,2], [3,4])
    Out[75]: 11
    np.inner([1,2], [3,4])
    Out[76]: 11
    
    2.对于二维数组，其作用是矩阵乘法
    np.dot([[1,2,3], [4,5,6]], [[1,0,0], [1,2,3], [4,5,6]])
    Out[69]: 
    array([[15, 19, 24],
           [33, 40, 51]])
     
    对于多维数组，其作用是将数组a的最后轴上的所有元素与数组b的倒数第二轴上的所有元素的乘积和
    np.dot([[    [0, 2, 0],
                 [1,2,3]],
                [[1,2,3], 
                 [4,5,6]] ], 
            [[[1,0,0], 
              [1,1,1], 
              [0, 0, 0]],
            [[1,0,0], 
             [1,2,3], 
             [4,5,6]]])
    Out[80]: 
    array([[[[ 2,  2,  2],
             [ 2,  4,  6]],
            [[ 3,  2,  2],
             [15, 19, 24]]],
           [[[ 3,  2,  2],
             [15, 19, 24]],
            [[ 9,  5,  5],
             [33, 40, 51]]]])

### 点积
    点积，又叫点乘，向量内积、数量积,dot product; scalar product,标量积
    公式：a * b = |a| * |b| * cosθ = a1*b1 + a2*b2 + a3*b3 + ... + an*bn 
    这里要求一维向量a和向量b的行列数相同。
    点乘又叫向量的内积、数量积，是一个向量和它在另一个向量上的投影的长度的乘积；是标量。 
    点乘反映着两个向量的“相似度”，两个向量越“相似”，它们的点乘越大。
    对两个向量执行点乘运算，就是对这两个向量对应位一一相乘之后求和的操作，点乘的结果是一个标量。
    向量内积,向量a和b的长度之积再乘以它们之间的夹角的余弦；向量内积的几何解释就是一个向量在另一个向量上的投影的积
    内积指的是一个向量(在另一个向量上)的投影乘上另一个向量的模(可以理解为向量的长度)，如果内积为零，意思是互相之间没有投影。
    np.inner([1, 0], [1, 2])
    Out[52]: 1
    math.sqrt(sum(vec**2 for vec in [1, 0])) 
    Out[53]: 1.0
    math.sqrt(sum(vec**2 for vec in [1, 2])) 
    Out[54]: 2.23606797749979
    对于一维数组，np.dot 其作用同np.inner
    
    ### 对于两个二维数组的inner，相当于按X和Y的最后顺序的轴方向上取向量，
    ### 然后依次计算内积后组成的多维数组
    np.inner([[1,2,3], [4,5,6]], [[1,0,0], [1,2,3], [4,5,6]])
    Out[67]: 
    array([[ 1, 14, 32],
           [ 4, 32, 77]])
    
    [[np.dot([1,2,3], [1,0,0]), np.dot([1,2,3],  [1,2,3]), np.dot([1,2,3], [4,5,6])], 
    [np.dot([4,5,6], [1,0,0]), np.dot([4,5,6], [1,2,3]), np.dot([4,5,6], [4,5,6])]]
    Out[71]: [[1, 14, 32], 
              [4, 32, 77]]


### 正交基(Orthogonal Basis)
    三个向量两两之间互相的内积等于零，于是这三个向量就是一组简单的正交基。

### 标准正交基
    每组正交基中的向量，其模(可以认为是长度)的大小都是1，这样的情况称为标准正交基。

### 叉乘
    两个向量的叉乘，又叫向量积、外积、叉积，叉乘的运算结果是一个向量而不是一个标量。并且两个向量的叉积与这两个向量组成的坐标平面垂直。
    向量的叉乘，即求同时垂直两个向量的向量，即c垂直于a，同时c垂直于b（a与c的夹角为90°，b与c的夹角为90°）
    c =  a×b = （a.y*b.z-b.y*a.z , b.x*a.z-a.x*b.z  , a.x*b.y-b.x*a.y）
    在二维空间中，叉乘还有另外一个几何意义就是：aXb等于由向量a和向量b构成的平行四边形的面积。
    在三维几何中，向量a和向量b的叉乘结果是一个向量，更为熟知的叫法是法向量，该向量垂直于a和b向量构成的平面。
    两个向量的外积，又叫叉乘、叉积向量积，其运算结果是一个向量而不是一个标量。并且两个向量的外积与这两个向量组成的坐标平面垂直。
    定义：向量a与b的外积a×b是一个向量，其长度等于|a×b| = |a||b|sin∠(a,b)，其方向正交于a与b。并且，(a,b,a×b)构成右手系。 
    特别地，0×a = a×0 = 0.此外，对任意向量a，a×a=0。
    向量的叉乘：a ∧ b
    a ∧ b = |a| * |b| * sinθ 
    向量积被定义为： 
    模长：（在这里θ表示两向量之间的夹角(共起点的前提下)（0° ≤ θ ≤ 180°），它位于这两个矢量所定义的平面上。） 
    方向：a向量与b向量的向量积的方向与这两个向量所在平面垂直，且遵守右手定则。
    若坐标系是满足右手定则的，当右手的四指从a以不超过180度的转角转向b时，竖起的大拇指指向是c的方向。c = a ∧ b
    np.outer只对一维数组进行计算，如果传入的是多维数组，则先将此数组展平为一维数组之后再进行算。
    outer计算列向量和行向量的矩阵乘积，即结果为矩阵
    np.outer([1,2,3], [4,5,6])
    Out[72]: 
    array([[ 4,  5,  6],
           [ 8, 10, 12],
           [12, 15, 18]])
           
    [row * np.array([4,5,6]) for row in [1,2,3]]
    Out[74]: [array([4, 5, 6]), array([ 8, 10, 12]), array([12, 15, 18])]

### 张量积np.tensordot
    tensordot()将两个多维数组a和b指定轴上的对应元素相乘并求和，它是最一般化的乘积运算函数。
    np.tensordot(Z,X,axes=[[0],[1]]) #指定Z的0轴与X的1轴进行乘积求和
    a = np.arange(60.).reshape(3,4,5)
    b = np.arange(24.).reshape(4,3,2)
    c = np.tensordot(a,b, axes=([1,0],[0,1]))
    
    对于多维数组的dot乘积，相当于tensordot(a,b,axes=[[-1],[-2]])
    np.dot(X,Z) == np.tensordot(X,Z,axes=[[-1],[-2]])

### 卷积核

### 互相关（cross-correlation）
    互相关
    设两个函数分别是f(t)和g(t)，则互相关函数定义为：
    它反映的是两个函数在不同的相对位置上互相匹配的程度。
    对卷积不要求倒序操作，也就是互相关；互相关不满足交换律；

### 卷积
    卷积，就是信号B(数据B)与信号A(数据A)错开时间或空间的内积和，错开的时间或空间长度就是卷积结果的自变量。
    或者说，离散域的卷积就是数据A与数据B里面的一段数据逐个相乘，然后再加起来。
    这里的“一段”就是卷积核的长度，当然也可以是无限长。连续域就是把加起来换成积分。
    卷积的方式有三种：
    如果自始至终卷积核都在“信号内”，则最后得到的结果长度会小于待卷积信号长度。假设待卷积信号的长度是n,卷积核大小是m,则结果长度为n-m+1;这种卷积的方式称为 valid;
    如果卷积核的中心刚好从待卷积信号的第一个元素滑到最后一个元素，则需要把原来的信号扩展长度；一般来说扩展的方式是在原来信号的边缘添加0元素，这个过程通常称为零填充(zero padding);
    通过零填充，卷积结果的长度和待卷积信号长度一样，这种卷积的方式称为same;
    如果通过零填充把卷积核能够划过的位置扩展到最大，则结果长度为 n+m-1;这种方式称为full.

### 编码器（encoder）
    编码器（encoder）将输入数据转换为一种不同的表示

### 降噪自动编码器(Denoising Auto-encoder, dAE)
    当采用无监督的方法分层预训练深度网络的权值时，为了学习到较鲁棒的特征，可以在网络的可视层（即数据的输入层）引入随机噪声，这种方法称为Denoise Autoencoder(简称dAE)。
    Denoising Autoencoder（降噪自动编码器）就是在Autoencoder的基础之上，为了防止过拟合问题而对输入的数据（网络的输入层）加入噪音，使学习得到的编码器W具有较强的鲁棒性，从而增强模型的泛化能力。
    通过与非破损数据（未引入噪音数据）训练的对比，破损数据（加噪数据）训练出来的Weight噪声比较小。降噪因此得名。

### 解码器（decoder）
    解码器函数将新的表示（经编码器处理后的表示）转换到原来的形式

### UniLM(统一语言模型，Uniﬁed Language Model )
    UniLM也是一个多层Transformer网络，跟bert类似，但是UniLM能够同时完成三种预训练目标，
    使用三种特殊的Mask的预训练目标，从而使得模型可以用于NLG，同时在NLU任务获得和BERT一样的效果。 模型使用了三种语言模型的任务：
    unidirectional prediction、bidirectional prediction、seuqnece-to-sequence prediction
    1、Unidirectional LM
    单向训练语言模型，mask词的语境就是其单侧的words，左边或者右边。
    x1 x2 [MASK] x4 对于MASK的预测，只能使用token1和token2以及自己位置能够被使用，使用的就是一个对角矩阵的。同理从右到左的LM也类似。
    2、Bidirectional LM
    双向训练语言模型，mask词的语境就是左右两侧的words。
    对于双向的LM，只对padding进行mask。
    3、Seq2Seq LM
    Seq-to-Seq语言模型，左边的seq我们称source sequence，右边的seq我们称为target sequence，我们要预测的就是target sequence，所以其语境就是所有的source sequence和其左侧已经预测出来的target sequence。
    在训练的时候，一个序列由[SOS]S_1[EOS]S_2[EOS]组成，其中S1是source segments，S2是target segments。
    随机mask两个segment其中的词，其中如果masked是source segment的词的话，则它可以attend to 所有的source segment的tokens，
    如果masked的是target segment，则模型只能attend to 所有的source tokens 以及target segment 中当前词（包含）和该词左边的所有tokens。
    这样的话，模型可以隐形地学习到一个双向的encoder和单向decoder。
    总的loss是三种LM的loss之和。
    我们在一个训练的batch中，1/3的时间训练bidirection LM，1/3的时间训练sequence-to-sequence LM objective， 1/6的时间训练left-to-right 和 1/6的时间训练 right-to-left LM。
    混合训练方式：对于一个batch，1/3时间采用双向(bidirectional)语言模型的目标，1/3的时间采用seq-to-seq语言模型目标，最后1/3平均分配给两种单向学习的语言模型，也就是left-to-right和right-to-left方式各占1/6时间。
    masking 方式：总体比例15%，其中80%的情况下直接用[MASK]替代，10%的情况下随机选择一个词替代，最后10%的情况用真实值。
    还有就是80%的情况是每次只mask一个词，另外20%的情况是mask掉bigram(二元分词)或者trigram(三元分词)。
    Attention 控制：不同的训练方式，其关注的语境是不一样的，
    不让当前预测词看掉的信息就采用掩码隐藏掉，只留下能让当前词可看的信息，换句话说，使用了掩码来控制在计算基于上下文的表征时 token 应该关注的上下文的量。

### 自回归(Auto Regressive，AR)
    根据上文内容预测下一个可能跟随的单词，就是常说的自左向右的语言模型任务，或者反过来也行，就是根据下文预测前面的单词，这种类型的LM被称为自回归语言模型。
    （GPT,ELMO）GPT 就是典型的自回归语言模型。
    ELMO尽管看上去利用了上文，也利用了下文，但是本质上仍然是自回归LM，这个跟模型具体怎么实现有关系。
    ELMO是做了两个方向（从左到右以及从右到左两个方向的语言模型），但是是分别有两个方向的自回归LM，然后把LSTM的两个方向的隐节点状态拼接到一起，来体现双向语言模型这个事情的。
    所以其实是两个自回归语言模型的拼接，本质上仍然是自回归语言模型。
    自回归语言模型有优点有缺点，缺点是只能利用上文或者下文的信息，不能同时利用上文和下文的信息，
    当然，貌似ELMO这种双向都做，然后拼接看上去能够解决这个问题，因为融合模式过于简单，所以效果其实并不是太好。
    它的优点，其实跟下游NLP任务有关，比如生成类NLP任务，比如文本摘要，机器翻译等，在实际生成内容的时候，就是从左向右的，自回归语言模型天然匹配这个过程。
    而Bert这种DAE模式，在生成类NLP任务中，就面临训练过程和应用过程不一致的问题，导致生成类的NLP任务到目前为止都做不太好。
    Transformer模型属于自回归模型，也就是说后面的token的推断是基于前面的token的。Decoder端的Mask的功能是为了保证训练阶段和推理阶段的一致性。
    在推理阶段，token是按照从左往右的顺序推理的。也就是说，在推理timestep=T的token时，decoder只能“看到”timestep < T的 T-1 个Token, 
    不能和timestep大于它自身的token做attention（因为根本还不知道后面的token是什么）。
    为了保证训练时和推理时的一致性，所以，训练时要同样防止token与它之后的token去做attention。
    Aotoregressive Lanuage Modeling，又叫自回归语言模型。它指的是，依据前面(或后面)出现的tokens来预测当前时刻的token，代表模型有ELMO、GTP等。
    缺点：它只能利用单向语义而不能同时利用上下文信息。ELMO 通过双向都做AR 模型，然后进行拼接，但从结果来看，效果并不是太好。
    优点： 对自然语言生成任务(NLG)友好，天然符合生成式任务的生成过程。这也是为什么 GPT 能够编故事的原因。

### 自编码(Auto Encoder，AE)
    自回归语言模型只能根据上文预测下一个单词，或者反过来，只能根据下文预测前面一个单词。
    相比而言，Bert通过在输入X中随机Mask掉一部分单词，然后预训练过程的主要任务之一是根据上下文单词来预测这些被Mask掉的单词，
    那些被Mask掉的单词就是在输入侧加入的所谓噪音。类似Bert这种预训练模式，被称为DAE LM。
    这种DAE LM的优缺点正好和自回归LM反过来，它能比较自然地融入双向语言模型，同时看到被预测单词的上文和下文，这是好处。
    缺点：主要在输入侧引入[Mask]标记，导致预训练阶段和微调(Fine-tuning)阶段不一致的问题，因为Fine-tuning阶段是看不到[Mask]标记的。
    DAE就要引入噪音，[Mask] 标记就是引入噪音的手段，这个正常。
    Autoencoding Language Modeling，又叫自编码语言。通过上下文信息来预测当前被mask的token，代表有BERT ，Word2Vec(CBOW)。
    缺点： 由于训练中采用了 [MASK] 标记，导致预训练与微调阶段不一致的问题。此外对于生成式问题， AE 模型也显得捉襟见肘，这也是目前 BERT 为数不多实现大的突破的领域。
    优点： 能够很好的编码上下文语义信息， 在自然语言理解(NLU)相关的下游任务上表现突出。

### 自编码器(AE)
    自编码器（autoencoder）=编码器（encoder）+解码器（decoder）。
    自编码器是一种网络类型,其目的是将输入编码到低维潜在空间,然后再解码回来。
    自编码器学习对原始输入进行重新构建。通过对编码器的输出施加各种限制,我们可以让自编码器学到比较有趣的数据潜在表示。
    自编码器分成两个部分，第一个部分是encoder，一般是多层网络，将输入的数据压缩成为一个向量，变成低维度，而该向量就称之为瓶颈。第二个部分是decoder，灌之以瓶颈，输出数据，我们称之为重建输入数据。
    我们的目的是要让重建数据和原数据一样，以达到压缩还原的作用。
    缺点：低维度的瓶颈显然丢失了很多有用的信息，重建的数据效果并不好。

### 去噪自编码器(DAE)
    降噪自动编码器DAE是在自动编码器的基础上，训练数据加入噪声，来训练整个网络，以和AE相同的方式去训练，得到的网络模型便是DAE。
    在实际的测试数据中，噪声是不可避免的，采用有噪声的训练数据训练网络，神经网络就能够学习到不加噪声的输入特征和噪声的主要特征。能够使网络在测试数据中有更强的泛化能力。

### 变分自编码器(VAE,variational autoencoder)
    VAE和AE，DAE不同的是，原先编码器是映射成一个向量，现在是映射成两个向量，一个向量表示分布的平均值，另外一个表示分布的标准差，两个向量都是相同的正态分布。现在从两个向量分别采样，采样的数据灌给解码器。
    VAE 不是将输入压缩成潜在空间中的固定编码,而是将转换为统计分布的参数,即平均值和方差。
    本质上来说,这意味着我们假设输入是由统计过程生成的,在编码和解码过程中应该考虑这一过程的随机性。
    然后,VAE 使用平均值和方差这两个参数来从分布中随机采样一个元素,并将这个元素解码到原始输入。
    这个过程的随机性提高了其稳健性,并迫使潜在空间的任何位置都对应有意义的表示,即潜在空间采样的每个点都能解码为有效的输出。

### 去耦变分自编码器(β-VAE)
    β-VAE是VAE的变体，增强了VAE模型表示解耦(Disentanglement，解纠缠)的能力。仅需在loss function中加上一个β即可达到目的。
    loss函数中，β就是一个超参数，当β为1的时候，它就是标准的VAE。
    一个较高的β值，就使得前变量空间z表示信息的丰富度降低，但同时模型的解纠缠能力增加。所以β可以作为表示能力和解纠缠能力之间的平衡因子。

### 语言模型（Language Model）
    语言模型简单来说就是一串词序列的概率分布。

### 掩码(mask)
    掩码(Mask)表示屏蔽某些值，以便在更新参数时它们不起作用。
    变换器(Transformer)模型中有两种掩码:填充掩码和序列掩码。
    填充掩码用于所有缩放的点积注意，序列掩码仅用于解码器的自注意。
    填充掩码解决了输入序列具有可变长度的问题。具体来说，在较短的序列后填0。
    但是如果输入序列太长，则会截取左侧的内容，并直接丢弃多余的内容。因为这些填充的位置实际上没有意义，注意机制不应该集中在这些位置，所以需要做一些处理。
    具体方法是在这些位置的值上加一个非常大的负数（负无穷大），这样这些位置的概率在softmax计算之后将接近0！
    填充掩码实际上是一个张量，每个值都是一个布尔值，false值指想要处理的值。
    padding mask: 处理非定长序列，区分 padding 和非 padding 部分，如在 RNN 等模型和 Attention 机制中的应用等；
    sequence mask: 防止标签泄漏 如: Transformer decoder 中的mask 矩阵，bert中的【Mask】位，XLNet中的mask矩阵等。
    Transformer 是包括 Encoder和 Decoder的，Encoder中 self-attention 的 padding mask就是用于处理非定长序列，
    而 Decoder 还需要防止标签泄露，即在t时刻不能看到t时刻之后的信息，因此在上述 padding mask的基础上，还要加上 sequence mask。
    在NLP中，文本一般是不定长的，所以在进行 batch训练之前，要先进行长度的统一，过长的句子可以通过truncating(截断)到固定的长度，过短的句子可以通过 padding(填充)增加到固定的长度，
    但是 padding 对应的字符只是为了统一长度，并没有实际的价值，因此希望在之后的计算中屏蔽它们，这时候就需要 Mask。
    在 Attention 机制中，同样需要忽略 padding 部分的影响，这里以Transformer encoder中的self-attention为例：self-attention中，Q和K在点积之后，需要先经过mask再进行softmax，
    因此，对于要屏蔽的部分，mask之后的输出需要为负无穷，这样softmax之后输出才为0。
    在语言模型中，常常需要从上一个词预测下一个词，但如果要在LM中应用 self attention 或者是同时使用上下文的信息，要想不泄露要预测的标签信息，就需要 mask 来“遮盖”它。
    sequence mask 一般是通过生成一个上三角矩阵来实现的，上三角区域对应要mask的部分。
    在Transformer 的 Decoder中，先不考虑 padding mask，一个包括四个词的句子[A,B,C,D]在计算了相似度scores之后，将scores的上三角区域mask掉，即替换为负无穷，
    再做softmax得到第三幅图。这样，比如输入 B 在self-attention之后，也只和A，B有关，而与后序信息无关。
    因为在softmax之后的加权平均中: B’ = 0.48A+0.52B，而 C，D 对 B’不做贡献。
    对于 decoder 的 self-attention，里面使用到的 scaled dot-product attention，同时需要padding mask 和 sequence mask 作为 attn_mask，具体实现就是两个mask相加作为attn_mask。
    其他情况，attn_mask 一律等于 padding mask。

### 掩码语言模型（Masked Language Model, MLM）
    MLM 随机遮蔽模型输入中的一些 token，目标在于仅基于遮蔽词的语境来预测其原始词汇 id。
    实际上就是一个完形填空任务，随机 Mask 掉文本中的某些字词，然后要模型去预测被 Mask 的字词。
    与从左到右的语言模型预训练不同，MLM 目标允许表征融合左右两侧的语境，从而预训练一个深度双向 Transformer。

### 句子预测(Next Sentense Prediction, NSP)
    在训练过程中，对于 sentenceA 和 sentenceB，随机选择 50% 对，
    将其中 sentenceB 置换成其他任意文本，然后标签为 0/1，从而构造成了二值分类任务。
    最终，pre-training 过程中，training loss 为平均 NSP 似然之和。
    NSP任务实际上是一个二分类，训练数据的正样本是通过采样同一个文档中的两个连续的句子，而负样本是通过采用两个不同的文档的句子。该任务主要是希望能提高下游任务的效果，例如NLI自然语言推理任务。
    但是后续的研究发现该任务效果并不好，主要原因是因为其任务过于简单。NSP其实包含了两个子任务，主题预测与关系一致性预测，但是主题预测相比于关系一致性预测简单太多了。
    这里提一下为啥包含了主题预测，因为正样本是在同一个文档中选取的，负样本是在不同的文档选取的，假如我们有2个文档，一个是娱乐相关的，一个是教育相关的，那么负样本选择的内容就是不同的主题，
    而正样都在娱乐文档中选择的话预测出来的主题就是娱乐，在教育的文档中选择的话就是后者这个主题了。


### 句子顺序预测(sentence-order prediction ,SOP)
    句子间顺序预测, 也就是给模型两个句子,让模型去预测两个句子的前后顺序 。
    
### 排列语言模型(Permutation Language Model, PLM)
    又称，乱序语言模型。
    PLM的本质就是LM联合概率的多种分解机制的体现；
    将LM的顺序拆解推广到随机拆解，但是需要保留每个词的原始位置信息（PLM只是语言模型建模方式的因式分解/排列，并不是词的位置信息的重新排列！）
    如果遍历 𝑇! 种分解方法，并且模型参数是共享的，PLM就一定可以学习到各种双向上下文；
    换句话说，当我们把所有可能的𝑇! 排列都考虑到的时候，对于预测词的所有上下文就都可以学习到了！
    由于遍历 𝑇! 种路径计算量非常大（对于10个词的句子，10!=3628800）。因此实际只能随机的采样𝑇!里的部分排列，并求期望；

### ELMo，(Embeddings from Language Models)
    ELMo顾名思义是从Language Models得来的embeddings，确切的说是来自于Bidirectional language models。
    利用语言模型来获得一个上下文相关的预训练表示，称为ELMo。
    与word2vec相比ELMo使上下文无关的静态(static)向量变成上下文相关的动态(dynamic)向量,一定程度解决了一词多义的问题。
    每一个词语的表征都是整个输入语句的函数。具体做法就是先在大语料上以language model为目标训练出bidirectional LSTM模型，然后利用LSTM产生词语的表征。
    在进行有监督的NLP任务时，可以将ELMo直接当做特征拼接到具体任务模型的词向量输入或者是模型的最高层表示上。
    不像传统的词向量，每一个词只对应一个词向量，ELMo利用预训练好的双向语言模型，然后根据具体输入从该语言模型中可以得到上下文依赖的当前词表示（对于不同上下文的同一个词的表示是不一样的），
    再当成特征加入到具体的NLP有监督模型里。
    引入双向语言模型，其实是2个单向语言模型（前向和后向）的集成；通过保存预训练好的2层biLSTM，通过特征集成或finetune(微调)应用于下游任务；
    缺点：本质上为自回归语言模型，只能获取单向的特征表示，不能同时获取上下文表示；LSTM不能解决长距离依赖。
    为什么不能用biLSTM构建双向语言模型？不能采取2层biLSTM同时进行特征抽取构建双向语言模型，否则会出现标签泄漏的问题；因此ELMO前向和后向的LSTM参数独立，共享词向量，独立构建语言模型；
    ELMo 和 BERT 的区别：ELMo 模型是通过语言模型任务得到句子中单词的 embedding 表示，以此作为补充的新特征给下游任务使用。
    因为 ELMO 给下游提供的是每个单词的特征形式，所以这一类预训练的方法被称为“Feature-based Pre-Training”。
    而 BERT 模型是“基于 Fine-tuning 的模式”，下游任务需要将模型改造成 BERT 模型，才可利用 BERT 模型预训练好的参数。

### Feature-based
    又称feature-extraction 特征提取。就是用预训练好的网络在新样本上提取出相关的特征，然后将这些特征输入一个新的分类器，从头开始训练的过程。
    也就是说在训练的过程中，网络的特征提取层是被冻结的，只有后面的密集链接分类器部分是可以参与训练的。

### 微调(Fine-tuning, finetune)
    冻结预训练模型的部分卷积层（通常是靠近输入的多数卷积层，因为这些层保留了大量底层信息）甚至不冻结任何网络层，
    训练剩下的卷积层（通常是靠近输出的部分卷积层）和全连接层。
    只要分阶段训练，模型稍有不同，都可以叫fine-tune(微调)
    fine-tuning: 微调。和feature-based的区别是，训练好新的分类器后，还要解冻特征提取层的顶部的几层，然后和分类器再次进行联合训练。
    之所以称为微调，就是因为在预训练好的参数上进行训练更新的参数，比预训练好的参数的变化相对小，这个相对是指相对于不采用预训练模型参数来初始化下游任务的模型参数的情况。
    也有一种情况，如果你有大量的数据样本可以训练，那么就可以解冻所有的特征提取层，全部的参数都参与训练，但由于是基于预训练的模型参数，所以仍然比随机初始化的方式训练全部的参数要快的多。
    对于作者团队使用BERT模型在下游任务的微调时，就采用了解冻所有层，微调所有参数的方法。

### 判别性微调（Discr）
    由于不同的层捕获不同类型的信息，它们应该在不同程度上进行微调。 
    不是对模型的所有层使用相同的学习速率，而是区分性微调允许我们以不同的学习速率调整每个层。

### 倾斜的三角学习率（STLR）
    它首先线性地增加学习率，然后根据训练时间线性衰减它; 具有短期增长和长衰减期

### Warmup学习率
    Warm-up的策略就是初期用一个逐渐递增的学习率去初始化网络，渐渐初始化到一个更优的搜索空间。
    Warmup是在ResNet论文中提到的一种学习率预热的方法，它在训练开始的时候先选择使用一个较小的学习率，
    训练了一些epoches或者steps(比如4个epoches,10000steps),再修改为预先设置的学习来进行训练。
    由于刚开始训练时,模型的权重(weights)是随机初始化的，此时若选择一个较大的学习率,可能带来模型的不稳定(振荡)，
    选择Warmup预热学习率的方式，可以使得开始训练的几个epoches或者一些steps内学习率较小,
    在预热的小学习率下，模型可以慢慢趋于稳定,等模型相对稳定后再选择预先设置的学习率进行训练,使得模型收敛速度变得更快，模型效果更佳。
    总之，使用Warmup预热学习率的方式,即先用最初的小学习率训练，然后每个step增大一点点，直到达到最初设置的比较大的学习率时（注：此时预热学习率完成），
    采用最初设置的学习率进行训练（注：预热学习率完成后的训练过程，学习率是衰减的），有助于使模型收敛速度变快，效果更佳。
    constant warmup: 是从一个很小的学习率一下变为比较大的学习率，但这可能会导致训练误差突然增大。
    gradual warmup: 从最初的小学习率开始，每个step增大一点点学习率，直到达到最初设置的比较大的学习率时，采用最初设置的学习率进行训练。
    warmup:学习率热身。规定前多少个热身步骤内，对学习率采取逐步递增的过程。热身步骤之后，会对学习率采用衰减策略。这样训练初期可以避免震荡，后期可以让loss降得更小。
    
### 通用语言模型微调(Universal Language Model Fine-tuning for Text Classification,ULMFiT)
    要点：三阶段训练：LM预训练+精调特定任务LM+精调特定分类任务；特征抽取：3层AWD-LSTM；精调特定分类任务：逐层解冻；
    ULMFiT由三个阶段组成：
    a）LM在一般领域语料库上进行训练，以捕获不同层次语言的一般特征。 
    b）使用判别性微调（'Discr'）和倾斜三角学习率（STLR）对目标任务数据进行微调，以学习任务特定的功能。 
    c）使用逐渐解冻，'Discr'和STLR对目标任务进行微调，以保留低级表示并适应高级表示

### SiATL(Simple Approach for Transfer Learning)
    SiATL要点：
    二阶段训练：LM预训练+特定任务精调分类任务（引入LM作为辅助目标，辅助目标对于小数据有用，与GPT相反）；
    特征抽取：LSTM+self-attention；
    精调特定分类任务：逐层解冻；

### GPT(Generative Pre-Training)
    使用的是标准的语言模型目标函数，即通过前k个词预测当前词，但是在语言模型网络上他们使用了Transformer解码器作为语言模型。
    Transformer模型主要是利用自注意力（self-attention）机制的模型
    具体NLP任务有监督微调时，与ELMo当成特征的做法不同，OpenAI GPT不需要再重新对任务构建新的模型结构，
    而是直接在transformer这个语言模型上的最后一层接上softmax作为任务输出层，然后再对这整个模型进行微调。
    GPT1.0要点：
        采用Transformer进行特征抽取，首次将Transformer应用于预训练语言模型；
        finetune阶段引入语言模型辅助目标（辅助目标对于大数据集有用，小数据反而有所下降，与SiATL相反），解决finetune过程中的灾难性遗忘；
        预训练和finetune一致，统一二阶段框架；
    GPT2.0要点：
        没有针对特定模型的精调流程：GPT2.0认为预训练中已包含很多特定任务所需的信息。
        生成任务取得很好效果，使用覆盖更广、质量更高的数据；
    缺点：
        依然为单向自回归语言模型，无法获取上下文相关的特征表示；

### BERT（Bidirectional Encoder Representation from Transformers)
    BERT 是“Bidirectional Encoder Representations from Transformers”的首字母缩写，整体是一个自编码语言模型（Autoencoder LM），并且其设计了两个任务来预训练该模型。
    第一个任务是采用 MaskLM 的方式来训练语言模型，通俗地说就是在输入一句话的时候，随机地选一些要预测的词，然后用一个特殊的符号[MASK]来代替它们，之后让模型根据所给的标签去学习这些地方该填的词。
    第二个任务在双向语言模型的基础上额外增加了一个句子级别的连续性预测任务，即预测输入 BERT 的两段文本是否为连续的文本，引入这个任务可以更好地让模型学到连续的文本片段之间的关系。
    BERT 相较于原来的 RNN、LSTM 可以做到并发执行，同时提取词在句子中的关系特征，并且能在多个不同层次提取关系特征，进而更全面反映句子语义。
    相较于 word2vec，其又能根据句子上下文获取词义，从而避免歧义出现。同时缺点也是显而易见的，模型参数太多，而且模型太大，少量数据训练时，容易过拟合。
    BERT 只使用了 Transformer 的 Encoder 模块，原论文中，作者分别用 12 层和 24 层 Transformer Encoder 组装了两套 BERT 模型，分别是：
    BERT-base: L=12,H=768,A=12, 参数量=110M; BERT-large: L=24,H=1024,A=16, 参数量=340M; 
    其中层的数量(即，Transformer Encoder 块的数量)为L，隐藏层的维度为H，自注意头的个数为A。
    「需要注意的是，与 Transformer 本身的 Encoder 端相比，BERT 的 Transformer Encoder 端输入的向量表示，多了 Segment Embeddings(句子向量，是第一个句子，还是第二个句子)。」
    BERT随机mask语料中15%的token，然后将masked token 位置输出的最终隐层向量送入softmax，来预测masked token。
    BERT在pretrain的时候 会对训练集进行MASK 操作, 其中mask的方法是:
    15%的原始数据被mask, 85% 没有被mask。对于被mask的15% 分3种处理方式: 其中80%真正替换mask，10%随机替换，10%不动。
    mask的时候15% 的有被替换的概率，其中80% 被真正替换。在这80%真正替换的里面有80%单个token被替换，20%的二元(bigram)或者三元tokens(trigram)被替换。
    80%真正替换、10%随机、10%保留的mask策略的好处，Transformer encoder就不知道会让其预测哪个单词，或者说不知道哪个单词会被随机单词给替换掉，
    那么它就不得不保持每个输入token的一个上下文的表征分布(a distributional contextual representation)。
    也就是说如果模型学习到了要预测的单词是什么，那么就会丢失对上下文信息的学习，而如果模型训练过程中无法学习到哪个单词会被预测，
    那么就必须通过学习上下文的信息来判断出需要预测的单词，这样的模型才具有对句子的特征表示能力。
    另外，由于随机替换相对句子中所有tokens的发生概率只有1.5%(即15%的10%)，所以并不会影响到模型的语言理解能力。
    在pre-train和fine-tuning的过程中出现不match，因为[mask]在微调的过程中不会出现。
    将部分词mask，这个主要作用是用被mask词前后的词来去猜测mask掉的词是什么，因为是人为mask掉的，所以计算机是知道mask词的正确值，所以也可以判断模型猜的词是否准确。
    与ELMo相比BERT，前者训练出的词级别(word-level)向量变成后者句子级别(sentence-level)的向量
    Bert是直接在输入端显示地通过引入Mask标记，在输入侧隐藏掉一部分单词，让这些单词在预测的时候不发挥作用，要求利用上下文中其它单词去预测某个被Mask掉的单词；
    引入Masked Language Model(MLM)预训练目标，能够获取上下文相关的双向特征表示；
    引入Next Sentence Prediction(NSP)预训练目标，擅长处理句子或段落的匹配任务；
    引入强大的特征抽取机制Transformer(多种机制并存)：
    Multi-Head self attention：多头机制类似于“多通道”特征抽取，self attention通过attention mask动态编码变长序列，解决长距离依赖（无位置偏差）、可并行计算；
    Feed-forward ：在位置维度计算非线性层级特征；
    Layer Norm & Residuals：加速训练，使“深度”网络更加健壮；
    引入大规模、高质量的文本数据；
    Bert 的预训练阶段使用的是不带标签的数据，貌似是无监督学习，其实可以看到通过 [MASK]，模型已经使用标签在做分类任务训练了，这有点类似词向量模型 CBOW，
    只不过它使用了更强的 Transformer 做语义特征提取器，从而考虑到更长的上下文信息，而不仅仅是只截取窗口长度的 token 使用浅层全连接神经网络做训练。
    另外 CBOW 经过训练后学习到的是每个 token 的静态词向量，是一个结果；而 Bert 学到的是“一种学习能力”，
    就是能根据 token 的上下文信息来学习出 token 的词向量以供下游任务使用，这种词向量是动态的，
    很好的解决了一词多义的问题（也就是同一个 token 在不同的上下文环境里具有不同的词向量 embedding）。

### ALBERT
    ALBERT也是采用和BERT一样的Transformer的encoder结果，激活函数使用的也是GELU。在ALBERT中主要有三个改进方向。
    为了说明这三个改进，先定义几个变量：词的embedding我们设置为E，encoder的层数我们设置为L，hidden size即encoder的输出值的维度我们设置为H，词典的大小为V。
    1、对Embedding因式分解（Factorized embedding parameterization）
    矩阵分解:在两个大维度之间加入一个小维度，从O(V*H)变为O(V*E+E*H)，其中H 远远大于 E，以达到降维的作用
    在BERT中，词embedding与encoder输出的embedding维度是一样的。但是ALBERT认为，词级别的embedding是没有上下文依赖的表述，而隐藏层的输出值不仅包括了词本生的意思还包括一些上下文信息，
    理论上来说隐藏层的表述包含的信息应该更多一些，因此应该让H>>E，所以ALBERT的词向量的维度是小于encoder输出值维度的。
    在NLP任务中，通常词典都会很大，embedding matrix的大小是E×V，如果和BERT一样让H=E，那么embedding matrix的参数量会很大，并且反向传播的过程中，更新的内容也比较稀疏。
    结合上述说的两个点，ALBERT采用了一种因式分解的方法来降低参数量。首先把one-hot向量映射到一个低维度的空间，大小为E，然后再映射到一个高维度的空间，
    说白了就是先经过一个维度很低的embedding matrix，然后再经过一个高维度matrix把维度变到隐藏层的空间内，从而把参数量从O(V×H)降低到了O(V×E+E×H)，当E<<H时参数量减少的很明显。
    在这里做了一个矩阵分解，将矩阵V x H(E)分解为两个小的矩阵V x E，E x H，E << H。在这里不再将E=H，而是将E设置为一个远小于H的值，然后再经过一个矩阵E x H将词向量维度映射到H。
    BERT的情况是，E=H；ALBERT的方案是，将E降低，在词嵌入和隐藏层之间加入一个project层，连接两个层。假设V=30000，H=1024，BERT参数量= E*V = H*V=30000*1024；
    ALBERT中，E=128；H=1024，其参数量 = (V +H)*E=30000*128+128*1024。  
    2、跨层的参数共享（Cross-layer parameter sharing）
    在ALBERT还提出了一种参数共享的方法，全连接层与attention层都进行参数共享，也就是说共享encoder内的所有参数，也就是ALBERT依然有多层的深度连接，但是各层之间的参数是一样的。
    很明显的，通过这种方式，ALBERT中隐藏层的参数量变为原来的1/12或者1/24。同样量级下的Transformer采用该方案后实际上效果是有下降的，但是参数量减少了很多，训练速度也提升了很多。
    ALBERT通过参数共享的方式降低了内存，但是因为层数没有变，在推断时的计算量并没有下降，预测阶段还是需要和BERT一样的时间。
    ALBERT解决的是训练时候的速度提升。隐藏层间参数共享能够极大的减少模型参数，对模型训练速度的提升也有一定的帮助。但是对推理预测速度却不会有任何帮助，因为前向传播时的计算量一点也没有减少。
    3、句间连贯（Inter-sentence coherence loss）
    BERT的NSP任务实际上是一个二分类，训练数据的正样本是通过采样同一个文档中的两个连续的句子，而负样本是通过采用两个不同的文档的句子。但是后续的研究发现该任务效果并不好，主要原因是因为其任务过于简单。
    NSP其实包含了两个子任务，主题预测与关系一致性预测，但是主题预测相比于关系一致性预测简单太多了。
    在ALBERT中，为了只保留一致性任务去除主题识别的影响，提出了一个新的任务 sentence-order prediction（SOP），SOP的正样本和NSP的获取方式是一样的，负样本把正样本的顺序反转即可。
    SOP因为实在同一个文档中选的，其只关注句子的顺序并没有主题方面的影响。并且SOP能解决NSP的任务，但是NSP并不能解决SOP的任务。
    在BERT中，句子间关系的任务是next sentence predict(NSP)，即向模型输入两个句子，预测第二个句子是不是第一个句子的下一句。
    在ALBERT中，句子间关系的任务是sentence-order prediction(SOP)，即句子间顺序预测，也就是给模型两个句子，让模型去预测两个句子的前后顺序。
    此外，ALBERT还有一个albert_tiny模型，其隐藏层仅有4层，模型参数量约为1.8M，非常的轻便。相对于BERT，其训练和推理预测速度提升约10倍，但精度基本保留。

### RoBERTa
    RoBERTa 主要是在 Bert 基础上做了几点调整：
    训练时间更长（100k->300k->500k steps），batch size 更大（由bert的256变为2k或8k），训练数据更多（在BERT16G语料变为了160G语料）。
    移除了 NSP 任务。
    训练序列更长。
    动态调整 Mask 机制，一开始把预训练的数据复制 10 份，每一份都随机选择 15% 的 Tokens 进行 Mask，也就是说，同样的一句话有 10 种不同的 Mask 方式。
    然后每份数据都训练 N/10 个 epoch。这就相当于在这 N 个 epoch 的训练中，每个序列的被 Mask 的 tokens 是会变化的。

### SimBERT
    SimBERT属于有监督训练，训练语料是自行收集到的相似句对，通过一句来预测另一句的相似句生成任务来构建Seq2Seq部分，
    [CLS]的向量事实上就代表着输入的句向量，所以可以同时用它来训练一个检索任务。
    假设SENT_a和SENT_b是一组相似句，那么在同一个batch中，把[CLS] SENT_a [SEP] SENT_b [SEP]和[CLS] SENT_b [SEP] SENT_a [SEP]都加入训练，做一个相似句的生成任务，这是Seq2Seq部分。
    
### SimCSE
    SimCSE去掉了SimBERT的生成部分，仅保留检索模型；
    由于SimCSE没有标签数据，所以把每个句子自身视为相似句传入。    
    本质上来说就是(自己,自己)作为正例、(自己,别人)作为负例来训练对比学习模型。
    当然这里会使用一些数据扩增手段，让正例的两个样本有所差异。
    SimCSE则提出了一个极为简单的方案：直接把Dropout当作数据扩增！

### WoBERT
    WoBERT 同样也是采用 BERT 的结构，但区别是在训练中加入的词，这是一个专门为生成式下游任务训练的模型。
    以字为粒度的生成方式有两个主要的弊端，第一是对于相同长度的句子，字粒度式的生成比词粒度式的生成需要更长的序列，这导致生成的时间过长，并且过长的生成序列还容易造成在解码阶段带来的累积误差。
    第二个问题是中文的词很多时候是固定搭配，拆为字来进行生成增加了模型生成的难度。
    基于以上的原因，在保留原本 BERT 词表中的字的基础上，新增了一定数量的词，并进行再训练。

### exhaustive search（穷举搜索）
    穷举所有可能的输出序列，3个时间步长，每个步长3种选择，共计 3^3=27 种排列组合。
    从所有的排列组合中找到输出条件概率最大的序列。穷举搜索能保证全局最优，但计算复杂度太高，当输出词典稍微大一点根本无法使用。
    
### greedy search（贪心搜索）
    贪心算法在翻译每个字的时候，直接选择条件概率最大的候选值作为当前最优。    
    贪心算法每一步选择中都采取在当前状态下最好或最优的选择，通过这种局部最优策略期望产生全局最优解。
    贪心算法本质上没有从整体最优上加以考虑，并不能保证最终的结果一定是全局最优的。但是相对穷举搜索，搜索效率大大提升。
    
### beam search（集束搜索）
    beam search是对greedy search的一个改进算法。相对greedy search扩大了搜索空间，但远远不及穷举搜索指数级的搜索空间，是二者的一个折中方案。
    beam search有一个超参数beam size（束宽），设为 k 。第一个时间步长，选取当前条件概率最大的 k 个词，当做候选输出序列的第一个词。
    之后的每个时间步长，基于上个步长的输出序列，挑选出所有组合中条件概率最大的 k 个，作为该时间步长下的候选输出序列。始终保持 k 个候选。最后从 k 个候选中挑出最优的。
    beam search不保证全局最优，但是比greedy search搜索空间更大，一般结果比greedy search要好。
    greedy search 可以看做是 beam size = 1时的 beam search。
    
### T5 模型
    Transfer Text-to-Text Transformer 的简写。
    使用了标准的Encoder-Decoder模型，并且构建了无监督/有监督的文本生成预训练任务。
    T5的预训练包含无监督和有监督两部分。无监督部分使用的是Google构建的近800G的语料（论文称之为C4），而训练目标则跟BERT类似，只不过改成了Seq2Seq版本。
    有监督部分，则是收集了常见的NLP监督任务数据，并也统一转化为SeqSeq任务来训练。
    希望用文字把我们要做的任务表达出来，然后都转化为文字的预测。

### 模型蒸馏
    越复杂的深度学习网络，其拟合效果越好，但伴随出现推理（预测）速度越慢的问题。
    模型蒸馏，其目的就是尽量不损失模型精度的前提下，大大的提升模型的推理速度。
    实现方法：
    第一步，训练好原本的复杂网络模型，如BERT，我们称为Teacher模型；
    第二步，用一个较为简单的模型去拟合Teacher模型（Student模型是去拟合Teacher模型推理的soft targets，因为soft targets包含的信息更多），称为Student模型；
    Student模型的Loss为：Loss=Crossentropy(s,t)，其中s表示Student模型推理的soft targets，t表示Teacher模型推理的soft targets
    但也有把hard targets也加进去Student的Loss中，即： Loss=Crossentropy(s,t)+Crossentropy(s,y)，其中y表示样本的真实标签（hard targets）。
    最后，利用训练好的Student模型进行推理预测。
    蒸馏训练
    第一种方法：
    在离线的情况下，将Teacher模型对所有样本的推理结果存入磁盘中，然后Student模型从磁盘中读取样本及Teacher模型推理的软标签，进行模型训练。
    第二种方法：
    将Teacher模型和Student模型同时加载到网络中，但将Teacher模型冻结，只进行前向传播，不进行反向传播更新参数；然后将前向传播的结果传递给Student模型的Loss中，训练Student模型。
    第三种方法：
    方法2存在这样的缺点：每一个batch，Student模型都需要等待Teacher模型推理结束才能进行反向传播，影响训练速度。
    那我们就可以将Teacher模型和Student模型分开部署，进行异步计算。Teacher模型只需要前向传播，而Student模型需要前向和反向传播，在处理时间上可能处于一个持平的状态。

### soft targets（软标签）
    soft targets：属于不同标签的概率，一般是softmax的计算结果。例如，我们的标签是“男”和“女”两种，那么软标签的形式应该就是（0.4，0.6）。

### hard targets（硬标签）
    hard targets：属于哪一种标签。例如，该样本的标签是“男”，那么硬标签就是（1，0）。
        
### BPE（Byte Pair Encoding，双字节编码）
    用于解决 集外词（OOV, out-of-vocabulary）和罕见词（Rare word）问题。
    BPE将单词作为单词片段处理（word pieces），以便于处理未出现单词。
    先将训练集单词划分成片段（利用BPE），然后将片段随机赋值后放到RNNs或CNNs中训练出片段的embedding，再将片段组合得出word的embedding后，进行工作。
    这样如果在训练集或者其他情况中，遇到生僻词或者未登录词时，直接利用片段进行组合来进行任务。
    
### 激活函数（Activation Function）
    激活函数（Activation Function），就是在人工神经网络的神经元上运行的函数，负责将神经元的输入映射到输出端。
    在神经元中，输入的 inputs 通过加权，求和后，还被作用了一个函数，这个函数就是激活函数。
    引入激活函数是为了增加神经网络模型的非线性。没有激活函数的每层都相当于矩阵相乘。就算你叠加了若干层之后，无非还是个矩阵相乘罢了。 
    如果不用激活函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合，这种情况就是最原始的感知机（Perceptron）。
    如果使用的话，激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中。
    
### Sigmoid函数
    Sigmoid函数是一个在生物学中常见的S型函数，也称为S型生长曲线。
    在信息科学中，由于其单增以及反函数单增等性质，Sigmoid函数常被用作神经网络的阈值函数，将变量映射到0,1之间。
    sigmoid 函数是一个 logistic 函数，意思就是说：不管输入是什么，得到的输出都在 0 到 1 之间。
    也就是说，你输入的每个神经元、节点或激活都会被缩放为一个介于 0 到 1 之间的值。
    sigmoid 函数输入一个很大的 x 值（正或负）时，我们得到几乎为 0 的 y 值
    在反向传播时，当 x 是一个很大的值（正或负）时，我们本质上就是用一个几乎为 0 的值来乘这个偏导数的其余部分。
    如果有太多的权重都有这样很大的值，那么我们根本就没法得到可以调整权重的网络，这可是个大问题。
    如果我们不调整这些权重，那么网络就只有细微的更新，这样算法就不能随时间给网络带来多少改善。
    对于针对一个权重的偏导数的每个计算，我们都将其放入一个梯度向量中，而且我们将使用这个梯度向量来更新神经网络。
    可以想象，如果该梯度向量的所有值都接近 0，那么我们根本就无法真正更新任何东西。
    总结起来说就是梯度消失问题使得 sigmoid 函数在神经网络中并不实用
    公式: f(x) = 1/(1+e^-x)

### Tanh函数
    Tanh是双曲函数中的一个，Tanh()为双曲正切。在数学中，双曲正切“Tanh”是由基本双曲函数双曲正弦和双曲余弦推导而来。
    f(x) = (e^x-e^-x)/(e^x+e^-x)

### Relu激活函数（The Rectified Linear Unit，整流线性单元）
    用于隐层神经元输出。x 值小于零的一切都映射为 0 的 y 值，但 x 值大于零的一切都映射为它本身。
    当我们将 ReLU 函数引入神经网络时，我们也引入了很大的稀疏性。
    稀疏：数量少，通常分散在很大的区域。在神经网络中，这意味着激活的矩阵含有许多 0。
    神经网络是稀疏的，能提升时间和空间复杂度方面的效率——常数值（通常）所需空间更少，计算成本也更低。
    如果在计算梯度时有太多值都低于 0 ，会得到相当多不会更新的权重和偏置，因为其更新的量为 0，即死亡 ReLU 问题。
    优点：
    相比于 sigmoid，由于稀疏性，时间和空间复杂度更低；不涉及成本更高的指数运算；
    能避免梯度消失问题。
    缺点：
    引入了死亡 ReLU 问题，即网络的大部分分量都永远不会更新。但这有时候也是一个优势；
    ReLU 不能避免梯度爆炸问题。
    公式如下
    f(x) = max(0, x)

### 指数线性单元（ELU）
    如果你输入的 x 值大于 0，则结果与 ReLU 一样——即 y 值等于 x 值；但如果输入的 x 值小于 0，则我们会得到一个稍微小于 0 的值: α(e^x-1)。
    所得到的 y 值取决于输入的 x 值，但还要兼顾参数 α——你可以根据需要来调整这个参数（常见的α值是在 0.1 到 0.3 之间）。
    优点：
    能避免死亡 ReLU 问题；
    能得到负值输出，这能帮助网络向正确的方向推动权重和偏置变化；
    在计算梯度时能得到激活，而不是让它们等于 0。
    缺点：
    由于包含指数运算，所以计算时间更长；
    无法避免梯度爆炸问题；
    神经网络不学习 α 值。

### 渗漏型整流线性单元激活函数（Leaky ReLU）
    渗漏型整流线性单元激活函数也有一个 α 值，通常取值在 0.1 到 0.3 之间。
    如果输入 x 大于 0，则输出为 x；如果输入 x 小于或等于 0，则输出为 α 乘以输入。
    优点：
    类似 ELU，Leaky ReLU 也能避免死亡 ReLU 问题，因为其在计算导数时允许较小的梯度；
    由于不包含指数运算，所以计算速度比 ELU 快。
    缺点：
    无法避免梯度爆炸问题；
    神经网络不学习 α 值；
    在微分时，两部分都是线性的；而 ELU 的一部分是线性的，一部分是非线性的。

### 扩展型指数线性单元激活函数（SELU）
    当实际应用这个激活函数时，必须使用 lecun_normal 进行权重初始化。如果希望应用 dropout，则应当使用 AlphaDropout。
    公式的两个值：α 和 λ 是预先确定的.
    如果输入值 x 大于 0，则输出值为 x 乘以 λ；如果输入值 x 小于 0，则会得到一个奇异函数——它随 x 增大而增大并趋近于 x 为 0 时的值 0.0848。
    本质上看，当 x 小于 0 时，先用 α 乘以 x 值的指数，再减去 α，然后乘以 λ 值。
    SELU 激活能够对神经网络进行自归一化（self-normalizing）
    在初始化函数为 lecun_normal 的假设下，网络参数会被初始化一个正态分布（或高斯分布），然后在 SELU 的情况下，网络会在论文中描述的范围内完全地归一化。
    本质上看，当乘或加这样的网络分量时，网络仍被视为符合高斯分布。我们就称之为归一化。
    反过来，这又意味着整个网络及其最后一层的输出也是归一化的。
    SELU 的输出是归一化的，这可称为内部归一化（internal normalization），因此事实上其所有输出都是均值为 0 且标准差为 1。
    当输入小于 0 时，方差减小；当输入大于 0 时，方差增大——而标准差是方差的平方根，这样我们就使得标准差为 1。
    优点：
    内部归一化的速度比外部归一化快，这意味着网络能更快收敛；
    不可能出现梯度消失或爆炸问题，见 SELU 论文附录的定理 2 和 3。

### gelu激活函数（Gaussian error linear units，高斯误差线性单元）
    高斯误差线性单元激活函数在最近的 Transformer 模型（谷歌的 BERT 和 OpenAI 的 GPT-2）中得到了应用。
    看得出来，这就是某些函数（比如双曲正切函数 tanh）与近似数值的组合。
    当 x 大于 0 时，输出为 x；但 x=0 到 x=1 的区间除外，这时曲线更偏向于 y 轴。
    优点：
    似乎是 NLP 领域的当前最佳；尤其在 Transformer 模型中表现最好；
    能避免梯度消失问题。

### 归一化（normalization）
    归一化首先是减去均值，然后除以标准差。
    因此，经过归一化之后，网络的组件（权重、偏置和激活）的均值为 0，标准差为 1。

### Layer Normalization(层归一化)
    Layer Normalization是一个通用的技术，其本质是规范优化空间，加速收敛。
    为了保证数据特征分布的稳定性，加入Layer Normalization，这样可以加速模型的优化速度。

### 过拟合(overfit)与欠拟合(underfit)
    训练开始时,优化和泛化是相关的:训练数据上的损失越小,测试数据上的损失也越小。
    这时的模型是欠拟合(underfit)的,即仍有改进的空间,网络还没有对训练数据中所有相关模式建模。
    但在训练数据上迭代一定次数之后,泛化不再提高,验证指标先是不变,然后开始变差,即模型开始过拟合。
    这时模型开始学习仅和训练数据有关的模式,但这种模式对新数据来说是错误的或无关紧要的。
    防止神经网络过拟合的常用方法包括:
    1、获取更多的训练数据
    2、减小网络容量
    3、添加权重正则化
    4、添加 dropout
    
### 权重正则化(weight regularization)
    为了防止模型从训练数据中学到错误或无关紧要的模式,最优解决方法是获取更多的训练数据。
    模型的训练数据越多,泛化能力自然也越好。如果无法获取更多数据,次优解决方法是调节模型允许存储的信息量,或对模型允许存储的信息加以约束。
    如果一个网络只能记住几个模式,那么优化过程会迫使模型集中学习最重要的模式,这样更可能得到良好的泛化。这种降低过拟合的方法叫作正则化(regularization)。
    奥卡姆剃刀(Occam’s razor)原理:如果一件事情有两种解释,那么最可能正确的解释就是最简单的那个,即假设更少的那个。
    这个原理也适用于神经网络学到的模型:给定一些训练数据和一种网络架构,很多组权重值(即很多模型)都可以解释这些数据。
    简单模型比复杂模型更不容易过拟合。    
    这里的简单模型(simple model)是指参数值分布的熵更小的模型(或参数更少的模型)。
    因此,一种常见的降低过拟合的方法就是强制让模型权重只能取较小的值,从而限制模型的复杂度,这使得权重值的分布更加规则(regular)。
    这种方法叫作权重正则化(weight regularization),其实现方法是向网络损失函数中添加与较大权重值相关的成本(cost)。
    这个成本有两种形式。 
    * L1 正则化(L1 regularization):
    添加的成本与权重系数的绝对值[权重的 L1 范数(norm)]成正比。 
    * L2 正则化(L2 regularization):
    添加的成本与权重系数的平方(权重的 L2 范数)成正比。
    神经网络的 L2 正则化也叫权重衰减(weight decay)。
    不要被不同的名称搞混,权重衰减与 L2 正则化在数学上是完全相同的。

### dropout 正则化
    dropout 是神经网络最有效也最常用的正则化方法之一。对某一层使用 dropout,就是在训练过程中随机将该层的一些输出特征舍弃(设置为 0)。
    假设在训练过程中,某一层对给定输入样本的返回值应该是向量 [0.2, 0.5,1.3, 0.8, 1.1] 。
    使用 dropout 后,这个向量会有几个随机的元素变成 0,比如 [0, 0.5,1.3, 0, 1.1] 。
    dropout 比率(dropout rate)是被设为 0 的特征所占的比例,通常在 0.2~0.5范围内。
    测试时没有单元被舍弃,而该层的输出值需要按 dropout 比率缩小,因为这时比训练时有更多的单元被激活,需要加以平衡。

### zoneout
    zoneout是rnn 时间维度上的“dropout”，要么维持前一个时刻的hidden vector，要么按照一般的样子更新。
    不是指单独的cell，而是指训练时的一种操作。
    Dropout就是通用的一种深度学习技巧，训练时随机失活一些神经元，可以增强模型泛化抑制过拟合作用。
    zoneout是指随机失活一个rnncell，跳过一步。

### 优化(optimization)
    优化(optimization)是指调节模型以在训练数据上得到最佳性能(即机器学习中的学习)
    
### 泛化(generalization)
    泛化(generalization)是指训练好的模型在前所未见的数据上的性能好坏。机器学习的目的当然是得到良好的泛化,但你无法控制泛化,只能基于训练数据调节模型。

### XLNet    
    通过Attention Mask机制，在Transformer内部随机Mask掉一部分单词（这个被Mask掉的单词比例跟当前单词在句子中的位置有关系，位置越靠前，被Mask掉的比例越高，位置越靠后，被Mask掉的比例越低），
    让这些被Mask掉的单词在预测某个单词的时候不发生作用。
    使用自回归语言模型，为解决双向上下文的问题，引入了排列语言模型；
    使用transformer-xl代替了transformer，能获取更长距离的依赖信息，即，融合Transformer-XL的优点处理过长文本；

### 词向量(word embedding)
    最简单的word embedding是把词进行基于词袋（BOW）的One-Hot表示。这种表示方法学习不到单词之间的关系（位置、语义），并且如果文档中有很多词，词向量可能会很长。
    另外一种方法就是通过word2vec训练词汇，将词汇用向量表示。该模型涉及两种算法：CBOW和Skip-Gram。这种方法可以将我们语义上相似的词用相似的向量表示，但是有个缺点，同一个词只有一种语义。
    使用语言模型预训练（如ELMo，GPT和 BERT其实可以看成是一个句子级别的上下文的词表示，它可以充分利用大规模的单语语料，并且可以对一词多义进行建模。

### 位置向量(Position Embedding)
    将每个位置编号，然后每个编号对应一个向量，通过结合位置向量和词向量，就给每个词都引入了一定的位置信息，这样就可以分辨出不同位置的词了。
    在RNN、CNN模型中其实都出现过Position Embedding，但在那些模型中，Position Embedding是锦上添花的辅助手段，也就是“有它会更好、没它也就差一点点”的情况，
    因为RNN、CNN本身就能捕捉到位置信息。但是在这个纯Attention模型中，Position Embedding是位置信息的唯一来源，因此它是模型的核心成分之一，并非仅仅是简单的辅助手段。
    Positional Embedding的成分直接叠加于Embedding之上，使得每个token的位置信息和它的语义信息(embedding)充分融合，并被传递到后续所有经过复杂变换的序列表达中去。
    论文中使用的Positional Encoding(PE)是正余弦函数，位置(pos)越小，波长越长，每一个位置对应的PE都是唯一的。
    之所以选用正余弦函数作为PE，是因为这可以使得模型学习到token之间的相对位置关系

### 注意力机制(attention机制)
    AttentionModel(注意力模型，AM)
    给定一组向量集合values，以及一个向量query，attention机制是一种根据该query计算values的加权求和的机制。
    attention的重点就是这个集合values中的每个value的“权值”的计算方法。
    有时候也把这种attention的机制叫做query的输出关注了原文的不同部分。
    具体过程：目标字及其上下文的字都有各自的原始Value，Attention机制将目标字作为Query、其上下文的各个字作为Key，
        并将Query与各个Key的相似性作为权重，把上下文各个字的Value融入目标字的原始Value中。
        Attention机制将目标字和上下文各个字的语义向量表示作为输入，首先通过线性变换获得目标字的Query向量表示、
        上下文各个字的Key向量表示以及目标字与上下文各个字的原始Value表示，然后计算Query向量与各个Key向量的相似度作为权重，
        加权融合目标字的Value向量和各个上下文字的Value向量，作为Attention的输出，即：目标字的增强语义向量表示。
    一个序列每个字符对其上下文字符的影响作用都不同，每个字对序列的语义信息贡献也不同，
    可以通过一种机制将原输入序列中字符向量通过加权融合序列中所有字符的语义向量信息来产生新的向量，即增强了原语义信息。
    注意力机制都可以一个三元组去描述，这个三元组是（Query，Key，Value）。
    假设我们有Q，K，V三个向量，每个输入单位（比如一个词）都具有对应的三元组的值，具体做法是将embedding后的词表示分别与Q，K，V相乘，得到最终的Q，K，V表示。
    query 和 key的维度必须保持一致；value 和query/key的维度可以不一致；key 和 value的长度必须保持一致；key 和value 本质上同一个句子在不同空间的表达；
    attention得到的输出，维度和value的维度一致，长度和 query的长度一致；Output(M*Dv) = Query(M*Dqk)* Key(Dqk*N)* Value(N*Dv)
    Output每个位置t是由Value的所有位置的vector加权平均之后的向量；而其权值是由位置为t的Query和Key的所有位置经过Attentiton计算得到的，权值的个数等于Key/Value的长度。
    而每一层线性映射参数矩阵都是独立的，所以经过映射后的Q, K, V各不相同，模型参数优化的目标在于将q, k, v被映射到新的高维空间，
    使得每层的Q, K, V在不同抽象层面上捕获到q, k, v之间的关系。
    一般来说，底层layer捕获到的更多是lexical-level(词汇层面)的关系，而高层layer捕获到的更多是semantic-level(语义层面)的关系。

### 自注意力机制(self attention机制)
    self attention其实是对attention进行了一个更广泛的定义罢了，
    比如很多时候我们是把k和v都当成一样的算来，做self的时候还可能是quey=key=value。
    Self-Attention:
        对于输入文本，我们需要对其中的每个字分别增强语义向量表示，因此，我们分别将每个字作为Query，加权融合文本中所有字的语义信息，
        得到各个字的增强语义向量。
        在这种情况下，Query、Key和Value的向量表示均来自于同一输入文本，即 Q = K = V(后面会经过变化变的不一样)， 
        同时对attention权重做了缩放，除去了维度值。
        因此，该Attention机制也叫Self-Attention。
    引入 Self-Attention 后会更容易捕获句子中长距离相互依赖特征，因为 Self-Attention 在计算过程中直接将句子任意两个单词的联系起来，
    此外，由于不依赖时间序列这一特性，Self-Attention 增加了计算的并行性。

### 多头注意力机制(Multi-head Attention)
    为了增强Attention的多样性，进一步利用不同的Self-Attention模块获得文本中每个字在不同语义空间下的增强语义向量，
    并将每个字的多个增强语义向量进行线性组合，从而获得一个最终的与原始字向量长度相同的增强语义向量
    “多头”实际上是指在初始化Q，K，V时，使用多组初始化值。
    Transformer使用了8组，每组都是随机初始化的，在经过训练后，我们就将得到8个获取了不同权重的结果表示。
    Multi-Head Attention的结果会被输入到前馈网络层中。为了避免同时输入8个结果，我们只需要再初始化一个矩阵W，
    和8个连接起来的attention结果做乘法，最终变换成前馈网络层可以接收的大小。
    在不改变参数量的情况下增强每一层attention的表现力。
    Multi-head Attention的本质是，在 「参数总量保持不变」 的情况下，
    将同样的query, key, value映射到原来的高维空间的「不同子空间」中进行attention的计算，在最后一步再合并不同子空间中的attention信息。
    这样降低了计算每个head的attention时每个向量的维度，在某种意义上防止了过拟合；由于Attention在不同子空间中有不同的分布，
    Multi- head Attention实际上是寻找了序列之间不同角度的关联关系，并在最后通过与权值矩阵相乘得以合并，将不同子空间中捕获到的关联关系综合起来。
    qi 和 kj 之间的attention score从1个变成了h个，这就对应了h个子空间中它们的关联度。
    Transformer 或 Bert 的特定层是有独特功能的，底层更偏向于关注语法，顶层更偏向于关注语义。
    同一层 Transformer 关注的方面是相同的，那么对于该方面而言，不同头的关注点应该也是一样的。
    但是在Multi-head Attention的同一层中，总有那么一两个头独一无二，和其它头关注的 Token 不同。
    利用多头机制，明显学会了不同的任务下采取不一样的注意力。
    Multi-head Attention的本质是，在 「参数总量保持不变」 的情况下，将同样的query, key, value映射到原来的高维空间的「不同子空间」中进行attention的计算，在最后一步再合并不同子空间中的attention信息。
    这样降低了计算每个head的attention时每个向量的维度，在某种意义上防止了过拟合；
    由于Attention在不同子空间中有不同的分布，Multi- head Attention实际上是寻找了序列之间不同角度的关联关系，并在最后concat这一步骤中，将不同子空间中捕获到的关联关系再综合起来。

### Seq2Seq
    Seq2Seq ( Sequence-to-sequence 的缩写)，就如字面意思，输入一个序列，输出另一个序列。这种结构最重要的地方在于输入序列和输出序列的长度是可变的。
    Seq2Seq 强调目的，不特指具体方法，满足输入序列，输出序列的目的，都可以统称为 Seq2Seq 模型。Seq2Seq 使用的具体方法基本都是属于 Encoder-Decoder 模型的范畴。
    在机器翻译里面，如将英语 「it is a cat.」翻译成汉语 「这是一只猫。」，输入 4 个单词，输出 5 个汉字。
    在训练数据集中，我们可以在每个句子后附特殊字符 <EOS> (end of sequence) 以表示序列终止，每个句子前用到了特殊字符 <BOS> (begin of seqence) 表示序列开始。
    Encoder 在最终时间步的隐状态作为输入句子表征和编码信息。Decoder 在各个时间步中使用输入句子的编码信息和上一个时间步的输出以及隐藏状态作为输入。
    案例：英文 it is a cat. 翻译成中文的过程。
        先将整个源句子进行符号化处理，以一个固定的特殊标记作为翻译的开始符号和结束符号。此时句子变成 it is a cat .
        对序列进行建模，得到概率最大的译词，如第一个词为 “这”。将生成的词加入译文序列，重复上述步骤，不断迭代。
        直到终止符号被模型选择出来，停止迭代过程，并进行反符号化处理，得到译文。

### encoder-decoder模型(编码-解码模型)
    Encoder-Decoder模型中的编码，就是将输入序列转化成一个固定长度的向量；解码，就是将之前生成的固定向量再转化成输出序列。
    准确的说，Encoder-Decoder并不是一个具体的模型，而是一类框架。
    RNN 的局限，在机器翻译中，输入某一序列，通过 RNN 将其转化为一个固定向量，再将固定序列转化为输出序列，如将英文翻译成中文。
    不管输入序列和输出序列长度是什么，中间的「向量 c」长度都是固定的。所以，RNN 结构的 Encoder-Decoder 模型存在长程梯度消失问题，
    对于较长的句子，我们很难寄希望于将输入的序列转化为定长的向量而保存所有有效信息，即便 LSTM 加了门控机制可以选择性遗忘和记忆，随着所需翻译的句子难度增加，这个结构的效果仍然不理想。

### Transformer
    Transformer是一个完全依靠Self-attention而不使用序列对齐的RNN或卷积的方式来计算输入输出表示的转换模型。
    Transformer整体架构就是一个Seq2Seq结构，包括编码器(Encoder)和解码器(decoder)，即是由编码组件、解码组件和它们之间的连接组成。
    编码组件部分由一堆编码器（encoder）构成（将n个编码器叠在一起）。
    解码组件部分也是由相同数量（与编码器对应）的解码器（decoder）组成的。
    
    Encoder的输出和decoder的结合方式是将最后一个encoder的输出将和每一层的decoder进行结合。
    所有的编码器(Encoder)在结构上都是相同的，但它们没有共享参数。每个编码器(Encoder)都可以分解成两个子层，
    即Encoder的每一层有两个操作，分别是注意力（self-attention）和前馈（feed-forward）；
    而Decoder的每一层有三个操作，分别是Self-Attention、Encoder-Decoder Attention以及Feed Forward操作。
    这里的Self-Attention和Encoder-Decoder Attention都是用的是Multi-Head Attention机制。
    
    从编码器输入的句子首先会经过一个自注意力（self-attention）层，这层帮助编码器在对每个单词编码时关注输入句子的其他单词。
    自注意力层的输出会传递到前馈（feed-forward）神经网络中。每个位置的单词对应的前馈神经网络都完全一样。
    解码器(decoder)中也有编码器的自注意力（self-attention）层和前馈（feed-forward）层。
    除此之外，这两个层之间还有一个编码-解码注意力层(Encoder-Decoder Attention)，用来关注输入句子的相关部分。
    
    一个编码器接收向量列表作为输入，接着将向量列表中的向量传递到自注意力层进行处理，然后传递到前馈神经网络层中，
    将输出结果传递到下一个编码器中。
    输入序列的每个单词都经过自编码过程。然后，他们各自通过前向传播神经网络——完全相同的网络，而每个向量都分别通过它。               

    模型框架如下
    Encoder 部分
        由 6 个相同的层(或者称之为块)组成，每个层包含 2 个部分：Multi-Head Self-Attention 和 Position-Wise Feed-Forward Network (全连接层)
        Multi-head self attention - 多组 self attention 的连接。
        Position-wise feed forward network，其实就是一个 MLP 网络,引入非线性变换，从而增加了模型的表现能力。
    Decoder 部分
        Decoder 也是由 6 个相同的层组成，每个层包含 3 个部分：Multi-Head Self-Attention、Multi-Head Context-Attention、Position-Wise Feed-Forward Network。
        Multi-head self attention (with mask) 与 encoder 部分相同，只是采用 0-1 mask 消除右侧单词对当前单词 attention 的影响。
        Multi-head attention(with encoder)引入encoder 部分的输出在此处作为multi-head 的其中几个head。
        Position-wise feed forward network与encoder 部分相同。
    上面每个部分（Encoder2个部分之间，Decode3个部分之间）都有残差连接 (redidual connection)，然后接一个 Layer Normalization。

    Transformer中的注意力机制
    谷歌将注意力机制一般化了，一个注意力函数描述为将Query与一组健值对(Key-Value)映射到输出，其中这三者均为向量形式。
    对于翻译任务，Query 可以认为是源词语向量序列，而 Key 和 Value 可以认为为目标词向量序列。
    即注意力通过计算 Query 和 Key 之间的相似性，并通过相似性来确定 Query 和 Value 之间的注意力关系。

#### Feed forward network(FNN层)
    每一层经过attention之后，还会有一个FFN，这个FFN的作用就是空间变换。FFN包含了2层linear transformation(线性变换)层，中间的激活函数是ReLu。
    FFN的加入引入了非线性(ReLu激活函数)，变换了attention output的空间, 从而增加了模型的表现能力。把FFN去掉模型也是可以用的，但是效果差了很多。

### Transformer-XL
    XL实际上是“extra-long”的意思，这意味着Transformer-XL在模型设计上做了长度方面的延申工作。
    句段层级的循环复用; Transformer每次处理的是定长片段。那么在XL版本中，上一次处理的片段信息将会被存储起来，在当前片段的处理中会把这部分信息添加进来，这便是“延长”的含义。这样做便完成了上下文之间的迁移。
    使用相对位置编码;在Transformer中，原本的位置embedding是一种绝对位置编码，因此，当我们采用上面提到的迁移方法时，绝对编码会让网络“产生困惑”，例如片段大小为4，那么每个片段的绝对位置编码都为（0，1，2，3），
    这变失去了位置顺序的信息。因此在XL中，使用相对位置编码。

### 人工神经网络（Artificial Neural Network，ANN）
    简称神经网络（Neural Network，NN）或类神经网络，在机器学习和认知科学领域，是一种数学模型或计算模型，用于对函数进行估计或近似。神经网络由大量的人工神经元联结进行计算。
    神经网络作为一类非线性的机器学习模型，可以更好的实现输入和输出之间的映射。

### 前馈神经网络（feedforward neural network，FNN）
    前馈神经网络简称前馈网络，是人工神经网络的一种。前馈神经网络采用一种单向多层结构。
    在它内部，参数从输入层向输出层单向传播。有异于递归神经网络，它的内部不会构成有向环。
    其中每一层包含若干个神经元。在此种神经网络中，各神经元可以接收前一层神经元的信号，并产生输出到下一层。
    第0层叫输入层，最后一层叫输出层，其他中间层叫做隐含层（或隐藏层、隐层）。隐层可以是一层。也可以是多层。
    “前馈”是指整个网络中无反馈，信号从输入层向输出层单向传播，可用一个有向无环图表示
    其实我们常用的网络，都是前馈神经网络，从输入到输出是一个有向图，中间不会有环或者反向传播。
    当然，我们在训练前馈神经网络的时候，会用到反向传播进行参数调整。但仍不影响整个网络的有向和前馈性质。
    前馈神经网络各神经元分层排列。每个神经元只与前一层的神经元相连。
    接收前一层的输出，并输出给下一层，数据正向流动，输出仅由当前的输入和网络权值决定，各层间没有反馈。
    包括：单层感知器，线性神经网络，BP神经网络、RBF神经网络等。

### 反馈神经网络
    反馈神经网络中，结构图的有向图是有回路的。
    在这种网络中，每个神经元同时将自身的输出信号作为输入信号反馈给其他神经元。
    反馈网络，也称记忆网络。网络中的神经元不仅可以接受其他神经元的信息也可以接受自己的历史信息，因此神经元具有记忆功能。
    记忆网络包括循环神经网络、Hopfield网络、玻尔兹曼机、受限玻尔兹曼机等。这种网络具有更强的计算与记忆能力。
    
### 反向传播（Backpropagation，缩写为BP）
    是“误差反向传播”的简称，是一种与最优化方法（如梯度下降法）结合使用的，用来训练人工神经网络的常见方法。
    该方法对网络中所有权重计算损失函数的梯度。这个梯度会反馈给最优化方法，用来更新权值以最小化损失函数。
    反向传播要求有对每个输入值想得到的已知输出，来计算损失函数梯度。因此，它通常被认为是一种监督式学习方法，虽然它也用在一些无监督网络（如自动编码器）中。
    它是多层前馈网络的Delta规则的推广，可以用链式法则对每层迭代计算梯度。反向传播要求人工神经元（或“节点”）的激励函数可微。

### 递归神经网络（RNN）
    RNN主要处理有时序关系的变长序列问题。
    每个神经元在每一时刻都一个特殊的隐藏(hidden)状态h(t)，由当前节点的输入I(t)和上一时刻t-1隐藏状态h(t-1)加权求和后经过一个非线性激活函数(tanh)得到。
    单纯的RNN因为无法处理随着递归，权重指数级爆炸或梯度消失问题，难以捕捉长期时间关联；而结合不同的LSTM可以很好解决这个问题。
    时间递归神经网络可以描述动态时间行为，因为和前馈神经网络（feedforward neural network）接受较特定结构的输入不同，RNN将状态在自身网络中循环传递，
    因此可以接受更广泛的时间序列结构输入。
    RNN存在梯度爆炸和梯度消失问题，如果时间序列较长，W_{hh}奇异值如果>1，t个σ连乘后会非常大，反之则会非常小。
    递归神经网络（RNN）是两种人工神经网络的总称。一种是时间递归神经网络（recurrent neural network），又名循环神经网络，包括RNN、LSTM、GRU等；另一种是结构递归神经网络（recursive neural network）。

### Simple RNN (without context)
    「Simple RNN」 ：这个encoder-decoder模型结构中，encoder将整个源端序列(不论长度)压缩成一个向量(encoder output)，
    源端信息和decoder之间唯一的联系只是: encoder output会作为decoder的initial states的输入。
    这样带来一个显而易见的问题就是，随着decoder长度的增加，encoder output的信息会衰减。
    这种模型有2个主要的问题:
    源端序列不论长短，都被统一压缩成一个固定维度的向量，并且显而易见的是这个向量中包含的信息中，
    关于源端序列末尾的token的信息更多，而如果序列很长，最终可能基本上“遗忘”了序列开头的token的信息。
    第二个问题同样由RNN的特性造成: 随着decoder timestep的信息的增加，initial hidden states中包含的encoder output相关信息也会衰减，
    decoder会逐渐“遗忘”源端序列的信息，而更多地关注目标序列中在该timestep之前的token的信息。

### Contextualized RNN(语境化RNN)
    「Contextualized RNN」 ：为了解决 encoder output随着decoder timestep增加而信息衰减的问题，
    有人提出了一种加了context的RNN sequence to sequence模型：
    decoder在每个timestep的input上都会加上一个context。
    为了方便理解，我们可以把这看作是encoded source sentence。
    这样就可以在decoder的每一步，都把源端的整个句子的信息和target端当前的token一起输入到RNN中，防止源端的context信息随着timestep的增长而衰减。
    但是这样依然有一个问题: context对于每个timestep都是静态的(encoder端的final hidden states，或者是所有timestep的output的平均值)。

### Contextualized RNN with Attention
    在每个timestep输入到decoder RNN结构中之前，会用当前的输入token的vector与encoder output中的每一个position(位置)的vector作一个"attention"操作，
    这个"attention"操作的目的就是计算当前token与每个position之间的"相关度"，从而决定每个position的vector在最终该timestep的context中占的比重有多少。
    最终的context即encoder output每个位置vector表达的 「加权平均」 。

### timestep
    文本处理中，一个单词代表一个timestep，在inference(推理)的时候，只能一个单词一个单词地输出；
    而在train的时候，我们有整个句子，因此可以一次feed若干个单词
    设我们输入数据的格式为(batch_size, time_steps, input_size)，其中time_steps表示序列本身的长度，
    如在Char RNN中，长度为10的句子对应的time_steps就等于10。最后的input_size就表示输入数据单个序列单个时间维度上固有的长度。
    这里一共五句话，把五句话当成一个批次，然后time_step为3
    sentences = ["i love you", "he loves me", "she likes baseball", "i hate you", "sorry for that"]
    >>>                       batch_size 为5
    time_step1:    i        he      she        i       sorry
    time_step2:    love     love    likes      hate    for
    time_step3:    you      me      baseball   you     that
    然后，rnn计算损失的时候,就会在每一个time_step计算batch_size大小个样本的平均损失(也有时候是最后一个time_step统计损失).

### LSTM (Long-Short-Term-Memory)
    一种特殊的RNN网络，该网络设计出来是为了解决长依赖问题。
    LSTM网络能通过一种被称为门的结构对细胞状态进行删除或者添加信息。
    LSTM相比RNN其实就是多了一个门(gate)机制和细胞记忆单元(cell-state)用来存储，用来记录信息。
    它的重复单元不同于标准RNN网络里的单元只有一个tanh激活网络层。
    一个LSTM里面包含三个门(输入门、输出门、遗忘门)来控制细胞状态。因为三者的计算方法都相同，区别只是使用了不同的权重矩阵以便反向传播时对三个门独立更新。
    LSTM的第一步就是决定细胞状态需要丢弃哪些信息(计算遗忘门)。
    这部分操作是通过一个称为忘记门的sigmoid单元来处理的。
    它通过查看前一隐藏状态h(t-1)和当前输入xt信息来输出一个0-1之间的向量，该向量里面的0-1值表示前一细胞状态C(t-1)中的哪些信息保留或丢弃多少。0表示不保留，1表示都保留。
    第二步是决定给细胞状态添加哪些新的信息。
    这一步又分为两个步骤，首先，利用h(t-1)和xt通过一个称为输入门的sigmoid操作来决定更新哪些信息，得到it。
    然后利用h(t-1)和xt通过一个tanh层得到新的候选细胞信息(临时细胞状态C't)，这些信息可能会被更新到细胞信息中。
    第三步：将更新旧的细胞信息C{t-1}，变为新的细胞信息C{t},即计算当前时刻的细胞状态。
    更新的规则就是通过忘记门选择忘记旧细胞信息的一部分(第一步的结果)，通过输入门选择添加候选细胞信息C‘{t}的一部分(第二步的2个结果)得到新的细胞信息C{t}。
    此步不涉及到激活函数，仅仅是信息的拼接，是一个线性变换。
    第四步：计算输出门和当前时刻隐藏层状态
    更新完细胞状态后(第三步的结果)需要根据输入的h(t-1)和xt来判断输出细胞的哪些状态特征，这里需要将输入经过一个称为输出门的sigmoid层得到判断条件，
    然后将细胞状态经过tanh层得到一个-1~1之间值的向量，该向量与输出门得到的判断条件相乘就得到了最终该RNN单元的输出。
    即是分两个步骤，首先，前一时刻隐藏层状态h(t-1)和当前时刻输入词xt经过一个sigmoid层激活得到输出门的值Ot；
    其次，细胞状态C{t}(第三步的结果)经过tanh层得到一个向量与输出门的值Ot进行点乘就得到当前时刻隐藏层的状态ht.
    各个控制单元的作用：
    输出门O(t-1)：用于保存C(t-1)中对h(t-1)有用的信息
    输入门it：用于判断当前输入xt是否对context有作用，当it=1时，使用xt作为输入
    遗忘门ft：用于判断当前细胞状态(cell_state)C(t)对上一个细胞状态C(t-1)的依赖程度,当前输入xt如果依赖上文信息，关闭遗忘门即可。
    细胞状态C(t)：它包含了当前输入xt和上一时刻细胞状态C(t-1)的信息，并且由于C(t)和C(t-1)之间是“短路连接”（由公式可以看出两者之间是线性关系），
    因此反向传播时，C(t)的梯度可以直接传播给C(t-1)，这也是LSTM能够有效缓解RNN中梯度消失和梯度爆炸的关键。

### BiLSTM
    BiLSTM是Bi-directional Long Short-Term Memory的缩写，是由前向LSTM-l与后向LSTM-r组合而成。
    利用LSTM对句子进行建模还存在一个问题：无法编码从后到前的信息。
    通过BiLSTM可以更好的捕捉双向的语义依赖。
    
### 门(gate)
    门能够有选择性的决定让哪些信息通过。其实门的结构很简单，就是一个sigmoid层和一个点乘操作的组合。
    因为sigmoid层的输出是0-1的值，这代表有多少信息能够流过sigmoid层。0表示都不能通过，1表示都能通过。
    
### 门循环单元（GRU）
    GRU是LSTM结构的变式；它将忘记门和输入门合并成一个新的门，称为更新门。GRU还有一个门称为重置门。
    重置门决定了如何将新的输入信息与前面的记忆相结合。更新门定义了前面记忆保存到当前时间步的量。
    
### 感知机(Perceptron)
    感知即意识对内外界信息的觉察、感觉、注意、知觉的一系列过程。模拟人类感知能力的机器，称之为‘感知机’，也称‘感知器’。
    感知机是二分类问题的线性分类模型，其输入为实例的特征向量，输出为实例的类别，分别是+1和-1，属于判别模型。
    感知器使用特征向量来表示的前馈神经网络，它是一种二元分类器，把矩阵上的输入 x（实数值向量）映射到输出值 f(x)上（一个二元的值）。

### 多层感知机(MLP,Multilayer Perceptron)
    多层感知器（Multilayer Perceptron,缩写MLP）是一种前向结构的人工神经网络，映射一组输入向量到一组输出向量。
    MLP可以被看作是一个有向图，由多个的节点层所组成，每一层都全连接到下一层。
    除了输入节点，每个节点都是一个带有非线性激活函数的神经元（或称处理单元）。
    一种被称为反向传播算法的监督学习方法常被用来训练MLP。 
    MLP是感知器的推广，克服了感知器不能对线性不可分数据进行识别的弱点。

### utterance embedding 
    多轮对话的每条文本（utterance）的向量表示,即为 utterance embedding 

### context embedding
    多轮对话由每条文本（utterance）组合成utterance embedding sequence；
    utterance embedding sequence 再通过一层Gated RNN（GRU、LSTM等）把无用的utterances中的噪声滤掉，进而取最后一个时刻的隐状态得到整个多轮对话（context）的context embedding

### 随机加权平均（SWA）
    随机加权平均（SWA）方法来自于集成。集成是用于提高机器学习模型性能的流行的技术。
    最简单的方式为，集成可以对不同初始化的模型的若干副本进行训练，并将对副本的预测平均以得到整体的预测。但是这种方法的缺点是必须承担n个不同副本的成本。研究人员提出快照集成（Snapshot Ensembles）方法。
    改方法是对一个模型进行训练，并将模型收敛到几个局部最优点，保存每个最优点的权重。这样一个单一的训练就可以产生n个不同的模型，将这些预测平均就能预测出整体。

### 未登录词（Out-of-vocabulary， OOV）
    未登录词就是训练时未出现，测试时出现了的单词。
    在自然语言处理或者文本处理的时候，我们通常会有一个字词库（vocabulary）。
    这个vocabulary要么是提前加载的，或者是自己定义的，或者是从当前数据集提取的。
    假设之后你有了另一个的数据集，这个数据集中有一些词并不在你现有的vocabulary里，我们就说这些词汇是Out-of-vocabulary，简称OOV。

### 查找表(lookup table)  
    查找表在不同的地方，其输入输出会有所不同；有时候，查找表就是一个全连接层。只不过输入比较稀疏的(二值)向量，所以矩阵乘法不需要那些输入为零值对应的系数，只要直接查表相加就可以了。 

### 表述性状态转移（Representational State Transfer，REST）
    REST是一种软件架构风格（约束条件和原则的集合，但并不是标准）。
    REST通过资源 的角度观察网络，以URI对网络资源进行唯一标识，响应端根据请求端的不同需求，通过无状态通信，对其请求的资源进行表述。
    满足REST约束条件和原则的架构或接口，就被称为是RESTful架构或RESTful接口。
    REST将资源的状态以最适合客户端或服务端的形式从服务器端转移到客户端。（或反过来）。

### 没有免费午餐定理(No Free Lunch，简称NFL)
    无免费午餐（No Free Lunch, NFL）定理证明了任何模型在所有问题上的性能都是相同的，其总误差和模型本身是没有关系的。
    NFL 定理的一个核心前提，也就是每种问题出现的概率是均等的，每个模型用于解决所有问题时，其平均意义上的性能是一样的。
    所有模型在等概率出现的问题上都有同样的性能，这件事可以从两个角度来理解：
    一是从模型的角度来看，如果单独拿出一个特定的模型来观察的话，这个模型必然会在解决某些问题时误差较小，而在解决另一些问题时误差较大；
    二是从问题的角度来看，如果单独拿出一个特定的问题来观察的话，必然有某些模型在解决这些问题时具有较高的精度，而另一些模型的精度就没那么理想了。
    脱离问题的实际情况谈论模型优劣是没有意义的，只有让模型的特点和问题的特征相匹配，模型才能发挥最大的作用。
    NFL定理表明没有一个学习算法可以在任何领域总是产生最准确的学习器。不管采用何种学习算法，至少存在一个目标函数，能够使得随机猜测算法是更好的算法
    NFL定理最重要意义是，在脱离实际意义情况下，空泛地谈论哪种算法好毫无意义，要谈论算法优劣必须针对具体学习问题

### 奥卡姆剃刀（Occam's Razor）
    奥卡姆剃刀（Occam's Razor）可以理解为如果有多种模型都能够同等程度地符合同一个问题的观测结果，那就应该选择其中使用假设最少的，也就是最简单的模型。
    尽管越复杂的模型通常能得到越精确的结果，但是在结果大致相同的情况下，模型就越简单越好。本质上说，奥卡姆剃刀的关注点是模型复杂度。

### 残差连接（skip connect）
    将输出表述为输入和输入的一个非线性变换的线性叠加。有时也叫跳过连接。
    在一定程度上，网络越深表达能力越强，性能越好。不过，随着网络深度的增加，带来了许多问题，梯度消散，梯度爆炸。
    深度学习依靠误差的链式反向传播来进行参数更新，一旦其中某一个导数很小，多次连乘后梯度可能越来越小，这就是常说的梯度消散，对于深层网络，传到浅层几乎就没了。
    但是如果使用了残差，每一个导数就加上了一个恒等项1。此时就算原来的导数df/dx很小，这时候误差仍然能够有效的反向传播。
    skip connect除了改善了反向传播过程中的梯度消散问题，另外还在一定程度上缓解了权重矩阵的退化问题。
    有时候虽然梯度范数大，但是如果网络的可用自由度对这些范数的贡献非常不均衡，也就是每个层中只有少量的隐藏单元对不同的输入改变它们的激活值，
    而大部分隐藏单元对不同的输入都是相同的反应，此时整个权重矩阵的秩不高。并且随着网络层数的增加，连乘后使得整个秩变的更低。
    这也是我们常说的网络退化问题，虽然是一个很高维的矩阵，但是大部分维度却没有信息，表达能力没有看起来那么强大。
    虽然权重矩阵是一个很高维的矩阵，但是大部分维度却没有信息，使得网络的表达能力没有看起来那么强大。
    这样的情况一定程度上来自于网络的对称性，而残差连接打破了网络的对称性。
    残差连接正是强制打破了网络的对称性，提升了网络的表征能力

### ReZero加权的残差连接
    在NLP领域，如Transformer的深度模型由于梯度消失/爆炸难以训练，往往需要花费很长的时间才能收敛。
    ReZero：在残差连接前增加一个权重，使模型能够更好接受到梯度信号，加快收敛速度。
    对每个残差连接加上一个可学习的系数——使得模型能在训练初期更加稳定，进而促进整个训练过程。
    这种方法能在上百层的Transformer上收敛，并在常见深层模型上大大缩短训练时间，同时取得相近的结果。
    正常的残差连接对一个L层的模型，第 i+1 层的输入（也即第i层的输出）可以表示为：x(i+1) = xi + Fi(xi)
    而ReZero做的很简单，它只是在残差连接前加了一个可学习的参数：x(i+1) = xi + αi * Fi(xi)
    并且所有的α都初始为0。显然，当α=1时就是正常的残差网络，当初始化α=0时就是ReZero
    
### 几种连接方式    
    FC：单纯的全连接
        x(i+1) = Fi(xi)
    FC+Res：全连接和残差结合 
        x(i+1) = x(i) + Fi(xi)
    FC+Norm：全连接和normalization
        x(i+1) = Norm(Fi(xi))
    FC+Res+Pre-Norm: 全连接和残差和pre-normalization
        x(i+1) = x(i) + Fi(Norm(xi))
    FC+Res+Post-Norm：全连接和残差和post-normalization
        x(i+1) = Norm(x(i) + Fi(xi))
    FC+ReZero：
        x(i+1) = x(i) + αi * Fi(xi)

### 对称性(Symmetry)和打破对称性（symmetry breaking）    
    「对称」就是对象在变换之后，结果和变换前不变。如有一变换，前后系统的状态，等价或相同，则此变换为对称变换。
    当某一层的表示的权重相同的话，会导致下一层的单元计算结果全部相同，这意味着，特征在传递到下层时，全部压缩为一个特征了。
    我们把这种现象称为对称性(Symmetry)。
    一个全连接的神经网络，同一层中的任意神经元是同构的，对于相同的输入他们会有同样的输出，
    此时如果将参数全部初始化为相同的值，那么无论前向传播还是反向传播，参数的取值还是完全相同，学习将无法打破这种对称性，
    最终同一网络层中的各个参数仍然是相同的。
    必须随机的初始化神经网络参数的值，以打破这种对称性。

### 随机初始化
    将权重w全部初始化为零或相同的随机数，那么每一层所学到的参数都是一样的，因为它们的梯度一样，所以在反向传播的过程中，每一层的神经元也是相同的。
    因此会导致代价函数在开始的一段时间内，明显下降，但是一段时间以后，停止继续下降。都会导致同样的问题，即对称问题(Symmetry problem)
    初始化为较小的随机数,开始模型可以很好的运行一段时间，但是随着时间增加，前向传递时，方差开始减少，梯度也开始向零靠近，会导致梯度消失(Gradient Vanishing)。
    初始化为较大的随机数,反向传播时，倒数趋于零，梯度也会消失。此外，权重较大且当输入也很大时，如果使用sigmoid做激活函数，会使输出趋向于0和1，会导致更多问题。
    随机初始化很小的值，使得W很小是因为，可以参照激活函数sigmoid和tanh，当W很大，用W * X+b=a得到的a很大，再用对a用激活函数如sigmoid(a)，
    由于a很大了，sigmoid(a)中的a会趋向正无穷或负无穷，则函数值sigmoid(a)趋向于一个平缓的趋势，
    在梯度下降的时候计算的梯度很小，会导致学习的很慢，故使得W取一个很小的值

### 放缩点积attention（scaled dot-Product attention）
    attention可以有很多种计算方式: 加性attention、点积attention，还有带参数的计算方式。
    Attention中(Q^T)*K矩阵计算，query和key的维度要保持一致；
    query和key的点积是 [seq_length * attention_head_size] * [attention_head_size * seq_length]=[seq_length * seq_length]
    其中，query可以看作M个维度为d的向量(长度为M的sequence的向量表达)拼接而成，key可以看作N个维度为d的向量(长度为N的sequence的向量表达)拼接而成。
    放缩点积attention（scaled dot-Product attention）就是使用点积进行相似度计算的attention，
    只是多除了一个（为K的维度）起到调节作用，使得内积不至于太大，防止梯度消失。
    当输入信息的维度 d 比较高，点积模型的值通常有比较大方差，从而导致 softmax 函数的梯度会比较小。因此，缩放点积模型可以较好地解决这一问题。
    缩放因子的作用是 「归一化」 。
    假设Q(m * d), K(N * d)里的元素的均值为0，方差为1，那么A{T}=Q{T} K中元素的均值为0，方差为d. 
    当d变得很大时， A 中的元素的方差也会变得很大，如果 A 中的元素方差很大，那么 softmax(A)的分布会趋于陡峭(分布的方差大，分布集中在绝对值大的区域)。
    总结一下就是softmax(A)的分布会和d有关。因此 A中每一个元素乘上 缩放因子 后，方差又变为1。
    这使得softmax(A) 的分布“陡峭”程度与d解耦，从而使得训练过程中梯度值保持稳定。

### 隐马尔科夫模型（Hidden Markov Model，HMM）
    隐马尔可夫模型（Hidden Markov Model，HMM）是统计模型，它用来描述一个含有隐含未知参数的马尔可夫过程。描述的是已知量和未知量的一个联合概率分布，p（x,y）。
    隐马尔可夫模型(Hidden Markov model, HMM)是一种结构最简单的动态贝叶斯网的生成模型，它是一种有向图模型。
    隐马尔科夫模型是关于时序的概率模型，描述由一个隐藏的马尔可夫链随机生成不可观测的状态随机序列，再由各个状态生成一个可观测的随机序列的过程。
    HMM只依赖于每一个状态和它对应的观察对象; 
    目标函数和预测目标函数不匹配：HMM学到的是状态和观察序列的联合分布P(Y,X)，而预测问题中，我们需要的是条件概率P(Y|X)。
    HMM模型对转移概率和表现概率直接建模，统计共同出现的概率，是一种生成式模型。

### 最大熵隐马尔科夫模型（MEMM）
    MEMM考虑到相邻状态之间依赖关系，且考虑整个观察序列，因此MEMM的表达能力更强；MEMM不考虑P(X)减轻了建模的负担，同时学到的是目标函数是和预测函数一致。
    HMM只限定在了观测与状态之间的依赖，而MEMM引入自定义特征函数，不仅可以表达观测之间的依赖，还可表示当前观测与前后多个状态之间的复杂依赖。

### 马尔可夫链(Markov chain)
    马尔可夫链（Markov chain），又称离散时间马尔可夫链（discrete-time Markov chain），为状态空间中经过从一个状态到另一个状态的转换的随机过程。
    该过程要求具备“无记忆”的性质：下一状态的概率分布只能由当前状态决定，在时间序列中它前面的事件均与之无关。
    这种特定类型的“无记忆性”称作马尔可夫性质。

### 随机场(random field, RF)
    随机场，可以看成是一组随机变量的集合（这组随机变量对应同一个样本空间）。当给每一个位置按照某种分布随机赋予一个值之后，其全体就叫做随机场。
    随机场包含两个要素：位置（site），相空间（phase space）。当给每一个位置中按照某种分布随机赋予相空间的一个值之后，其全体就叫做随机场。
    我们不妨拿种地来打个比方。“位置”好比是一亩亩农田； “相空间”好比是种的各种庄稼。
    我们可以给不同的地种上不同的庄稼，这就好比给随机场的每个“位置”，赋予相空间里不同的值。所以，俗气点说，随机场就是在哪块地里种什么庄稼的事情。
    
### 马尔科夫随机场(MRF)
    马尔科夫随机场(MRF)，描述了具有某种特性的集合。拿种地打比方，如果任何一块地里种的庄稼的种类仅仅与它邻近的地里种的庄稼的种类有关，与其它地方的庄稼的种类无关，那么这些地里种的庄稼的集合，就是一个马尔可夫随机场。
    如果给定的 MRF 中每个随机变量下面还有观察值，我们要确定的是给定观察集合下，这个 MRF 的分布，也就是条件分布，那么这个 MRF 就称为 Conditional random fields (CRF)。
    它的条件分布形式完全类似于 MRF 的分布形式，只不过多了一个观察集合 X。所以，CRF 本质上是给定了条件 (观察值 observations) 集合的 MRF.

### 条件随机场(conditional random field, CRF)
    HMM(隐马尔可夫模型)是CRF的一个特例。HMM等价于只有一个特征函数的CRF。
    可以用于构造在给定一组输入随机变量的条件下,另一组输出随机变量的条件概率分布模型.
    条件随机场(conditional random field)是给定随机变量X条件下,随机变量Y的马尔可夫随机场.
    Y是输出变量,表示标记序列,即状态序列,X是输入变量,也就是我们得到的需要标注的观测序列.
    我们利用训练数据集通过极大似然估计或正则化的极大似然估计得到条件概率模型,
    在预测时,我们根据给定的输入序列,求出条件概率最大的输出序列.
    CRF是一种判别式模型。
    NER（命名实体识别）这个任务用到的是线性链条件随机场。
    线性链条件随机场的形式是这样的，观测点是你要标注的这些词本身和他们对应的特征，例如说词性是不是专有名词、语义角色是不是主语之类的。
    隐节点，是这些词的标签，比如说是不是人名结尾，是不是地名的开头这样。
    CRF不仅解决了HMM输出独立性假设的问题，还解决了MEMM的标注偏置问题，MEMM容易陷入局部最优是因为只在局部做归一化，
    而CRF统计了全局概率，在做归一化时考虑了数据在全局的分布，而不是仅仅在局部归一化，这样就解决了MEMM中的标记偏置的问题。
    使得序列标注的解码变得最优解。
    【条件随机场】其实分为两个关键词【条件】和【随机场】，【条件】对应于【条件概率】。

### HMM和CRF的三个基本问题
    序列、参数、概率，这三因素，知道其中的两个，求解另外一个，就演化为了三个基本问题。
    求概率->概率问题；求参数->学习问题；求序列->预测问题。
    至于序列，HMM与CRF的是一样的，都有个观测序列和状态序列；至于参数，因两个不同的模型，具体参数多少是有些不同的；而至于概率，一个是联合分布概率，一个是条件概率。
| 模型 | 概率计算问题 | 预测问题 | 学习问题 |
| ------ | ------ | ------ | ------ |
| 定义 | 已知观测序列、模型参数，求观察序列Q的概率 | 已知模型参数、观测序列、观测序列条件概率，求解状态系列 | 已知观测序列，求解隐状态参数λ=(A,B,π)，使得在模型中观测序列发生的可能性P(Q|λ)最大 |
| HMM | 暴力计算法、前向算法、后向算法 | 近似算法、维特比算法 | 极大似然估计 |
| CRF | 前向算法、后向算法 | 维特比算法 | 梯度下降法、拟牛顿法 |

### 生成式模型(Generative Model)
    无穷样本 -> 概率密度模型 = 产生式模型 -> 预测
    通常，生成式模型计算联合概率分布
    基本思想是首先建立样本的概率密度模型，再利用模型进行推理预测。要求已知样本无穷或尽可能的大限制。
    训练阶段是只对 P(X,Y)建模，需要确定维护这个联合概率分布的所有的信息参数。在预测阶段再对新的样本计算P(X,Y)，导出Y
    生成模型是评估给定输出Y，如何从概率分布上生成输入序列 X 。
    估计的是联合概率分布，P(Y, X)=P(Y|X)*P(X)，由联合概率密度分布 P(X,Y)，然后求出条件概率分布 P(Y|X) 作为预测的模型，即生成模型公式为：P(Y|X)= P(X,Y)/ P(X)。
    基本思想是首先建立样本的联合概率密度模型 P(X,Y)，然后再得到后验概率 P(Y|X)，再利用它进行分类，其主要关心的是给定输入 X 产生输出 Y 的生成关系。

### 判别式模型(Discriminative Model)
    有限样本 -> 判别函数 = 判别式模型 -> 预测
    通常，判别式模型计算条件概率
    基本思想是有限样本条件下建立判别函数，不考虑样本的产生模型，直接研究预测模型。
    判别模型是直接对 P(Y|X)建模，就是说，直接根据X特征来对Y建模训练。
    判别模型是给定输入序列 X，直接评估对应的输出 Y。
    估计的是条件概率分布， P(Y|X)，是给定观测变量 X 和目标变量 Y 的条件模型。
    由数据直接学习决策函数 Y=f(X) 或者条件概率分布 P(Y|X) 作为预测的模型，其主要关心的是对于给定的输入 X，应该预测什么样的输出 Y。

### LDA（Latent Dirichlet Allocation）
    LDA（Latent Dirichlet Allocation）是一种文档主题生成模型，也称为一个三层贝叶斯概率模型，包含词、主题和文档三层结构。
    所谓生成模型，就是说，我们认为一篇文章的每个词都是通过“以一定概率选择了某个主题，
    并从这个主题中以一定概率选择某个词语”这样一个过程得到。文档到主题服从多项式分布，主题到词服从多项式分布。
    LDA的目的就是要识别主题，即把文档—词汇矩阵变成文档—主题矩阵（分布）和主题—词汇矩阵（分布）


### 关系抽取的流水线模型(Pipline Method)
    Pipline Method，流水线方法：输入一个句子，首先进行命名实体识别，然后对识别出来的实体进行两两组合，再进行关系分类。
    流水线的方法存在蛮大的缺点，例如：
    1.错误传播，实体识别模块的错误会传播到后面的分类模块；
    2.忽略了两个子任务之间存在的关系。例如“中国的首都是北京”的例子，如果存在“首都”关系，那么前一个实体必然是国家类别，后一个实体比如是城市类别。流水线的方法，忽略了这些信息；
    3.产生了没必要的冗余信息，由于需要对识别出来的实体进行两两配对，然后再进行关系分类；那些没有关系的实体对就会产生多余的信息，提高错误率。

### 关系抽取的联合抽取模型(Joint Method)
    Joint Method，即联合抽取方法，则跟流水线的方法不同，基于流水线方法的诸多缺陷，Joint Method能够通过一个实体识别和关系分类的联合模型，直接得到有关系的实体三元组。
    Joint Method主要分为两个流派，基于参数共享(Parameter Sharing)和基于标注策略（Tagging Policy）两类。
    基于参数共享(Parameter Sharing)的联合抽取方法，每个词都会被映射到一个实体标记（BILOS:Begin Inside Last Outside Single)，它包含了该字在实体中的位置信息。
    NER模块没有用CRF，而是额外用了一层LSTM来解码双向LSTM编码出来的Hidden state，并建模它和实体标记之间的关系。关系分类模块采用CNN模型，处理BiLSTM的Hidden state并输出关系类别。
    基于标注策略（Tagging Policy）的联合抽取方法，将实体识别和关系分类两个问题，转化为一个序列标注的问题，然后通过一个端对端的神经网络模型直接得到关系实体三元组。
这种标注策略主要由下图中三部分组成：
    1）实体中词的位置信息{B（实体开始），I（实体内部），E（实体结尾），S（单个实体）}；
    2）关系类型信息{根据预先定义的关系类型进行编码}；
    3）实体角色信息{1（实体1），2（实体2）}。注意，这里只要不是实体关系三元组内的词全部标签都为"O"。
    如`北京是中国首都。`,其系列标注如下：
    北:B-首都-1
    京:E-首都-1
    是:O
    中:B-首都-2
    国:E-首都-1
    首:O
    都:O
    。:O
    "B-首都-1"表示这个词是一个实体的begin，同时这个实体属于关系`首都`的第一个实体。
    但这种方法也有些缺点，如"一个s,多个(p,o)","多个s,一个(p,o)","多个s,多个(p,o)","同一对(s,o)多种关系p","s,o出现重叠的情况"，估计得改成多标签多分类的预测问题了。

### cbow
    在cbow方法中，是用周围词预测中心词，利用中心词的预测标签，使用梯度下降(Gradient Descent)方法，不断的去调整周围词的向量。
    当训练完成之后，每个词都会作为中心词，把周围词的词向量进行了调整，这样也就获得了整个文本里面所有词的词向量。
    要注意的是， cbow的对周围词的调整是统一的：求出的梯度(gradient)会同样的作用到每个周围词的词向量当中去。
    cbow预测行为的次数跟整个文本的词数几乎是相等的,每次预测行为才会进行一次反向传播算法(BackPropagation)，复杂度大概是O(V);

### skip-gram
    skip-gram是用中心词来预测周围的词。在skip-gram中，会利用对周围词的预测结果情况，使用梯度下降(Gradient Descent)来不断的调整中心词的词向量，最终所有的文本遍历完毕之后，也就得到了文本所有词的词向量。
    可以看出，skip-gram进行预测的次数是要多于cbow的：因为每个词在作为中心词时，都要使用周围词进行预测一次。这样相当于比cbow的方法多进行了K-1次（假设K为窗口大小），因此时间的复杂度为O(KV)，训练时间要比cbow要长。
    但是在skip-gram当中，每个词都要收到周围的词的影响，每个词在作为中心词的时候，都要进行K次的预测、调整。
    因此， 当数据量较少，或者词为生僻词出现次数较少时， 这种多次的调整会使得词向量相对的更加准确。
    因为尽管cbow从另外一个角度来说，某个词也是会受到多次周围词的影响，但是他的调整是跟周围的词一起调整的，梯度(gradient)会平均分到该词上， 相当于该生僻词没有收到专门的训练，它只是沾了周围词的光而已。

### True Positive(真正，TP)
    将正类预测为正类数
    
### True Negative(真负，TN)
    将负类预测为负类数
    
### False Positive(假正，FP)
    将负类预测为正类数→误报 (Type I error)
    
### False Negative(假负，FN)
    将正类预测为负类数→漏报 (Type II error)

### 真阳性率（TPR）    
    TPR：在所有实际为阳性的样本中，被正确地判断为阳性之比率。TPR=TP/(TP+FN)

### 伪阳性率（FPR）
    FPR：在所有实际为阴性的样本中，被错误地判断为阳性之比率。FPR=FP/(FP+TN)
    
### ROC曲线
    ROC的全称是Receiver Operating Characteristic Curve,也称“受试者工作特征曲线”,或者感受性曲线,
    ROC来说，横坐标就是FPR，而纵坐标就是TPR，当 TPR越大，而FPR越小时，说明分类结果是较好的。
    ROC本质上就是在设定某一阈值之后，计算出该阈值对应的TPR & FPR，便可以绘制出ROC上对应的一个点，当设定若干个阈值之后，便可以连成ROC曲线，因此可以想见，当所采样的阈值越多，ROC Curve越平滑。
       
### AUC
    AUC：Aera Under Curve，即ROC曲线下的面积
    AUC值表示用不同阈值下TPR与FPR连成的ROC曲线下方的面积。AUC值越高,模型对于正负样本的区分能力越强,效果越好。
    这个面积显然不会大于1，又因为ROC曲线一般都在y=x这条直线上方，所以AUC的值域为(0, 1)
    使用AUC作为评价指标是因为很多时候我们并不能够从ROC曲线上清晰准确地判断哪个分类器的性能更好，而作为一个数值，AUC越大，对应的分类器的性能越好。
    其物理意义可以表示为：随机给定一正一负两个样本，将正样本排在负样本之前的概率，因此AUC越大，说明正样本越有可能被排在负样本之前，即分类额结果越好。
    
### KS曲线
    KS曲线是两条线，其横轴是阈值，纵轴是TPR与FPR。两条曲线之间之间相距最远的地方对应的阈值，就是最能划分模型的阈值。
    KS曲线横轴的指标，是阈值（Threshold）。KS曲线中有两条线，这两条线有共同的横轴，但是纵轴分别有两个指标：FPR与TPR。

### KS值
    KS值是MAX(TPR - FPR），即两曲线相距最远的距离。
    
### WOE
WOE的全称是“Weight of Evidence”，即证据权重。WOE是对原始自变量的一种编码形式。
要对一个变量进行WOE编码，需要首先把这个变量进行分组处理（也叫离散化、分箱等等，说的都是一个意思）。
响应客户（风险模型中，对应的是违约客户，总之，指的是模型中预测变量取值为“是”或者说1的个体）;
WOE表示的实际上是“当前分组中响应客户占所有响应客户的比例”和“当前分组中没有响应的客户占所有没有响应的客户的比例”的差异。
WOE也可以这么理解，他表示的是当前这个组中响应的客户和未响应客户的比值，和所有样本中这个比值的差异。这个差异是用这两个比值的比值，再取对数来表示的。WOE越大，这种差异越大，这个分组里的样本响应的可能性就越大，WOE越小，差异越小，这个分组里的样本响应的可能性就越小。
当前分组中，响应的比例越大，WOE值越大；
当前分组WOE的正负，由当前分组响应和未响应的比例，与样本整体响应和未响应的比例的大小关系决定，当前分组的比例小于样本整体比例时，WOE为负，当前分组的比例大于整体比例时，WOE为正，当前分组的比例和整体比例相等时，WOE为0。
WOE的取值范围是全体实数。

### IV值
IV的全称是Information Value，中文意思是信息价值，或者信息量。
我们需要一些具体的量化指标来衡量每自变量的预测能力，并根据这些量化指标的大小，来确定哪些变量进入模型。IV就是这样一种指标，他可以用来衡量自变量的预测能力。类似的指标还有信息增益、基尼系数等等。
对于变量的一个分组，这个分组的响应和未响应的比例与样本整体响应和未响应的比例相差越大，IV值越大，否则，IV值越小；
极端情况下，当前分组的响应和未响应的比例和样本整体的响应和未响应的比例相等时，IV值为0；
IV值的取值范围是[0,+∞)，且，当当前分组中只包含响应客户或者未响应客户时，IV = +∞。






    

