
sigmoid是非线性的，所可以用在神经网络中间层中作为激活函数，也可以用在最后一层将结果映射到(0, 1)之间。
优点：
可以看到sigmoid函数处处连续，便于求导；
可以将函数值的范围压缩到[0,1]，可以压缩数据，且幅度不变。
可以做二分类问题；
缺点：
在输入很大的情况下，函数进入饱和区，函数值变化很小，使x对y的区分度不高，同时也容易梯度消失，不利于深层神经网络的反馈传输，反向传输时计算量大。
函数均值不为0，当输出大于0时，则梯度方向将大于0，也就是说接下来的反向运算中将会持续正向更新；同理，当输出小于0时，接下来的方向运算将持续负向更新。

在使用逻辑回归做二分类问题时，sigmoid函数常用作逻辑回顾的假设函数，简单理解，就是在线性回归的基础上套一个sigmoid函数，将线性回归的结果 (-inf,+inf ) ，映射到 (0,1) 范围内，使他变为一个二分类问题。

sigmoid对应伯努利分布，softmax对应类别分布（categorical distribution）;

softmax用于多分类问题，在多分类神经网络种，常常作为最后一层的激活函数，前一层的数值映射为(0,1)的概率分布，且各个类别的概率归一，从表达式中很容易看出来。
与sigmoid不同的是，softmax没有函数图像，它不是通过固定的 y=f(x) 的映射将固定的值映射为固定的值，
softmax是计算各个类别占全部的比例，可以理解为输入一个向量，然后出一个向量，输出的向量的个位置的元素表示原向量对应位置的元素所占整个向量全部元素的比例。

很容易想到，原始向量经过softmax之后，原始向量中较大的元素，在输出的向量中，对应位置上还是较大，反之，原始向量中较小的元素还是会很小，保留了原始向量元素之间的大小关系。
在做多分类问题时，输出向量的第几维最大，就表示属于第几个class的概率最大，由此分类。

softmax是用来做多分类的，sigmoid是用来做二分类的，可以理解为sigmoid是softmax当类别数为2的特殊情况，可以看到sigmoid函数的曲线是不断趋近于0或者1，
而最终结果是要得到一个0或1的离散值，那为什么不用sign函数（符号函数，当x>0，sign(x)=1;当x=0，sign(x)=0; 当x<0， sign(x)=-1）代替，
除了sign的梯度为0不能反向传播以外，我认为是sigmoid函数比sign更“soft”(圆滑)一点，
在不影响结果的情况下，还保留了一些其他的信息，上面sigmoid函数的优点中也提到，可以压缩数据且幅度不变，
y保留了x之间的大小关系，不光可以区分属于不同类的元素，还可以区分属于同一类的元素谁更属于这一类。



