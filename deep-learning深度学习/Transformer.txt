
Transformer

Transformer整体架构看似复杂，其实就是一个Seq2Seq结构，包括编码器(Encoder)和解码器(decoder)，即是由编码组件、解码组件和它们之间的连接组成。
编码组件部分由一堆编码器（encoder）构成（将n个编码器叠在一起）。解码组件部分也是由相同数量（与编码器对应）的解码器（decoder）组成的。

Encoder的输出和decoder的结合方式是将最后一个encoder的输出将和每一层的decoder进行结合。
所有的编码器(Encoder)在结构上都是相同的，但它们没有共享参数。每个编码器(Encoder)都可以分解成两个子层，
即Encoder的每一层有两个操作，分别是注意力（self-attention）和前馈（feed-forward）；
而Decoder的每一层有三个操作，分别是Self-Attention、Encoder-Decoder Attention以及Feed Forward操作。
这里的Self-Attention和Encoder-Decoder Attention都是用的是Multi-Head Attention机制。

从编码器输入的句子首先会经过一个自注意力（self-attention）层，这层帮助编码器在对每个单词编码时关注输入句子的其他单词。
自注意力层的输出会传递到前馈（feed-forward）神经网络中。每个位置的单词对应的前馈神经网络都完全一样。
解码器(decoder)中也有编码器的自注意力（self-attention）层和前馈（feed-forward）层。
除此之外，这两个层之间还有一个编码-解码注意力层(Encoder-Decoder Attention)，用来关注输入句子的相关部分。

一个编码器接收向量列表作为输入，接着将向量列表中的向量传递到自注意力层进行处理，然后传递到前馈神经网络层中，将输出结果传递到下一个编码器中。
输入序列的每个单词都经过自编码过程。然后，他们各自通过前向传播神经网络——完全相同的网络，而每个向量都分别通过它。


