
显卡、GPU和CUDA
显卡（Video card，Graphics card）全称显示接口卡，又称显示适配器; 
显卡是电脑进行数模信号转换的设备，承担输出显示图形的任务。具体来说，显卡接在电脑主板上，它将电脑的数字信号转换成模拟信号让显示器显示出来。
原始的显卡一般都是集成在主板上，只完成最基本的信号输出工作，并不用来处理数据。随着显卡的迅速发展，就出现了GPU的概念，显卡也分为独立显卡和集成显卡。
GPU是显卡上的一块芯片，就像CPU是主板上的一块芯片。

1）、最开始GPU仅用于图形渲染;
2）、后来人们发现，GPU这么一个强大的器件只用于图形处理太浪费了，它应该用来做更多的工作，例如浮点运算。怎么做呢？
直接把浮点运算交给GPU是做不到的，因为那个时候它只能用于图形处理。最开始，是把浮点运算做一些处理，包装成图形渲染任务，然后交给GPU来做。这就是GPGPU（General Purpose GPU）的概念。
不过这样做有一个缺点，就是你必须有一定的图形学知识，否则你不知道如何包装。
3）、于是，为了让不懂图形学知识的人也能体验到GPU运算的强大，Nvidia公司又提出了CUDA的概念。
这也就是为何说，并不是所有GPU都支持CUDA。
计算机右击–>管理–>设备管理器–>显示适配器,可看到是否支持cuda;
若支持cuda,会有NVIDIA的显示适配器，再去英伟达官网选择自己的系列类型
https://developer.nvidia.com/cuda-gpus
更详细的CUDA版本查看
（1）打开控制面板；
（2）在右上方的搜索框里输入NVIDIA；
（3）鼠标放在搜索出来的NVIDIA上；
（4）完成上一步骤后，界面里可以查看NVIDIA的一些信息，显卡的驱动版本

要查看您电脑支持的CUDA版本，可以按照以下步骤操作：
1、安装NVIDIA 显卡驱动程序，驱动程序包含了CUDA的安装包。可以从NVIDIA官网下载对应版本的驱动程序。
https://www.nvidia.cn/Download/index.aspx?lang=cn
2、打开终端或命令提示符，输入以下命令：nvcc --version
3、如果您的电脑支持CUDA，则会显示CUDA的版本信息，如下所示：
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2019 NVIDIA Corporation
Built on Sun_Jul_28_19:07:16_PDT_2019
Cuda compilation tools, release 10.1, V10.1.243
在上面的示例中，CUDA版本为10.1。

要查看Windows电脑可安装的CUDA版本，可以按照以下步骤进行：
1、打开NVIDIA官网的CUDA下载页面：
https://developer.nvidia.com/cuda-downloads
2、在“操作系统”下拉菜单中选择“Windows”。
3、在“体系结构”下拉菜单中选择您的操作系统的位数（32位或64位）。
4、在“版本”下拉菜单中，您将看到所有可用的CUDA版本。您可以根据您的需要选择一个版本。
5、单击选定的CUDA版本，您将看到该版本的详细信息，包括支持的GPU和操作系统版本。
请注意，您需要检查您的GPU是否与所选的CUDA版本兼容。您可以在NVIDIA的CUDA支持网页上找到有关支持的GPU的信息：
https://developer.nvidia.com/cuda-gpus

CUDA(Compute Unified Device Architecture)，通用并行计算架构，是一种运算平台。它包含CUDA指令集架构以及GPU内部的并行计算引擎。
只要开发CUDA程序，从而可以更加方便的利用GPU强大的计算能力，而不是像以前那样先将计算任务包装成图形渲染任务，再交由GPU处理。

CPU和GPU的关系
在没有GPU之前，基本上所有的任务都是交给CPU来做的。有GPU之后，二者就进行了分工，CPU负责逻辑性强的事物处理和串行计算，GPU则专注于执行高度线程化的并行处理任务（大规模计算任务）。

独立显卡和集成显卡的区别。
所谓集成，是指显卡集成在主板上，不能随意更换。而独立显卡是作为一个独立的器件插在主板的AGP接口上的，可以随时更换升级。
另外，集成显卡使用物理内存，而独立显卡有自己的显存。
集成显卡和独立显卡都是有GPU的。

###################################################################################################
# GPU的使用：

明明内存够用，但是还是提示内存错误：
错误类型：CUDA_ERROE_OUT_OF_MEMORY
E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to alloc 17179869184 bytes on host: CUDA_ERROR_OUT_OF_MEMORY
W ./tensorflow/core/common_runtime/gpu/pool_allocator.h:195] could not allocate pinned host memory of size: 17179869184


yhq@gpu2-131:~$ free -h
              total        used        free      shared  buff/cache   available
Mem:            62G         31G        6.4G        773M         24G         27G
Swap:           63G         32M         63G
yhq@gpu2-131:~$ 
实际上报错是说GPU资源不够用
nvidia-smi
用上面的命令看下哪些GPU没有被使用，例如3号和5号没有被使用，则你可以用如下命令运行你的程序
export CUDA_VISIBLE_DEVICES=3,5 python main.py

yhq@gpu2-131:~$ nvidia-smi
Tue Jul 16 10:09:13 2019       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.130                Driver Version: 384.130                   |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 108...  Off  | 00000000:05:00.0 Off |                  N/A |
| 27%   48C    P8    18W / 250W |  10625MiB / 11171MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX 108...  Off  | 00000000:06:00.0 Off |                  N/A |
| 28%   49C    P8    19W / 250W |  10948MiB / 11172MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  GeForce GTX 108...  Off  | 00000000:09:00.0 Off |                  N/A |
| 24%   43C    P8    10W / 250W |  10627MiB / 11172MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  GeForce GTX 108...  Off  | 00000000:0A:00.0 Off |                  N/A |
| 23%   37C    P8    10W / 250W |  10627MiB / 11172MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0     12707      C   /usr/bin/python3                           10615MiB |
|    1      9554      C   /usr/bin/python3                           10653MiB |
|    1     12707      C   /usr/bin/python3                             285MiB |
|    2     12707      C   /usr/bin/python3                           10617MiB |
|    3     12707      C   /usr/bin/python3                           10617MiB |
+-----------------------------------------------------------------------------+

nvidia-smi
显示所有GPU的当前信息状态
显示的表格中：
Fan：                     风扇转速（0%--100%），N/A表示没有风扇
Temp：                 GPU温度（GPU温度过高会导致GPU频率下降）
Perf：                    性能状态，从P0（最大性能）到P12（最小性能）
Pwr：                     GPU功耗
Persistence-M：   持续模式的状态（持续模式耗能大，但在新的GPU应用启动时花费时间更少）
Bus-Id：               GPU总线，domain:bus:device.function
Disp.A：                Display Active，表示GPU的显示是否初始化
Memory-Usage：显存使用率
Volatile GPU-Util：GPU使用率
ECC：                   是否开启错误检查和纠正技术，0/DISABLED, 1/ENABLED
Compute M.：     计算模式，0/DEFAULT,1/EXCLUSIVE_PROCESS,2/PROHIBITED

上机器中，虽说主机的内存够用，但GPU内存被用光了，故而报错误：CUDA_ERROE_OUT_OF_MEMORY

另外，有时候服务器GPU资源有空余但并不是完全满足需要，可以通过下面的方法来解决：
服务器的GPU大小为M
tensorflow只能申请N（N<M）
也就是tensorflow不能申请到GPU的全部资源 然后就会报错

解决方法：
找到代码中Session
在session定义前 增加
config = tf.ConfigProto(allow_soft_placement=True)
#最多占gpu资源的70%
gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.7)
#开始不会给tensorflow全部gpu资源 而是按需增加
config.gpu_options.allow_growth = True
sess = tf.Session(config=config)
这样就没问题了

其实tensorflow 算是一个比较贪心的工具了
就算用device_id指定gpu 也会占用别的GPU的显存资源 必须在执行程序前
执行
export CUDA_VISIBLE_DEVICES=n python main.py

（n为可见的服务器编号）
再去执行python 代码.py 才不会占用别的GPU资源

# 禁用GPU,禁止使用GPU，仅仅需要CUDA_VISIBLE_DEVICES设置为空即可；CUDA_DEVICE_ORDER设置为PCI_BUS_ID是为了保证设备ID与物理ID一致(NVML工具nvidia-smi按设备的PCI Bus ID 为设备分配索引序号，由于PCI Bus ID 是硬件相关的，我们把设备的 PCI Bus ID 或者按该ID分配的索引号为物理ID；设置 CUDA_DEVICE_ORDER = PCI_BUS_ID 来要求运行时设备查询按照 PCI_BUS_ID 的顺序索引，从而使得 设备ID=物理ID 保证CUDA应用按期望使用指定设备)
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"   # see issue #152
os.environ["CUDA_VISIBLE_DEVICES"] = ""


#  查看自己的电脑是否支持GPU
方法一：
1、首先打开任务管理器
2、然后点击性能
3、然后点击左侧下拉框，往下拉，如果看到有GPU，及适配器，如：NVIDIA GeForce MX3.* 则表示自己的电脑是有GPU的
方法二：
1、计算机右击–>管理–>设备管理器–>显示适配器
2、去英伟达官网查看，选择自己的系列类型： https://developer.nvidia.com/cuda-gpus
3、如何检查显卡支持哪个版本的CUDA？
3.1、打开控制面板，然后在右上方的搜索框里输入NVIDIA，如下图所示：第一张是打开控制面板时的图，第二张是输入NVIDIA之后，从图中的左上角可以看到搜索出来的NVIDIA。
3.2、完成上面步骤之后，鼠标放在搜索出来的NVIDIA上，如下图红框圈出部分，然后双击。
3.3、完成上一步骤后，进入如下界面，在该界面里可以查看NVIDIA的一些信息，显卡的驱动版本，如下图的左侧菜单所示。
3.4、点击帮助菜单，在下拉的菜单里选择系统信息选项，如下图红色框圈出选项。
3.5、在弹出的系统信息窗口里有两个菜单页面：显示和组件，选择组件，可以看到很多文件名，在文件名中找到NVCUDA，在产品名称一列可以看到该NVCUDA的版本，如下图红色框圈出部分。该CUDA是9.1版本的
4、更新NVDIA驱动程序：https://www.nvidia.cn/Download/index.aspx?lang=cn






